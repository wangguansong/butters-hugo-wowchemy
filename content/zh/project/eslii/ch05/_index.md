---
title: 5 基拓展和正则化
summary: >
  第 139-190 页。第五章为基拓展及其正则化方法，进入非线性方法的范畴。

date: 2018-10-17T09:39:00+08:00
lastmod: 2022-06-14T11:08:00+08:00

weight: 501

---

之前的章节介绍了回归和分类问题中对特征变量呈线性的模型。线性回归、线性判别分析、对数几率回归、和分离超平面等模型都是建立在线性模型假设上的。然而真实的关系函数 $f(X)$ 几乎不可能是 $X$ 的线性函数。在回归问题中，$f(X)=\operatorname{E}(Y|X)$ 通常对 $X$ 为非线性并且非加性的[^1]，对 $f(X)$ 的线性假设常常是一个方便的近似，这种简化有时可能是必要的。线性函数的“方便”在于它们易于理解，并可视为是 $f(X)$ 的一阶泰勒近似；在 $N$ 比较小而且 $p$ 比较大时，有可能线性模型是唯一能避免过拟合的方法，所以这个简化“有时是必要的”。在分类问题中也与之类似，线性的贝叶斯最优判别边界假定了 $\operatorname{Pr}(Y=1|X)$ 的某种单调变换是 $X$ 的一个线性函数。这也是对真实概率的一个近似。

本章和下一章会介绍跨越线性假设的常用方法。本章的核心思想是在输入变量中添加由 $X$ 生成的新变量，然后在新的衍生输入特征变量的空间上使用线性模型。

假设 $h_m(X):\mathbb{R}^p\mapsto\mathbb{R}$ 为 $X$ 的第 $m$ 个转换变量，$m=1,\dots,M$。则 $X$ 上的 **线性基扩展（linear basis expansion）** 模型可表述为：

{{< math >}}
$$f(X) = \sum_{m=1}^M \beta_m h_m(X) \tag{5.1}$$
{{< /math >}}

这个方法的优点在于确定了 $h_m$ 后，模型对扩展后的这些输入变量为线性，可以用之前介绍的方法拟合。

以下为几个简单并使用广泛的 $h_m$ 例子：

* $h_m(X)=X_m,m=1,\dots,p$  
  等同于原来的线性模型。
* $h_m(X)=X_j^2$ 或 $h_m(X)=X_jX_k$  
  允许在输入变量中添加多项式项，从而在近似中达到更高阶的泰勒展开。但需要注意的是变量个数会随着多项式的级数以指数级增长。$p$ 个变量的完整二次多项式模型需要 $O(p^2)$ 个平方项和交叉项；更一般地，$d$ 阶层多项式则需要 $O(p^d)$ 个展开项。
* $h_m(X)=\log(X_j),\sqrt{X_j},\dots$  
  纳入单个输入变量一些其他的非线性转换。也可使用其他使用了多个输入变量的简单函数，比如 $h_m(X)=\\|X\\|$。
* $h_m(X)=I(L_m\leq X_k<U_m)$  
  $X_k$ 在某个区间上的指示函数。将 $X_k$ 取值划分 $M_k$ 个不重叠的区间，使其对模型结果的影响呈分段常数。

有些场景会使某种类型的函数，比如对数函数或指数函数，成为合适的基函数 $h_m$。不过更常见的情况是利用基扩展方法来实现对 $f(X)$ 更灵活的近似。多项式函数就是后者的一个例子，只不过它在全局的性质也导致了其局限性：为了在某个区间达到某种函数形式而调整的系数，可能会使它在其他区间上的曲线产生非常巨大的波动。本章介绍 **分段多项式（piecewise-polynomials）** 和 **样条（spline）** 这两族使用的方法，它们可表达出局部多项式的性质。还会介绍 **小波（wavelet）** 基函数，它在信号和图像模型中非常有效。这些方法会生成一个基函数的 **字典（dictionary）** $\mathcal{D}$，其中所包含基函数的个数 $\|\mathcal{D}\|$ 通常会非常大，大到无法用样本数据进行拟合。于是在使用字典中函数时，也必须使用某种控制模型复杂度的方法。以下为三个常见的方法：

* 函数限制，即预先限制可选函数的范围。例如限制加性函数，即假设模型需要满足
  {{< math >}}
  $$\begin{align} f(X)
    &=\sum_{j=1}^p f_j(X_j) \\
    &=\sum_{j=1}^p \sum_{m=1}^{M_j} \beta_{jm} h_{jm}(X_j) \tag{5.2}
  \end{align}$$
  {{< /math >}}
  每个元素函数 $f_j$ 中使用的基函数个数 $M_j$ 控制了模型的复杂度。
* 函数选择，即自适应地在字典中寻找对模型拟合贡献最大的那些基函数 $h_m$。[第三章]({{< relref "../ch03/_index.md" >}})中介绍的变量选择方法在此也适用。诸如 CART、MARS、和提升（boosting）方法等分段贪心算法也属于这个范畴。
* 正则化，即在使用整个字典的同时对系数进行约束限制。岭回归就是正则化方法的一个简单例子，而套索回归同时是正则化和变量选择的例子。本章会介绍正则化的一些更复杂的方法。

----------

### 内容概要

{{< list_children >}}

----------

### 本章练习

- 练习 5.1：[第 5.2 节]({{< relref "../ch05/ch05_02.md#练习-51" >}})
- 练习 5.2：
- 练习 5.3：[第 5.2 节]({{< relref "../ch05/ch05_02.md#练习-53" >}})
- 练习 5.4：[第 5.2 节]({{< relref "../ch05/ch05_02.md#练习-54" >}})
- 练习 5.5：
- 练习 5.6：
- 练习 5.7：[第 5.4 节]({{< relref "../ch05/ch05_04.md#练习-57" >}})
- 练习 5.8：
- 练习 5.9：[第 5.4 节]({{< relref "../ch05/ch05_04.md#练习-59" >}})
- 练习 5.10：[第 5.5 节]({{< relref "../ch05/ch05_05.md#练习-510" >}})
- 练习 5.11：[第 5.4 节]({{< relref "../ch05/ch05_04.md#练习-511" >}})
- 练习 5.12：[第 5.6 节]({{< relref "../ch05/ch05_06.md#练习-512" >}})
- 练习 5.13：[第 5.5 节]({{< relref "../ch05/ch05_05.md#练习-513" >}})
- 练习 5.14：[第 5.7 节]({{< relref "../ch05/ch05_07.md#练习-514" >}})
- 练习 5.15：
- 练习 5.16：
- 练习 5.17：
- 练习 5.18：[第 5.9 节]({{< relref "../ch05/ch05_09.md#练习-518" >}})
- 练习 5.19：[第 5.9 节]({{< relref "../ch05/ch05_09.md#练习-519" >}})

[^1]: 加性（additive）模型指的是 $f(X)$ 可分解为若干个元素 $f_j(X)$ 之和的形式。