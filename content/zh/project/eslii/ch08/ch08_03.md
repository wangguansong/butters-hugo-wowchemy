---
title: 8.3 贝叶斯方法
summary: >
  第 267-270 页。如果使用无信息先验分布，则贝叶斯方法会得到与最大似然和自助法一样的结果。

date: 2018-12-14T23:06:00+08:00

weight: 803

---

在以贝叶斯方式的推断中，为数据指定一个给定参数的样本概率模型 $\operatorname{Pr}(\mathbf{Z}|\theta)$（概率密度或质量函数），以及一个体现了从样本学习之前的已知信息的参数先验分布 $\operatorname{Pr}(\theta)$。然后据此计算后验分布：

{{< math >}}
$$\operatorname{Pr}(\theta|\mathbf{Z}) = \frac
  {\operatorname{Pr}(\mathbf{Z}|\theta) \cdot \operatorname{Pr}(\theta)}
  {\int \operatorname{Pr}(\mathbf{Z}|\theta) \cdot \operatorname{Pr}(\theta) d\theta}
\tag{8.23}$$
{{< /math >}}

这代表了从样本学习之后对参数 $\theta$ 的更新信息。可以从这个概率分布进行抽样或者计算其均值、众数和其他汇总统计量，来考察这个后验分布。贝叶斯方法与通常的频率学派（frequentist）方法在推断上有所不同，前者用先验分布来表达从样本学习之前的不确定性，并且从样本学习之后仍然保留用后验分布来表达的不确定性。

后验分布也是预测未来观测样本 $z^\text{new}$ 的预测值的基础，其**预测分布（predictive distribution）** 为：

{{< math >}}
$$\operatorname{Pr}(z^\text{new}|\mathbf{Z}) = \int
\operatorname{Pr}(z^\text{new}|\theta) \cdot \operatorname{Pr}(\theta|\mathbf{Z}) d\theta
\tag{8.24}$$
{{< /math >}}

与之相比，最大似然方法会用在最大似然估计值处的数据密度值 $\operatorname{Pr}(z^\text{new}|\theta)$来预测未来数据。对比表达式 8.24 中的预测分布，最大似然方法没有考虑在估计 $\theta$ 中的不确定性。

下面在之前的函数平滑例子中应用贝叶斯方法。首先从等式 8.5 中的参数模型出发，并暂时假设参数 $\sigma^2$ 已知。将观测到的特征值 $x_1$，$x_2$，……，$x_N$ 视为固定值，所以数据中的随机性仅来自于 $y$ 在其均值 $\mu(x)$ 附近的变动。

{{< math >}}
$$\begin{align}
Y &= \mu(X) + \varepsilon; \varepsilon \sim \mathcal{N}(0, \sigma^2) \\
\mu(x) &= \sum_{j=1}^7 \beta_j h_j(x)
\tag{8.5}\end{align}$$
{{< /math >}}

第二个要确定的是先验分布。在函数上的概率分布的定义比较复杂，一个方法是使用一个高斯过程的先验分布，并指定其中任两个函数值 $\mu(x)$ 和 $\mu(x')$ 之间的先验协方差（Wahba, 1990；Neal, 1996）。

在这个例子中采用更简单的方式：考虑一个 $\mu(x)$ 的有限 B-样条基函数集，我们可以为系数 $\beta$ 选定一个先验分布，而这隐含地定义了 $\mu(x)$ 的先验分布。我们选定的是一个均值为零的高斯先验分布：

{{< math >}}
$$\beta \sim \mathcal{N}(0, \tau\mathbf{\Sigma}) \tag{8.25}$$
{{< /math >}}

后续会讨论对先验相关性矩阵 $\mathbf{\Sigma}$ 和方差 $\tau$ 的选择。因此隐含的 $\mu(x)$ 过程的先验分布是高斯分布，其协方差核函数为：

{{< math >}}
$$\begin{align} K(x,x')
&= \operatorname{cov}[\mu(x), \mu(x')] \\
&= \tau \cdot h(x)^T \mathbf{\Sigma} h(x')
\tag{8.26}\end{align}$$
{{< /math >}}

$\beta$ 的后验分布也是高斯分布，其均值和协方差矩阵为：

{{< math >}}
$$\begin{align}
\operatorname{E}(\beta|\mathbf{Z}) &=
  \left( \mathbf{H}^T\mathbf{H} +
  \frac{\sigma^2}{\tau}\mathbf{\Sigma}^{-1} \right)^{-1}
  \mathbf{H}^T \mathbf{y} \\
\text{cov}(\beta|\mathbf{Z}) &=
  \left( \mathbf{H}^T\mathbf{H} +
  \frac{\sigma^2}{\tau}\mathbf{\Sigma}^{-1} \right)^{-1} \sigma^2
\end{align}\tag{8.27}$$
{{< /math >}}

相应的 $\mu(x)$ 拟合值的后验分布的均值和协方差为：

{{< math >}}
$$\begin{align}
\operatorname{E}(\mu(x)|\mathbf{Z}) &=
  h(x)^T \left( \mathbf{H}^T\mathbf{H} +
  \frac{\sigma^2}{\tau}\mathbf{\Sigma}^{-1} \right)^{-1}
  \mathbf{H}^T \mathbf{y} \\
\text{cov}[\mu(x),\mu(x')|\mathbf{Z}] &=
  h(x)^T \left( \mathbf{H}^T\mathbf{H} +
  \frac{\sigma^2}{\tau}\mathbf{\Sigma}^{-1} \right)^{-1} h(x')\sigma^2
\end{align}\tag{8.28}$$
{{< /math >}}

那么如何选择先验相关性矩阵 $\mathbf{\Sigma}$？在一些场景中，可以通过关于参数的学科领域知识来选择先验分布。在此例中，我们认定函数 $\mu(x)$ 是平滑的，并且通过用 B-样条的平滑低维基函数来表述 $\mu$ 的方式来确保这一点。因此我们可以用单位矩阵作为先验相关性矩阵 $\mathbf{\Sigma}=\mathbf{I}$。当基函数个数比较大时，这个条件可能不充分，可对 $\mathbf{\Sigma}$ 进行约束来达到更多的平滑性，这正是平滑样条模型（[第 5.8.1 节]({{< relref "../ch05/ch05_08.md" >}})）。

{{< figure
  id="f0803"
  src="https://public.guansong.wang/eslii/ch08/eslii_fig_08_03.png"
  title="**图 8.3**：函数平滑例子：函数 $\mu(x)$ 的高斯先验分布的十个抽样。"
>}}

{{< figure
  id="f0804"
  src="https://public.guansong.wang/eslii/ch08/eslii_fig_08_04.png"
  title="**图 8.4**：函数平滑例子：函数 $\mu(x)$ 的后验分布的十个抽样，两图中对应了不同的先验方差 $\tau$。紫色曲线为后验分布均值。"
>}}

[图 8.3](#figure-f0803) 展示了从相应的 $\mu(x)$ 的先验分布的十个抽样。根据等式 8.27 确定的后验分布生成参数值 $\beta'$，从而生成函数 $\mu(x)$ 相应的后验取值，$\mu'(x)=\sum_1^7\beta'_jh_j(x)$。[图 8.4](#figure-f0804) 中展示了这样生成的十个后验曲线。图中使用了两个先验方差 $\tau$ 取值，$1$ 和 $1000$。注意右图与第 263 页中[图 8.2]({{< relref "../ch08/ch08_02.md" >}})的左下图中的自助分布比较相似。这个相似性不是偶然的。随着 $\tau\rightarrow\infty$，后验分布（等式 8.27）与自助分布（等式 8.7）趋于一致。另一方面，当 $\tau=1$ 时，[图 8.4](#figure-f0804) 左图中的后验曲线 $\mu(x)$ 要比自助曲线更平滑，这是因为其在平滑性上赋予了更大的先验权重。

在 $\tau\rightarrow\infty$ 条件下的分布（表达式 8.25）被称为 $\theta$ 的 **无信息先验（noninformative prior）** 分布。在高斯模型中，最大似然和参数自助法会倾向于与用自由参数无信息先验分布的贝叶斯方法结果一致。这个一致性是由于当先验密度函数是一个常数，后验分布与似然度成正比。这两者的对应关系也可推广到非参数场景中，非参数自助法会近似于使用无信息先验的贝叶斯方法；更多细节见[第 8.4 节]({{< relref "../ch08/ch08_04.md" >}})。

然而上面的论述中有一些从贝叶斯的角度看起来不合理的处理方式。具体来说，使用了一个常数作为 $\sigma^2$ 的无信息先验分布，并且在后验分布中用最大似然估计 $\hat{\sigma}^2$ 替换。更规范的贝叶斯方法也会选择 $\sigma$ 的先验分布（通常是 $g(\sigma)\propto 1/\sigma$），计算 $\mu(x)$ 和 $\sigma$ 的联合后验分布，然后再对 $\sigma$ 积分，而不只是对后验分布进行最大化（“最大后验概率”或“MAP”估计）。