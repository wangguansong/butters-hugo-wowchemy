---
title: 18.2 Diagonal Linear Discriminant Analysis and Nearest Shrunken Centroids
summary: >
  第 651-654 页。

date: 2022-04-01T11:00:00+08:00
draft: true 

linktitle: 18.2 对角 LDA 和 NSC
weight: 1802

---

Gene expression arrays are an important new technology in biology, and
are discussed in Chapters 1 and 14. The data in our next example form
a matrix of 2308 genes (columns) and 63 samples (rows), from a set of
microarray experiments. Each expression value is a log-ratio log(R/G). R
is the amount of gene-specific RNA in the target sample that hybridizes
to a particular (gene-specific) spot on the microarray, and G is the corre-
sponding amount of RNA from a reference sample. The samples arose from
small, round blue-cell tumors (SRBCT) found in children, and are classified
into four major types: BL (Burkitt lymphoma), EWS (Ewing’s sarcoma),
NB (neuroblastoma), and RMS (rhabdomyosarcoma). There is an addi-
tional test data set of 20 observations. We will not go into the scientific
background here.

Since p ≫ N , we cannot fit a full linear discriminant analysis (LDA) to
the data; some sort of regularization is needed. The method we describe
here is similar to the methods of Section 4.3.1, but with important modifi-
cations that achieve feature selection. The simplest form of regularization
assumes that the features are independent within each class, that is, the
within-class covariance matrix is diagonal. Despite the fact that features
will rarely be independent within a class, when p ≫ N we don’t have
enough data to estimate their dependencies. The assumption of indepen-
dence greatly reduces the number of parameters in the model and often
results in an effective and interpretable classifier.

Thus we consider the diagonal-covariance LDA rule for classifying the
classes. The discriminant score [see (4.12) on page 110] for class k is

$$\tag{18.2}$$

Here x ∗ = (x ∗ 1 , x ∗ 2 , . . . , x ∗ p ) T is a vector of expression values for a test ob-
servation, s P j is the pooled within-class standard deviation of the jth gene,
and x̄ kj = i∈C k x ij /N k is the mean of the N k values for gene j in class
k, with C k being the index set for class k. We call x̃ k = (x̄ k1 , x̄ k2 , . . . x̄ kp ) T
the centroid of class k. The first part of (18.2) is simply the (negative)
standardized squared distance of x ∗ to the kth centroid. The P second part
is a correction based on the class prior probability π k , where k=1 π k = 1.
The classification rule is then

$$\tag{18.3}$$

We see that the diagonal LDA classifier is equivalent to a nearest centroid
classifier after appropriate standardization. It is also a special case of the
naive-Bayes classifier, as described in Section 6.6.3. It assumes that the
features in each class have independent Gaussian distributions with the
same variance.

The diagonal LDA classifier is often effective in high dimensional set-
tings. It is also called the “independence rule” in Bickel and Levina (2004),
who demonstrate theoretically that it will often outperform standard lin-
ear discriminant analysis in high-dimensional problems. Here the diagonal
LDA classifier yielded five misclassification errors for the 20 test samples.
One drawback of the diagonal LDA classifier is that it uses all of the fea-
tures (genes), and hence is not convenient for interpretation. With further
regularization we can do better—both in terms of test error and inter-
pretability.

We would like to regularize in a way that automatically drops out fea-
tures that are not contributing to the class predictions. We can do this
by shrinking the classwise mean toward the overall mean, for each feature
separately. The result is a regularized version of the nearest centroid clas-
sifier, or equivalently a regularized version of the diagonal-covariance form
of LDA. We call the procedure nearest shrunken centroids (NSC).

The shrinkage procedure is defined as follows. Let

$$\tag{18.4}$$

where x̄ j is the overall mean for gene j, m 2 k = 1/N k − 1/N and s 0 is a
small positive constant, typically chosen to be the median of the s j values.
This constant guards against large d kj values that arise from expression
values near zero. With constant within-class variance σ 2 , the variance of
the contrast x̄ kj − x̄ j in the numerator is m 2 k σ 2 , and hence the form of the
standardization in the denominator. We shrink the d kj toward zero using
soft thresholding

$$\tag{18.5}$$

{{< figure
  id="f1802"
  src="https://public.guansong.wang/eslii/ch18/eslii_fig_18_02.png"
  title="**图 18.02**："
>}}

see Figure 18.2. Here ∆ is a parameter to be determined; we used 10-fold
cross-validation in the example (see the top panel of Figure 18.4). Each d kj
is reduced by an amount ∆ in absolute value, and is set to zero if its value
is less than zero. The soft-thresholding function is shown in Figure 18.2;
the same thresholding is applied to wavelet coefficients in Section 5.9. An
alternative is to use hard thresholding

$$\tag{18.6}$$

we prefer soft-thresholding, as it is a smoother operation and typically
works better. The shrunken versions of x̄ kj are then obtained by reversing
the transformation in (18.4):

$$\tag{18.7}$$

We then use the shrunken centroids x̄ ′ kj in place of the original x̄ kj in the
discriminant score (18.2). The estimator (18.7) can also be viewed as a
lasso-style estimator for the class means (Exercise 18.2).

Notice that only the genes that have a nonzero d ′ kj for at least one of the
classes play a role in the classification rule, and hence the vast majority
of genes can often be discarded. In this example, all but 43 genes were
discarded, leaving a small interpretable set of genes that characterize each
class. Figure 18.3 represents the genes in a heatmap.

{{< figure
  id="f1803"
  src="https://public.guansong.wang/eslii/ch18/eslii_fig_18_03.png"
  title="**图 18.03**："
>}}

{{< figure
  id="f1804"
  src="https://public.guansong.wang/eslii/ch18/eslii_fig_18_04.png"
  title="**图 18.04**："
>}}

Figure 18.4 (top panel) demonstrates the effectiveness of the shrinkage.
With no shrinkage we make 5/20 errors on the test data, and several errors
on the training and CV data. The shrunken centroids achieve zero test er-
rors for a fairly broad band of values for ∆. The bottom panel of Figure 18.4
shows the four centroids for the SRBCT data (gray), relative to the overall
centroid. The blue bars are shrunken versions of these centroids, obtained
by soft-thresholding the gray bars, using ∆ = 4.3. The discriminant scores
(18.2) can be used to construct class probability estimates:

$$\tag{18.8}$$


These can be used to rate the classifications, or to decide not to classify a
particular sample at all.

Note that other forms of feature selection can be used in this setting,
including hard thresholding. Fan and Fan (2008) show theoretically the
importance of carrying out some kind of feature selection with diagonal
linear discriminant analysis in high-dimensional problems.