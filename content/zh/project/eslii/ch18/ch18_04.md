---
title: 18.4 Linear Classifiers with L1 Regularization
summary: >
  第 661-668 页。

date: 2022-04-01T11:00:00+08:00
math: true

type: book
weight: 1804

authors: ["Butters"]
tags: ["译文"]
categories: ["统计学习基础（译注）"]

---

The methods of Section 18.3 use an L 2 penalty to regularize their pa-
rameters, just as in ridge regression. All of the estimated coefficients are
nonzero, and hence no feature selection is performed. In this section we dis-
cuss methods that use L 1 penalties instead, and hence provide automatic
feature selection.

Recall the lasso of Section 3.4.2,

$$\min_\beta \frac{1}{2} \sum_{i=1}^N
(y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2 +
\lambda \sum_{j=1}^p |\beta_j| \tag{18.18}$$

which we have written in the Lagrange form (3.52). As discussed there, the
use of the L 1 penalty causes a subset of the solution coefficients β̂ j to be
exactly zero, for a sufficiently large value of the tuning parameter λ.

In Section 3.8.1 we discussed the LARS algorithm, an efficient procedure
for computing the lasso solution for all λ. When p > N (as in this chapter),
as λ approaches zero, the lasso fits the training data exactly. In fact, by
convex duality one can show that when p > N the number of non-zero
coefficients is at most N for all values of λ (Rosset and Zhu, 2007, for
example). Thus the lasso provides a (severe) form of feature selection.

Lasso regression can be applied to a two-class classification problem by
coding the outcome ±1, and applying a cutoff (usually 0) to the predictions.
For more than two classes, there are many possible approaches, including
the ova and ovo methods discussed in Section 18.3.3. We tried the ova-
approach on the cancer data in Section 18.3. The results are shown in
line (4) of Table 18.1. Its performance is among the best.

A more natural approach for classification problems is to use the lasso
penalty to regularize logistic regression. Several implementations have been
proposed in the literature, including path algorithms similar to LARS (Park
and Hastie, 2007). Because the paths are piecewise smooth but nonlinear,
exact methods are slower than the LARS algorithm, and are less feasible
when p is large.

Friedman et al. (2010) provide very fast algorithms for fitting L 1 -pen-
alized logistic and multinomial regression models. They use the symmetric
multinomial logistic regression model as in (18.10) in Section 18.3.2, and
maximize the penalized log-likelihood

$$\max_{\\{\beta_{0k},\beta_k\in\mathbb{R}^p\\}\_1^K} \left[
  \sum_{i=1}^N \log \operatorname{Pr}(g_i|x_i) -
  \lambda \sum_{k=1}^K \sum_{j=1}^p |\beta_{kj}|
\right]\tag{18.19}$$

compare with (18.11). Their algorithm computes the exact solution at a
pre-chosen sequence of values for λ by cyclical coordinate descent (Sec-
tion 3.8.6), and exploits the fact that solutions are sparse when p ≫ N ,
as well as the fact that solutions for neighboring values of λ tend to be
very similar. This method was used in line (7) of Table 18.1, with the over-
all tuning parameter λ chosen by cross-validation. The performance was
similar to that of the best methods, except here the automatic feature se-
lection chose 269 genes altogether. A similar approach is used in Genkin
et al. (2007); although they present their model from a Bayesian point of
view, they in fact compute the posterior mode, which solves the penalized
maximum-likelihood problem.

In genomic applications, there are often strong correlations among the
variables; genes tend to operate in molecular pathways. The lasso penalty
is somewhat indifferent to the choice among a set of strong but corre-
lated variables (Exercise 3.28). The ridge penalty, on the other hand, tends
to shrink the coefficients of correlated variables toward each other (Exer-
cise 3.29 on page 99). The elastic net penalty (Zou and Hastie, 2005) is a
compromise, and has the form

$$\sum_{j=1}^p (\alpha|\beta_j| + (1-\alpha) \beta_j^2) \tag{18.20}$$

The second term encourages highly correlated features to be averaged, while
the first term encourages a sparse solution in the coefficients of these aver-
aged features. The elastic net penalty can be used with any linear model,
in particular for regression or classification.

Hence the multinomial problem above with elastic-net penalty becomes

$$\max_{\\{\beta_{0k},\beta_k\in\mathbb{R}^p\\}\_1^K} \left[
  \sum_{i=1}^N \log \operatorname{Pr}(g_i|x_i) -
  \lambda \sum_{k=1}^K \sum_{j=1}^p (\alpha|\beta_{kj}| + (1-\alpha)\beta_{kj}^2)
\right]$$
$$\tag{18.21}$$

The parameter α determines the mix of the penalties, and is often pre-
chosen on qualitative grounds. The elastic net can yield more that N non-
zero coefficients when p > N , a potential advantage over the lasso. Line
(8) in Table 18.1 uses this model, with α and λ chosen by cross-validation.
We used a sequence of 20 values of α between 0.05 and 1.0, and a 100
values of λ uniform on the log scale covering the entire range. Values of
α ∈ [0.75, 0.80] gave the minimum CV error, with values of λ < 0.001 for all
tied solutions. Although it has the lowest test error among all methods, the
margin is small and not significant. Interestingly, when CV is performed
separately for each value of α, a minimum test error of 8.8 is achieved at
α = 0.10, but this is not the value chosen in the two-dimensional CV.

{{< figure
  id="f1805"
  src="https://public.guansong.wang/eslii/ch18/eslii_fig_18_05.png"
  title="**图 18.05**："
>}}

Figure 18.5 shows the lasso and elastic-net coefficient paths on the two-
class leukemia data (Golub et al., 1999). There are 7129 gene-expression
measurements on 38 samples, 27 of them in class ALL (acute lymphocytic
leukemia), and 11 in class AML (acute myelogenous leukemia). There is
also a test set with 34 samples (20, 14). Since the data are linearly separa-
ble, the solution is undefined at λ = 0 (Exercise 18.11), and degrades for
very small values of λ. Hence the paths have been truncated as the fitted
probabilities approach 0 and 1. There are 19 non-zero coefficients in the
left plot, and 39 in the right.

{{< figure
  id="f1806"
  src="https://public.guansong.wang/eslii/ch18/eslii_fig_18_06.png"
  title="**图 18.06**："
>}}

Figure 18.6 (left panel) shows the misclas-
sification errors for the lasso logistic regression on the training and test
data, as well as for 10-fold cross-validation on the training data. The right
panel uses binomial deviance to measure errors, and is much smoother. The
small sample sizes lead to considerable sampling variance in these curves,
even though individual curves are relatively smooth (see, for example, Fig-
ure 7.1 on page 220). Both of these plots suggest that the limiting solution
λ ↓ 0 is adequate, leading to 3/34 misclassifications in the test set. The
corresponding figures for the elastic net are qualitatively similar and are
not shown.

For p ≫ N , the limiting coefficients diverge for all regularized logistic
regression models, so in practical software implementations a minimum
value for λ > 0 is either explicitly or implicitly set. However, renormalized
versions of the coefficients converge, and these limiting solutions can be
thought of as interesting alternatives to the linear optimal separating hy-
perplane (SVM). With α = 0 the limiting solution coincides with the SVM
(see end of Section 18.3.2), but all the 7129 genes are selected. With α = 1,
the limiting solution coincides with an L 1 separating hyperplane (Rosset
et al., 2004a), and includes at most 38 genes. As α decreases from 1, the
elastic-net solutions include more genes in the separating hyperplane.

#### 18.4.1 Application of Lasso to Protein Mass Spectroscopy

Protein mass spectrometry has become a popular technology for analyzing
the proteins in blood, and can be used to diagnose a disease or understand
the processes underlying it.

For each blood serum sample i, we observe the intensity x ij for many
time of flight values t j . This intensity is related to the number of particles
observed to take approximately t j time to pass from the emitter to the
detector during a cycle of operation of the machine. The time of flight has
a known relationship to the mass over charge ratio (m/z) of the constituent
proteins in the blood. Hence the identification of a peak in the spectrum
at a certain t j tells us that there is a protein with a corresponding mass
and charge. The identity of this protein can then be determined by other
means.

{{< figure
  id="f1807"
  src="https://public.guansong.wang/eslii/ch18/eslii_fig_18_07.png"
  title="**图 18.07**："
>}}

Figure 18.7 shows an example taken from Adam et al. (2003). It shows
the average spectra for healthy patients and those with prostate cancer.
There are 16,898 m/z sites in total, ranging in value from 2000 to 40,000.
The full dataset consists of 157 healthy patients and 167 with cancer, and
the goal is to find m/z sites that discriminate between the two groups.
This is an example of functional data; the predictors can be viewed as a
function of m/z. There has been much interest in this problem in the past
few years; see e.g. Petricoin et al. (2002).

The data were first standardized (baseline subtraction and normaliza-
tion), and we restricted attention to m/z values between 2000 and 40,000
(spectra outside of this range were not of interest). We then applied near-
est shrunken centroids and lasso regression to the data, with the results for
both methods shown in Table 18.2.

| Method | Test Errors/108 | number of Sites |
|--------|-----------------|-----------------|
| 1. Nearest shrunken centroids | 34 | 459 |
| 2. Lasso | 22 | 113 |
| 3. Lasso on peaks | 28 | 35 |

**表 18.2：**

By fitting harder to the data, the lasso achieves a considerably lower
test error rate. However, it may not provide a scientifically useful solu-
tion. Ideally, protein mass spectrometry resolves a biological sample into
its constituent proteins, and these should appear as peaks in the spectra.
The lasso doesn’t treat peaks in any special way, so not surprisingly only
some of the non-zero lasso weights were situated near peaks in the spectra.
Furthermore, the same protein may yield a peak at slightly different m/z
values in different spectra. In order to identify common peaks, some kind
of m/z warping is needed from sample to sample.

To address this, we applied a standard peak-extraction algorithm to each
spectrum, yielding a total of 5178 peaks in the 217 training spectra. Our
idea was to pool the collection of peaks from all patients, and hence con-
struct a set of common peaks. For this purpose, we applied hierarchical
clustering to the positions of these peaks along the log m/z axis. We cut
the resulting dendrogram horizontally at height log(0.005) 3 , and computed
averages of the peak positions in each resulting cluster. This process yielded
728 common clusters and their corresponding peak centers.

Given these 728 common peaks, we determined which of these were
present in each individual spectrum, and if present, the height of the peak.
A peak height of zero was assigned if that peak was not found. This pro-
duced a 217 × 728 matrix of peak heights as features, which was used in a
lasso regression. We scored the test spectra for the same 728 peaks.

The prediction results for this application of the lasso to the peaks are
shown in the last line of Table 18.2: it does fairly well, but not as well
as the lasso on the raw spectra. However, the fitted model may be more
useful to the biologist as it yields 35 peak positions for further study. On
the other hand, the results suggest that there may be useful discriminatory
information between the peaks of the spectra, and the positions of the lasso
sites from line (2) of the table also deserve further examination.

#### 18.4.2 The Fused Lasso for Functional Data

In the previous example, the features had a natural order, determined by
the mass-to-charge ratio m/z. More generally, we may have functional fea-
tures x i (t) that are ordered according to some index variable t. We have
already discussed several approaches for exploiting such structure.

We can represent x i (t) by their coefficients in a basis of functions in t,
such as splines, wavelets or Fourier bases, and then apply a regression using
these coefficients as predictors. Equivalently, one can instead represent the
coefficients of the original features in these bases. These approaches are
described in Section 5.3.

In the classification setting, we discuss the analogous approach of penal-
ized discriminant analysis in Section 12.6. This uses a penalty that explicitly
controls the resulting smoothness of the coefficient vector.

The above methods tend to smooth the coefficients uniformly. Here we
present a more adaptive strategy that modifies the lasso penalty to take
into account the ordering of the features. The fused lasso (Tibshirani et
al., 2005) solves

$$\max_{\beta\in\mathbb{R}^p} \left\\{
  \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 +
  \lambda_1 \sum_{j=1}^p |\beta_j| +
  \lambda_2 \sum_{j=1}^{p-1} |\beta_{j+1}-\beta_j|
\right\\}$$
$$\tag{18.22}$$

This criterion is strictly convex in β, so a unique solution exists. The first
penalty encourages the solution to be sparse, while the second encourages
it to be smooth in the index j.

The difference penalty in (18.22) assumes an uniformly spaced index j. If
instead the underlying index variable t has nonuniform values t j , a natural
generalization of (18.22) would be based on divided differences

$$\lambda_2 \sum_{j=1}^{p-1}
\frac{|\beta_{j+1} - \beta_j|}{|t_{j+1} - t_j|} \tag{18.23}$$

This amounts to having a penalty modifier for each of the terms in the
series.

A particularly useful special case arises when the predictor matrix X =
I N , the N × N identity matrix. This is a special case of the fused lasso,
used to approximate a sequence {y i } N1 . The fused lasso signal approximator
solves

$$\max_{\beta\in\mathbb{R}^p} \left\\{
  \sum_{i=1}^N (y_i - \beta_0 - \beta_i)^2 +
  \lambda_1 \sum_{i=1}^N |\beta_i| +
  \lambda_2 \sum_{i=1}^{N-1} |\beta_{i+1}-\beta_i|
\right\\}$$
$$\tag{18.24}$$

{{< figure
  id="f1808"
  src="https://public.guansong.wang/eslii/ch18/eslii_fig_18_08.png"
  title="**图 18.08**："
>}}

Figure 18.8 shows an example taken from Tibshirani and Wang (2007). The
data in the panel come from a Comparative Genomic Hybridization (CGH)
array, measuring the approximate log (base-two) ratio of the number of
copies of each gene in a tumor sample, as compared to a normal sample.
The horizontal axis represents the chromosomal location of each gene. The
idea is that in cancer cells, genes are often amplified (duplicated) or deleted,
and it is of interest to detect these events. Furthermore, these events tend
to occur in contiguous regions. The smoothed signal estimate from the
fused lasso signal approximator is shown in dark red (with appropriately
chosen values for λ 1 and λ 2 ). The significantly nonzero regions can be used
to detect locations of gains and losses of genes in the tumor.

There is also a two-dimensional version of the fused lasso, in which the
parameters are laid out in a grid of pixels, and a penalty is applied to the
first differences to the left, right, above and below the target pixel. This
can be useful for denoising or classifying images. Friedman et al. (2007)
develop fast generalized coordinate descent algorithms for the one- and
two-dimensional fused lasso.