---
title: 18.5 Classification When Features are Unavailable
summary: >
  第 668-674 页。

date: 2022-04-01T11:00:00+08:00
math: true

type: book
weight: 1805

authors: ["Butters"]
tags: ["译文"]
categories: ["统计学习基础（译注）"]

---

In some applications the objects under study are more abstract in nature,
and it is not obvious how to define a feature vector. As long as we can fill
in an N × N proximity matrix of similarities between pairs of objects in our
database, it turns out we can put to use many of the classifiers in our arsenal
by interpreting the proximities as inner-products. Protein structures fall
into this category, and we explore an example in Section 18.5.1 below.

In other applications, such as document classification, feature vectors are
available but can be extremely high-dimensional. Here we may not wish
to compute with such high-dimensional data, but rather store the inner-
products between pairs of documents. Often these inner-products can be
approximated by sampling techniques.

Pairwise distances serve a similar purpose, because they can be turned
into centered inner-products. Proximity matrices are discussed in more de-
tail in Chapter 14.

### 18.5.1 Example: String Kernels and Protein Classification

An important problem in computational biology is to classify proteins into
functional and structural classes based on their sequence similarities. Pro-
tein molecules are strings of amino acids, differing in both length and com-
position. In the example we consider, the lengths vary between 75–160
amino-acid molecules, each of which can be one of 20 different types, labeled
using letters. Here are two examples, of length 110 and 153, respectively:

IPTSALVKETLALLSTHRTLLIANETLRIPVPVHKNHQLCTEEIFQGIGTLESQTVQGGTV
ERLFKNLSLIKKYIDGQKKKCGEERRRVNQFLDYLQEFLGVMNTEWI

PHRRDLCSRSIWLARKIRSDLTALTESYVKHQGLWSELTEAERLQENLQAYRTFHVLLA
RLLEDQQVHFTPTEGDFHQAIHTLLLQVAAFAYQIEELMILLEYKIPRNEADGMLFEKK
LWGLKVLQELSQWTVRSIHDLRFISSHQTGIP

There have been many proposals for measuring the similarity between a
pair of protein molecules. Here we focus on a measure based on the count
of matching substrings (Leslie et al., 2004), such as the LQE above.

To construct our features, we count the number of times that a given
sequence of length m occurs in our string, and we compute this number
for all possible sequences of length m. Formally, for a string x, we define a
feature map

$$\Phi_m(x) = \\{\phi_a(x)\\}\_{a\in\mathcal{A}\_m} \tag{18.25}$$

where A m is the set of subsequences of length m, and φ a (x) is the number
of times that “a” occurs in our string x. Using this, we define the inner
product

$$K_m(x_1, x_2) = \langle \Phi_m(x_1), \Phi_m(x_2) \rangle \tag{18.26}$$

which measures the similarity between the two strings x 1 , x 2 . This can be
used to drive, for example, a support vector classifier for classifying strings
into different protein classes.

Now the number of possible sequences a is |A m | = 20 m , which can be
very large for moderate m, and the vast majority of the subsequences do
not match the strings in our training set. It turns out that we can compute
the N × N inner-product matrix or string kernel K m (18.26) efficiently
using tree-structures, without actually computing the individual vectors.
This methodology, and the data to follow, come from Leslie et al. (2004). 4

The data consist of 1708 proteins in two classes— negative (1663) and
positive (45). The two examples above, which we will call “x 1 ” and “x 2 ”,
are from this set. We have marked the occurrences of subsequence LQE ,
which appears in both proteins. There are 20 3 possible subsequences, so
Φ 3 (x) will be a vector of length 8000. For this example φ LQE (x 1 ) = 1 and
φ LQE (x 2 ) = 2.

{{< figure
  id="f1809"
  src="https://public.guansong.wang/eslii/ch18/eslii_fig_18_09.png"
  title="**图 18.09**："
>}}

Using software from Leslie et al. (2004), we computed the string kernel
for m = 4, which was then used in a support vector classifier to find the
maximal margin solution in this 20 4 = 160, 000-dimensional feature space.
We used 10-fold cross-validation to compute the SVM predictions on all of
the training data. The orange curve in Figure 18.9 shows the cross-validated
ROC curve for the support vector classifier, computed by varying the cut-
point on the real-valued predictions from the cross-validated support vector
classifier. The area under the curve is 0.84. Leslie et al. (2004) show that
the string kernel method is competitive with, but perhaps not as accurate
as, more specialized methods for protein string matching.

Many other classifiers can be computed using only the information in the
kernel matrix; some details are given in the next section. The results for
the nearest centroid classifier (green), and distance-weighted one-nearest
neighbors (blue) are shown in Figure 18.9. Their performance is similar to
that of the support vector classifier.

### 18.5.2 Classification and Other Models Using Inner-Product Kernels and Pairwise Distances

There are a number of other classifiers, besides the support-vector ma-
chine, that can be implemented using only inner-product matrices. This
also implies they can be “kernelized” like the SVM.

An obvious example is nearest-neighbor classification, since we can trans-
form pairwise inner-products to pairwise distances:

$$\\| x_i - x_{i'} \\|^2 = \langle x_i, x_i \rangle +
\langle x_{i'}, x_{i'} \rangle - 2 \langle x_i, x_{i'} \rangle \tag{18.27}$$

A variation of 1-NN classification is used in Figure 18.9, which produces
a continuous discriminant score needed to construct a ROC curve. This
distance-weighted 1-NN makes use of the distance of a test points to the
closest member of each class; see Exercise 18.14.
Nearest-centroid classification follows easily as well. For training pairs
(x i , g i ), i = 1, . . . , N , a test point x 0 , and class centroids x̄ k , k = 1, . . . , K
we can write

$$\\| x_0 - \bar{x}\_k \\|^2 = \langle x_0, x_0 \rangle -
\frac{2}{N_k} \sum_{g_i=k} \langle x_0, x_i \rangle -
\frac{1}{N_k^2} \sum_{g_i=k} \sum_{g_{i'}=k} \langle x_i, x_{i'} \rangle $$
$$\tag{18.28}$$

Hence we can compute the distance of the test point to each of the cen-
troids, and perform nearest centroid classification. This also implies that
methods like K-means clustering can also be implemented, using only the
inner products of the data points.

Logistic and multinomial regression with quadratic regularization can
also be implemented with inner-product kernels; see Section 12.3.3 and
Exercise 18.13. Exercise 12.10 derives linear discriminant analysis using an
inner-product kernel.

Principal components can be computed using inner-product kernels as
well; since this is frequently useful, we give some details. Suppose first
that we have a centered data matrix X, and let X = UDV T be its SVD
(18.12). Then Z = UD is the matrix of principal component variables (see
Section 14.5.1). But if K = XX T , then it follows that K = UD 2 U T , and
hence we can compute Z from the eigen decomposition of K. If X is not
centered, then we can center it using X̃ = (I − M)X, where M = N 1 11 T
is the mean operator. Thus we compute the eigenvectors of the double-
centered kernel (I − M)K(I − M) for the principal components from an
uncentered inner-product matrix. Exercise 18.15 explores this further, and
Section 14.5.4 discusses in more detail kernel PCA for general kernels, such
as the radial kernel used in SVMs.

If instead we had available only the pairwise (squared) Euclidean dis-
tances between observations,

$$\Delta_{ii'}^2 = \\| x_i - x_{i'} \\|^2 \tag{18.29}$$

it turns out we can do all of the above as well. The trick is to convert the
pairwise distances to centered inner-products, and then proceed as before.
We write

$$\Delta_{ii'}^2 = \\| x_i - \bar{x} \\|^2 + \\| x_{i'} - \bar{x} \\|^2 -
2 \langle x_i - \bar{x}, x_{i'} - \bar{x} \rangle \tag{18.30}$$

Defining B = {−∆ 2 ii ′ /2}, we double center B:

$$\tilde{\mathbf{K}} =
(\mathbf{I} - \mathbf{M}) \mathbf{B} (\mathbf{I} - \mathbf{M}) \tag{18.31}$$

it is easy to check that K̃ ii ′ = hx i − x̄, x i ′ − x̄i, the centered inner-product
matrix.

Distances and inner-products also allow us to compute the medoid in each
class—the observation with smallest average distance to other observations
in that class. This can be used for classification (closest medoids), as well as
to drive k-medoids clustering (Section 14.3.10). With abstract data objects
like proteins, medoids have a practical advantage over means. The medoid is
one of the training examples, and can be displayed. We tried closest medoids
in the example in the next section (see Table 18.3), and its performance is
disappointing.

| Method | CV Error (SE) |
|--------|---------------|
| 1. Nearest shrunken centroids | 0.17(0.05) |
| 2. SVM | 0.23(0.06) |
| 3. Nearest medoids | 0.65(0.07) |
| 4. 1-NN | 0.44(0.07) |
| 5. Nearest centroids | 0.29(0.07) |

**表 18.3：**

It is useful to consider what we cannot do with inner-product kernels and
distances:

• We cannot standardize the variables; standardization significantly im-
proves performance in the example in the next section.
• We cannot assess directly the contributions of individual variables.
In particular, we cannot perform individual t-tests, fit the nearest
shrunken centroids model, or fit any model that uses the lasso penalty.
• We cannot separate the good variables from the noise: all variables get
an equal say. If, as is often the case, the ratio of relevant to irrelevant
variables is small, methods that use kernels are not likely to work as
well as methods that do feature selection.

### 18.5.3 Example: Abstracts Classification

This somewhat whimsical example serves to illustrate a limitation of ker-
nel approaches. We collected the abstracts from 48 papers, 16 each from
Bradley Efron (BE), Trevor Hastie and Rob Tibshirani (HT) (frequent co-
authors), and Jerome Friedman (JF). We extracted all unique words from
these abstracts, and defined features x ij to be the number of times word
j appears in abstract i. This is the so-called bag of words representation.
Quotations, parentheses and special characters were first removed from the
abstracts, and all characters were converted to lower case. We also removed
the word “we”, which could unfairly discriminate HT abstracts from the
others.

There were 4492 total words, of which p = 1310 were unique. We sought
to classify the documents into BE, HT or JF on the basis of the features
x ij . Although it is artificial, this example allows us to assess the possible
degradation in performance if information specific to the raw features is
not used.

{{< figure
  id="f1810"
  src="https://public.guansong.wang/eslii/ch18/eslii_fig_18_10.png"
  title="**图 18.10**："
>}}

We first applied the nearest shrunken centroid classifier to the data, using
10-fold cross-validation. It essentially chose no shrinkage, and so used all the
features; see the first line of Table 18.3. The error rate is 17%; the number
of features can be reduced to about 500 without much loss in accuracy.
Note that the nearest shrunken classifier requires the raw feature matrix
X in order to standardize the features individually. Figure 18.10 shows the
top 20 discriminating words, with a positive score indicating that a word
appears more in that class than in the other classes.

Some of these terms make sense: for example “frequentist” and “Bayesian”
reflect Efron’s greater emphasis on statistical inference. However, many oth-
ers are surprising, and reflect personal writing styles: for example, Fried-
man’s use of “presented” and HT’s use of “propose”.

We then applied the support vector classifier with linear kernel and no
regularization, using the “all pairs” (ovo) method to handle the three
classes (regularization of the SVM did not improve its performance). The
result is shown in Table 18.3. It does somewhat worse than the nearest
shrunken centroid classifier.

As mentioned, the first line of Table 18.3 represents nearest shrunken cen-
troids (with no shrinkage). Denote by s j the pooled within-class standard
deviation for feature j, and s 0 the median of the s j values. Then line (1)
also corresponds to nearest centroid classification, after first standardizing
each feature by s j + s 0 [recall (18.4) on page 652].

Line (3) shows that the performance of nearest medoids is very poor,
something which surprised us. It is perhaps due to the small sample sizes
and high dimensions, with medoids having much higher variance than
means. The performance of the one-nearest neighbor classifier is also poor.

The performance of the nearest centroid classifier is also shown in Ta-
ble 18.3 in line (5): it is better than nearest medoids, but worse than that
of nearest shrunken centroids, even with no shrinkage. The difference seems
to be the standardization of each feature that is done in nearest shrunken
centroids. This standardization is important here, and requires access to
the individual feature values. Nearest centroids uses a spherical metric, and
relies on the fact that the features are in similar units. The support vector
machine estimates a linear combination of the features and can better deal
with unstandardized features.

----------
### 本节练习

#### 练习 18.15

Kernel PCA. In Section 18.5.2 we show how to compute the
principal component variables Z from an uncentered inner-product matrix
K. We compute the eigen-decomposition (I − M)K(I − M) = UD 2 U T ,
with M = 11 T /N , and then Z = UD. Suppose we have the inner-productExercises
vector k 0 , containing the N inner-products between a new point x 0 and
each of the x i in our training set. Show that the (centered) projections of
x 0 onto the principal-component directions are given by

$$\mathbf{z}\_0 = \mathbf{D}^{-1}\mathbf{U}^T (\mathbf{I} - \mathbf{M})
[\mathbf{k}\_0 - \mathbf{K}\mathbf{1} / N] \tag{18.58}$$