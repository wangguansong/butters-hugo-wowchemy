---
title: 18.3 Linear Classifiers with Quadratic Regularization
summary: >
  第 654-660 页。

date: 2022-04-01T11:00:00+08:00
draft: true 

weight: 1803

---

Ramaswamy et al. (2001) present a more difficult microarray classification
problem, involving a training set of 144 patients with 14 different types of
cancer, and a test set of 54 patients. Gene expression measurements were
available for 16, 063 genes.

| Methods | CV erros (SE) Out of 144 | Test errors Out of 54 | Number of Genes Used|
|---------|---------------|-------------|-----------|
| 1. Nearest shrunken centroids           | 35 (5.0) | 17 | 6,520 |
| 2. L 2 -penalized discriminant analysis | 25 (4.1) | 12 | 16,063 |
| 3. Support vector classifier            | 26 (4.2) | 14 | 16,063 |
| 4. Lasso regression (one vs all)        | 30.7 (1.8) | 12.5 | 1,429 |
| 5. k-nearest neighbors                  | 41 (4.6) | 26 | 16,063 |
| 6. L 2 -penalized multinomial           | 26 (4.2) | 15 | 16,063 |
| 7. L 1 -penalized multinomial           | 17 (2.8) | 13 | 269 |
| 8. Elastic-net penalized multinomial    | 22 (3.7) | 11 | 384.8 |

**表 18.1：**



Table 18.1 shows the prediction results from eight different classification
methods. The data from each patient was first standardized to have mean
0 and variance 1; this seems to improve prediction accuracy overall this
example, suggesting that the “shape” of each gene-expression profile is
important, rather than the absolute expression levels. In each case, the
regularization parameter has been chosen to minimize the cross-validation
error, and the test error at that value of the parameter is shown. When
more than one value of the regularization parameter yields the minimal
cross-validation error, the average test error at these values is reported.

RDA (regularized discriminant analysis), regularized multinomial logistic
regression, and the support vector machine are more complex methods that
try to exploit multivariate information in the data. We describe each in
turn, as well as a variety of regularization methods, including both L 1 and
L 2 and some in between.

### 18.3.1 Regularized Discriminant Analysis

Regularized discriminant analysis (RDA) is described in Section 4.3.1. Lin-
ear discriminant analysis involves the inversion of a p × p within-covariance
matrix. When p ≫ N , this matrix can be huge, has rank at most N < p,
and hence is singular. RDA overcomes the singularity issues by regulariz-
ing the within-covariance estimate Σ̂. Here we use a version of RDA that
shrinks Σ̂ towards its diagonal:

$$\hat{\mathbf{\Sigma}}(\gamma) = \gamma \hat{\mathbf{\Sigma}} +
(1 - \gamma) \operatorname{diag}(\hat{\mathbf{\Sigma}}),
\text{ with } \gamma \in [0,1] \tag{18.9}$$

Note that γ = 0 corresponds to diagonal LDA, which is the “no shrinkage”
version of nearest shrunken centroids. The form of shrinkage in (18.9) is
much like ridge regression (Section 3.4.1), which shrinks the total covariance
matrix of the features towards a diagonal (scalar) matrix. In fact, viewing
linear discriminant analysis as linear regression with optimal scoring of the
categorical response (see (12.57) in Section 12.6), the equivalence becomes
more precise.

The computational burden of inverting this large p×p matrix is overcome
using the methods discussed in Section 18.3.5. The value of γ was chosen
by cross-validation in line 2 of Table 18.1; all values of γ ∈ (0.002, 0.550)
gave the same CV and test error. Further development of RDA, including
shrinkage of the centroids in addition to the covariance matrix, can be
found in Guo et al. (2006).

### 18.3.2 Logistic Regression with Quadratic Regularization

Logistic regression (Section 4.4) can be modified in a similar way, to deal
with the p ≫ N case. With K classes, we use a symmetric version of the
multiclass logistic model (4.17) on page 119:

$$\operatorname{Pr}(G=k | X=x) = \frac{\exp(\beta_{k0} + x^T \beta_k)}
{\sum_{\ell=1}^K \exp( \beta_{\ell 0} + x^T \beta_\ell)} \tag{18.10}$$

This has K coefficient vectors of log-odds parameters β 1 , β 2 , . . . , β K . We
regularize the fitting by maximizing the penalized log-likelihood

$$\max_{\\{\beta_{0k}, \beta_k\\}\_1^K} \left[
\sum_{i=1}^N \log \operatorname{Pr}(g_i|x_i) -
\frac{\lambda}{2} \sum_{k=1}^K \\|\beta_k\\|\_2^2 \right] \tag{18.11}$$

This regularization automatically resolves the redundancy in the paramet-
rization, and forces k=1 β̂ kj = 0, j = 1, . . . , p (Exercise 18.3). Note that
the constant terms β k0 are not regularized (and so one should be set to
zero). The resulting optimization problem is convex, and can be solved by
a Newton algorithm or other numerical techniques. Details are given in Zhu
and Hastie (2004). Friedman et al. (2010) provide software for computing
the regularization path for the two- and multiclass logistic regression mod-
els. Table 18.1, line 6 reports the results for the multiclass logistic regres-
sion model, referred to there as “multinomial”. It can be shown (Rosset
et al., 2004a) that for separable data, as λ → 0, the regularized (two-
class) logistic regression estimate (renormalized) converges to the maximal
margin classifier (Section 12.2). This gives an attractive alternative to the
support-vector machine, discussed next, especially in the multiclass case.

### 18.3.3 The Support Vector Classifier

The support vector classifier is described for the two-class case in Sec-
tion 12.2. When p > N , it is especially attractive because in general the
classes are perfectly separable by a hyperplane unless there are identical
feature vectors in different classes. Without any regularization the support
vector classifier finds the separating hyperplane with the largest margin;
that is, the hyperplane yielding the biggest gap between the classes in
the training data. Somewhat surprisingly, when p ≫ N the unregularized
support vector classifier often works about as well as the best regularized
version. Overfitting often does not seem to be a problem, partly because of
the insensitivity of misclassification loss.

There are many different methods for generalizing the two-class support-
vector classifier to  K > 2 classes. In the “one versus one” (ovo) approach,
we compute all K2 pairwise classifiers. For each test point, the predicted
class is the one that wins the most pairwise contests. In the “one versus all”
(ova) approach, each class is compared to all of the others in K two-class
comparisons. To classify a test point, we compute the confidences (signed
distance from the hyperplane) for each of the K classifiers. The winner is the
class with the highest confidence. Finally, Vapnik (1998) and Weston and
Watkins (1999) suggested (somewhat complex) multiclass criteria which
generalize the two-class criterion (12.7).

Tibshirani and Hastie (2007) propose the margin tree classifier, in which
support-vector classifiers are used in a binary tree, much as in CART
(Chapter 9). The classes are organized in a hierarchical manner, which can
be useful for classifying patients into different cancer types, for example.

Line 3 of Table 18.1 shows the results for the support vector classifier
using the ova method; Ramaswamy et al. (2001) reported (and we con-
firmed) that this approach worked best for this problem. The errors are
very similar to those in line 6, as we might expect from the comments
at the end of the previous section. The error rates are insensitive to the
choice of C [the regularization parameter in (12.8) on page 420], for values
of C > 0.001. Since p > N , the support vector hyperplane can perfectly
separate the training data by setting C = ∞.

### 18.3.4 Feature Selection

Feature selection is an important scientific requirement for a classifier when
p is large. Neither discriminant analysis, logistic regression, nor the support-
vector classifier perform feature selection automatically, because all use
quadratic regularization. All features have nonzero weights in both models.
Ad-hoc methods for feature selection have been proposed, for example,
removing genes with small coefficients, and refitting the classifier. This is
done in a backward stepwise manner, starting with the smallest weights and
moving on to larger weights. This is known as recursive feature elimination
(Guyon et al., 2002). It was not successful in this example; Ramaswamy
et al. (2001) report, for example, that the accuracy of the support-vector
classifier starts to degrade as the number of genes is reduced from the full
set of 16, 063. This is rather remarkable, as the number of training samples
is only 144. We do not have an explanation for this behavior.

All three methods discussed in this section (RDA, LR and SVM) can
be modified to fit nonlinear decision boundaries using kernels. Usually the
motivation for such an approach is to increase the model complexity. With
p ≫ N the models are already sufficiently complex and overfitting is always
a danger. Yet despite the high dimensionality, radial kernels (Section 12.3.3)
sometimes deliver superior results in these high dimensional problems. The
radial kernel tends to dampen inner products between points far away from
each other, which in turn leads to robustness to outliers. This occurs often
in high dimensions, and may explain the positive results. We tried a radial
kernel with the SVM in Table 18.1, but in this case the performance was
inferior.

### 18.3.5 Computational Shortcuts When p ≫ N

The computational techniques discussed in this section apply to any method
that fits a linear model with quadratic regularization on the coefficients.
That includes all the methods discussed in this section, and many more.
When p > N , the computations can be carried out in an N -dimensional
space, rather than p, via the singular value decomposition introduced in
Section 14.5. Here is the geometric intuition: just like two points in three-
dimensional space always lie on a line, N points in p-dimensional space lie
in an (N − 1)-dimensional affine subspace.

Given the N × p data matrix X, let

$$\begin{align} \mathbf{X}
&= \mathbf{U} \mathbf{D} \mathbf{V}^T \tag{18.12}\\\\
&= \mathbf{R} \mathbf{V}^T \tag{18.13}
\end{align}$$

be the singular-value decomposition (SVD) of X; that is, V is p × N with
orthonormal columns, U is N × N orthogonal, and D a diagonal matrix
with elements d 1 ≥ d 2 ≥ d N ≥ 0. The matrix R is N × N , with rows r i T .

As a simple example, let’s first consider the estimates from a ridge re-
gression:

$$\hat{\beta} = (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1}
\mathbf{X}^T \mathbf{y} \tag{18.14}$$

Replacing X by RV T and after some further manipulations, this can be
shown to equal

$$\hat{\beta} = \mathbf{V} (\mathbf{R}^T \mathbf{R} + \lambda \mathbf{I})^{-1}
\mathbf{R}^T \mathbf{y} \tag{18.15}$$

(Exercise 18.4). Thus β̂ = V θ̂, where θ̂ is the ridge-regression estimate
using the N observations (r i , y i ), i = 1, 2, . . . , N . In other words, we can
simply reduce the data matrix from X to R, and work with the rows of
R. This trick reduces the computational cost from O(p 3 ) to O(pN 2 ) when
p > N .

These results can be generalized to all models that are linear in the
parameters and have quadratic penalties. Consider any supervised learning
problem where we use a linear function f (X) = β 0 + X T β to model a
parameter in the conditional distribution of Y |X. We fit the parameters βP N
by minimizing some loss function i=1 L(y i , f (x i )) over the data with a
quadratic penalty on β. Logistic regression is a useful example to have in
mind. Then we have the following simple theorem:

Let f ∗ (r i ) = θ 0 + r i T θ with r i defined in (18.13), and consider the pair of
optimization problems:

$$\begin{align}
(\hat{\beta}\_0, \hat{\beta}) &=
\underset{\beta_0,\beta \in \mathbb{R}^p}{\arg\min}
\sum_{i=1}^N L(y_i, \beta_0 + x_i^T \beta) + \lambda \beta^T \beta \tag{18.16}\\\\
(\hat{\theta}\_0, \hat{\theta}) &=
\underset{\theta_0,\theta \in \mathbb{R}^N}{\arg\min}
\sum_{i=1}^N L(y_i, \theta_0 + r_i^T \theta) + \lambda \theta^T \theta\tag{18.17}
\end{align}$$

The theorem says that we can simply replace the p vectors x i by the
N -vectors r i , and perform our penalized fit as before, but with far fewer
predictors. The N -vector solution θ̂ is then transformed back to the p-
vector solution via a simple matrix multiplication. This result is part of
the statistics folklore, and deserves to be known more widely—see Hastie
and Tibshirani (2004) for further details.

Geometrically, we are rotating the features to a coordinate system in
which all but the first N coordinates are zero. Such rotations are allowed
since the quadratic penalty is invariant under rotations, and linear models
are equivariant.

This result can be applied to many of the learning methods discussed
in this chapter, such as regularized (multiclass) logistic regression, linear
discriminant analysis (Exercise 18.6), and support vector machines. It also
applies to neural networks with quadratic regularization (Section 11.5.2).
Note, however, that it does not apply to methods such as the lasso, which
uses nonquadratic (L 1 ) penalties on the coefficients.

Typically we use cross-validation to select the parameter λ. It can be
seen (Exercise 18.12) that we only need to construct R once, on the original
data, and use it as the data for each of the CV folds.

The support vector “kernel trick” of Section 12.3.7 exploits the same re-
duction used in this section, in a slightly different context. Suppose we have
at our disposal the N × N gram (inner-product) matrix K = XX T . From
(18.12) we have K = UD 2 U T , and so K captures the same information as
R. Exercise 18.13 shows how we can exploit the ideas in this section to fit
a ridged logistic regression with K using its SVD.