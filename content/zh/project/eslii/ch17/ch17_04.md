---
title: 17.4 离散变量无向图模型
summary: >
  第 638-645 页。

date: 2022-04-01T11:00:00+08:00
draft: true 
math: true

type: book
weight: 1704

authors: ["Butters"]
tags: ["译文"]
categories: ["统计学习基础（译注）"]

---

Undirected Markov networks with all discrete variables are popular, and
in particular pairwise Markov networks with binary variables being the
most common. They are sometimes called Ising models in the statistical
mechanics literature, and Boltzmann machines in the machine learning lit-
erature, where the vertices are referred to as “nodes” or “units” and are
binary-valued.

In addition, the values at each node can be observed (“visible”) or un-
observed (“hidden”). The nodes are often organized in layers, similar to a
neural network. Boltzmann machines are useful both for unsupervised and
supervised learning, especially for structured input data such as images,
but have been hampered by computational difficulties. Figure 17.6 shows
a restricted Boltzmann machine (discussed later), in which some variables
are hidden, and only some pairs of nodes are connected. We first consider
the simpler case in which all p nodes are visible with edge pairs (j, k) enu-
merated in E.

{{< figure
  id="f1706"
  src="https://public.guansong.wang/eslii/ch17/eslii_fig_17_06.png"
  title="**图 17.06**："
>}}

Denoting the binary valued variable at node j by X j , the Ising model
for their joint probabilities is given by

$$p(X, \mathbf{\Theta}) = \exp \left[
  \sum_{(j,k)\in E} \theta_{jk} X_j X_k - \Phi(\mathbf{\Theta})
\right] \text{ for } X \in \mathcal{X} \tag{17.28}$$

with X = {0, 1} p . As with the Gaussian model of the previous section,
only pairwise interactions are modeled. The Ising model was developed in
statistical mechanics, and is now used more generally to model the joint
effects of pairwise interactions. Φ(Θ) is the log of the partition function,
and is defined by

$$\Phi(\mathbf{\Theta}) = \log \sum_{x\in\mathcal{X}} \left[
  \exp \left( \sum_{(j,k) \in E} \theta_{jk} x_j x_k \right)
\right]
\tag{17.29}$$

The partition function ensures that the probabilities add to one over the
sample space. The terms θ jk X j X k represent a particular parametrization
of the (log) potential functions (17.5), and for technical reasons requires
a constant node X 0 ≡ 1 to be included (Exercise 17.10), with “edges” to
all the other nodes. In the statistics literature, this model is equivalent
to a first-order-interaction Poisson log-linear model for multiway tables of
counts (Bishop et al., 1975; McCullagh and Nelder, 1989; Agresti, 2002).

The Ising model implies a logistic form for each node conditional on the
others (exercise 17.11):

$$\operatorname{Pr}(X_j=1|X_{-j}=x_{-j}) = \frac{1}
{1 + \exp(-\theta_{j0} - \sum_{(j,k) \in E} \theta_{jk} x_k)}$$
$$\tag{17.30}$$

where X −j denotes all of the nodes except j. Hence the parameter θ jk
measures the dependence of X j on X k , conditional on the other nodes.


### 7.4.1 Estimation of the Parameters when the Graph Structure is Known

Given some data from this model, how can we estimate the parameters?
Suppose we have observations x i = (x i1 , x i2 , . . . , x ip ) ∈ {0, 1} p , i = 1, . . . , N .The log-likelihood is

$$\begin{align} \ell(\mathbf{\Theta})
&= \sum_{i=1}^N \log \operatorname{Pr}\_\mathbf{\Theta} (X_i=x_i) \\\\
&= \sum_{i=1}^N \left[\sum_{(j,k) \in E} \theta_{jk} x_{ij} x_{ik} - \Phi(\mathbf{\Theta}) \right]
\tag{17.31}\end{align}$$

The gradient of the log-likelihood is

$$\frac{\partial \ell(\mathbf{\Theta})}{\partial \theta_{jk}} =
\sum_{i=1}^N x_{ij} x_{ik} -
N \frac{\partial \Phi(\mathbf{\Theta})}{\partial \theta_{jk}}
\tag{17.32}$$

and

$$\begin{align} \frac{\partial \Phi(\mathbf{\Theta})}{\partial \theta_{jk}}
&= \sum_{x \in \mathcal{X}} x_j x_k \cdot p(x, \mathbf{\Theta}) \\\\
&= \operatorname{E}\_{\mathbf{\Theta}} (X_j X_k)
\tag{17.33}\end{align}$$

Setting the gradient to zero gives

$$\hat{\operatorname{E}} (X_j X_k) -
\operatorname{E}\_{\mathbf{\Theta}} (X_j X_k) = 0
\tag{17.34}$$

where we have defined

$$\hat{\operatorname{E}} (X_j X_k) = \frac{1}{N}\sum_{i=1}^N x_{ij} x_{ik}
\tag{17.35}$$

the expectation taken with respect to the empirical distribution of the data.
Looking at (17.34), we see that the maximum likelihood estimates simply
match the estimated inner products between the nodes to their observed
inner products. This is a standard form for the score (gradient) equation
for exponential family models, in which sufficient statistics are set equal to
their expectations under the model.

To find the maximum likelihood estimates, we can use gradient search
or Newton methods. However the computation of E Θ (X j X k ) involves enu-
meration of p(X, Θ) over 2 p−2 of the |X | = 2 p possible values of X, and is
not generally feasible for large p (e.g., larger than about 30). For smaller
p, a number of standard statistical approaches are available:

Poisson log-linear modeling, where we treat the problem as a large regres-
sion problem (Exercise 17.12). The response vector y is the vector of
2 p counts in each of the cells of the multiway tabulation of the data 4 .
The predictor matrix Z has 2 p rows and up to 1 + p + p 2 columns that
characterize each of the cells, although this number depends on the
sparsity of the graph. The computational cost is essentially that of a
regression problem of this size, which is O(p 4 2 p ) and is manageable
for p < 20. The Newton updates are typically computed by iteratively
reweighted least squares, and the number of steps is usually in the
single digits. See Agresti (2002) and McCullagh and Nelder (1989) for
details. Standard software (such as the R package glm) can be used
to fit this model.

Gradient descent requires at most O(p 2 2 p−2 ) computations to compute
the gradient, but may require many more gradient steps than the
second–order Newton methods. Nevertheless, it can handle slightly
larger problems with p ≤ 30. These computations can be reduced
by exploiting the special clique structure in sparse graphs, using the
junction-tree algorithm. Details are not given here.

Iterative proportional fitting (IPF) performs cyclical coordinate descent on
the gradient equations (17.34). At each step a parameter is updated
so that its gradient equation is exactly zero. This is done in a cyclical
fashion until all the gradients are zero. One complete cycle costs the
same as a gradient evaluation, but may be more efficient. Jirouśek and
Přeučil (1995) implement an efficient version of IPF, using junction
trees.

When p is large (> 30) other approaches have been used to approximate
the gradient.

• The mean field approximation (Peterson and Anderson, 1987) esti-
mates E Θ (X j X k ) by E Θ (X j )E Θ (X j ), and replaces the input vari-
ables by their means, leading to a set of nonlinear equations for the
parameters θ jk .

• To obtain near-exact solutions, Gibbs sampling (Section 8.6) is used
to approximate E Θ (X j X k ) by successively sampling from the esti-
mated model probabilities Pr Θ (X j |X −j ) (see e.g. Ripley (1996)).
We have not discussed decomposable models, for which the maximum
likelihood estimates can be found in closed form without any iteration
whatsoever. These models arise, for example, in trees: special graphs with
tree-structured topology. When computational tractability is a concern,
trees represent a useful class of models and they sidestep the computational
concerns raised in this section. For details, see for example Chapter 12 of
Whittaker (1990).

### 17.4.2 Hidden Nodes ::scream::

We can increase the complexity of a discrete Markov network by including
latent or hidden nodes. Suppose that a subset of the variables X H are
unobserved or “hidden”, and the remainder X V are observed or “visible.”
Then the log-likelihood of the observed data is

$$\begin{align} \ell(\mathbf{\Theta})
&= \sum_{i=1}^N \log \operatorname{Pr}\_\mathbf{\Theta} (X_{\mathcal{V}}=x_{i\mathcal{V}}) \\\\
&= \sum_{i=1}^N \left[
  \log \sum_{x_\mathcal{H} \in \mathcal{X}\_\mathcal{H}}
  \exp \sum_{(j,k) \in E}
  (\theta_{jk} x_{ij} x_{ik} - \Phi(\mathbf{\Theta})) \right]
\tag{17.36}\end{align}$$

The sum over x H means that we are summing over all possible {0, 1} values
for the hidden units. The gradient works out to be

$$\frac{d \ell(\mathbf{\Theta})}{d \theta_{jk}} =
\hat{\operatorname{E}}\operatorname{E}\_\mathbf{\Theta} (X_j X_k | X_\mathcal{V}) -
\operatorname{E}\_\mathbf{\Theta} (X_j X_k)
\tag{17.37}$$

The first term is an empirical average of X j X k if both are visible; if one
or both are hidden, they are first imputed given the visible data, and then
averaged over the hidden variables. The second term is the unconditional
expectation of X j X k .

The inner expectation in the first term can be evaluated using basic rules
of conditional expectation and properties of Bernoulli random variables. In
detail, for observation i

$$\begin{align}
& \operatorname{E}\_\mathbf{\Theta} (X_j X_k | X_\mathcal{V} = x_{i\mathcal{V}})\\\\
=&\begin{cases}
x_{ij} x_{ik} & \text{ if } j,k \in \mathcal{V} \\\\
x_{ij} \operatorname{Pr}\_\mathbf{\Theta} (X_k=1 | X_\mathcal{V} = x_{i\mathcal{V}}) & \text{ if } j \in \mathcal{V}, k \in \mathcal{H} \\\\
\operatorname{Pr}\_\mathbf{\Theta} (X_j=1, X_k=1 | X_\mathcal{V} = x_{i\mathcal{V}}) & \text{ if } j,k \in \mathcal{H}
\end{cases}
\tag{17.38}\end{align}$$

Now two separate runs of Gibbs sampling are required; the first to estimate
E Θ (X j X k ) by sampling from the model as above, and the second to esti-
mate E Θ (X j X k |X V = x iV ). In this latter run, the visible units are fixed
(“clamped”) at their observed values and only the hidden variables are
sampled. Gibbs sampling must be done for each observation in the training
set, at each stage of the gradient search. As a result this procedure can be
very slow, even for moderate-sized models. In Section 17.4.4 we consider
further model restrictions to make these computations manageable.

### 17.4.3 Estimation of the Graph Structure

The use of a lasso penalty with binary pairwise Markov networks has been
suggested by Lee et al. (2007) and Wainwright et al. (2007). The first au-
thors investigate a conjugate gradient procedure for exact maximization of
a penalized log-likelihood. The bottleneck is the computation of E Θ (X j X k )
in the gradient; exact computation via the junction tree algorithm is man-
ageable for sparse graphs but becomes unwieldy for dense graphs.

The second authors propose an approximate solution, analogous to the
Meinshausen and Bühlmann (2006) approach for the Gaussian graphical
model. They fit an L 1 -penalized logistic regression model to each node as
a function of the other nodes, and then symmetrize the edge parameter
estimates in some fashion. For example if θ̃ jk is the estimate of the j-k
edge parameter from the logistic model for outcome node j, the “min”
symmetrization sets θ̂ jk to either θ̃ jk or θ̃ kj , whichever is smallest in abso-
lute value. The “max” criterion is defined similarly. They show that under
certain conditions either approximation estimates the nonzero edges cor-
rectly as the sample size goes to infinity. Hoefling and Tibshirani (2008)
extend the graphical lasso to discrete Markov networks, obtaining a pro-
cedure which is somewhat faster than conjugate gradients, but still must
deal with computation of E Θ (X j X k ). They also compare the exact and
approximate solutions in an extensive simulation study and find the “min”
or “max” approximations are only slightly less accurate than the exact pro-
cedure, both for estimating the nonzero edges and for estimating the actual
values of the edge parameters, and are much faster. Furthermore, they can
handle denser graphs because they never need to compute the quantities
E Θ (X j X k ).

Finally, we point out a key difference between the Gaussian and binary
models. In the Gaussian case, both Σ and its inverse will often be of interest,
and the graphical lasso procedure delivers estimates for both of these quan-
tities. However, the approximation of Meinshausen and Bühlmann (2006)
for Gaussian graphical models, analogous to the Wainwright et al. (2007)
approximation for the binary case, only yields an estimate of Σ −1 . In con-
trast, in the Markov model for binary data, Θ is the object of interest, and
its inverse is not of interest. The approximate method of Wainwright et al.
(2007) estimates Θ efficiently and hence is an attractive solution for the
binary problem.

### 17.4.4 Restricted Boltzmann Machines

In this section we consider a particular architecture for graphical models
inspired by neural networks, where the units are organized in layers. A
restricted Boltzmann machine (RBM) consists of one layer of visible units
and one layer of hidden units with no connections within each layer. It is
much simpler to compute the conditional expectations (as in (17.37) and
(17.38)) if the connections between hidden units are removed 5 . Figure 17.6
shows an example; the visible layer is divided into input variables V 1 and
output variables V 2 , and there is a hidden layer H. We denote such a
network by

$$\mathcal{V}\_1 \leftrightarrow \mathcal{H}
\leftrightarrow \mathcal{V}\_2 \tag{17.39}$$

For example, V 1 could be the binary pixels of an image of a handwritten
digit, and V 2 could have 10 units, one for each of the observed class labels
0-9.

The restricted form of this model simplifies the Gibbs sampling for es-
timating the expectations in (17.37), since the variables in each layer are
independent of one another, given the variables in the other layers. Hence
they can be sampled together, using the conditional probabilities given by
expression (17.30).

The resulting model is less general than a Boltzmann machine, but is still
useful; for example it can learn to extract interesting features from images.
By alternately sampling the variables in each layer of the RBM shown
in Figure 17.6, it is possible to generate samples from the joint density
model. If the V 1 part of the visible layer is clamped at a particular feature
vector during the alternating sampling, it is possible to sample from the
distribution over labels given V 1 . Alternatively classification of test items
can also be achieved by comparing the unnormalized joint densities of each
label category with the observed features. We do not need to compute the
partition function as it is the same for all of these combinations.

As noted the restricted Boltzmann machine has the same generic form
as a single hidden layer neural network (Section 11.3). The edges in the
latter model are directed, the hidden units are usually real-valued, and the
fitting criterion is different. The neural network minimizes the error (cross-
entropy) between the targets and their model predictions, conditional on
the input features. In contrast, the restricted Boltzmann machine maxi-
mizes the log-likelihood for the joint distribution of all visible units—that
is, the features and targets. It can extract information from the input fea-
tures that is useful for predicting the labels, but, unlike supervised learning
methods, it may also use some of its hidden units to model structure in the
feature vectors that is not immediately relevant for predicting the labels.
These features may turn out to be useful, however, when combined with
features derived from other hidden layers.

Unfortunately, Gibbs sampling in a restricted Boltzmann machine can
be very slow, as it can take a long time to reach stationarity. As the net-
work weights get larger, the chain mixes more slowly and we need to run
more steps to get the unconditional estimates. Hinton (2002) noticed em-
pirically that learning still works well if we estimate the second expectation
in (17.37) by starting the Markov chain at the data and only running for a
few steps (instead of to convergence). He calls this contrastive divergence:
we sample H given V 1 , V 2 , then V 1 , V 2 given H and finally H given V 1 , V 2
again. The idea is that when the parameters are far from the solution, it
may be wasteful to iterate the Gibbs sampler to stationarity, as just a single
iteration will reveal a good direction for moving the estimates.

We now give an example to illustrate the use of an RBM. Using con-
trastive divergence, it is possible to train an RBM to recognize hand-written
digits from the MNIST dataset (LeCun et al., 1998). With 2000 hidden
units, 784 visible units for representing binary pixel intensities and one
10-way multinomial visible unit for representing labels, the RBM achieves
an error rate of 1.9% on the test set. This is a little higher than the 1.4%
achieved by a support vector machine and comparable to the error rate
achieved by a neural network trained with backpropagation. The error rate
of the RBM, however, can be reduced to 1.25% by replacing the 784 pixel
intensities by 500 features that are produced from the images without using
any label information. First, an RBM with 784 visible units and 500 hidden
units is trained, using contrastive divergence, to model the set of images.
Then the hidden states of the first RBM are used as data for training a
second RBM that has 500 visible units and 500 hidden units. Finally, the
hidden states of the second RBM are used as the features for training an
RBM with 2000 hidden units as a joint density model. The details and
justification for learning features in this greedy, layer-by-layer way are de-
scribed in Hinton et al. (2006). Figure 17.7 gives a representation of the
composite model that is learned in this way and also shows some examples
of the types of distortion that it can cope with.


{{< figure
  id="f1707"
  src="https://public.guansong.wang/eslii/ch17/eslii_fig_17_07.png"
  title="**图 17.07**："
>}}