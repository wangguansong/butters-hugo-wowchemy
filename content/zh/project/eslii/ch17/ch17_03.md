---
title: 17.3 连续变量无向图模型
summary: >
  第 630-638 页。

date: 2022-04-01T11:00:00+08:00
draft: true 

weight: 1703

---

Here we consider Markov networks where all the variables are continuous.
The Gaussian distribution is almost always used for such graphical models,
because of its convenient analytical properties. We assume that the observa-
tions have a multivariate Gaussian distribution with mean µ and covariance
matrix Σ. Since the Gaussian distribution represents at most second-order
relationships, it automatically encodes a pairwise Markov graph. The graph
in Figure 17.1 is an example of a Gaussian graphical model.

The Gaussian distribution has the property that all conditional distri-
butions are also Gaussian. The inverse covariance matrix Σ−1 contains
information about the partial covariances between the variables; that is,
the covariances between pairs i and j, conditioned on all other variables.
In particular, if the ijth component of Θ = Σ−1 is zero, then variables i and
j are conditionally independent, given the other variables (Exercise 17.3).

It is instructive to examine the conditional distribution of one variable
versus the rest, where the role of Θ is explicit. Suppose we partition X =
(Z, Y ) where Z = (X1 , . . . , Xp−1 ) consists of the first p − 1 variables and
Y = Xp is the last. Then we have the conditional distribution of Y give Z
(Mardia et al., 1979, e.g.)

{{< math >}}
$$Y|Z = z \sim \mathcal{N}(
  \mu_Y + (z-\mu_Z)^T\mathbf{\Sigma_{ZZ}}^{-1}\sigma_{ZY},
  \sigma_{YY} - \sigma_{ZY}^T \mathbf{\Sigma_{ZZ}}^{-1}\sigma_{ZY})$$
$$\tag{17.6}$$
{{< /math >}}

where we have partitioned Σ as

{{< math >}}
$$\mathbf{\Sigma} = \begin{pmatrix}
  \mathbf{\Sigma}_{ZZ} & \sigma_{ZY} \\
  \sigma_{ZY}^T & \sigma_{YY}
\end{pmatrix}\tag{17.7}$$
{{< /math >}}

The conditional mean in (17.6) has exactly the same form as the pop-
ulation multiple linear regression of Y on Z, with regression coefficient
β = Σ−1
ZZ σZY [see (2.16) on page 19]. If we partition Θ in the same way,
since ΣΘ = I standard formulas for partitioned inverses give

{{< math >}}
$$\theta_{ZY} = - \theta_{YY} \cdot \mathbf{\Sigma}_{ZZ}^{-1} \sigma_{ZY}\tag{17.8}$$
{{< /math >}}

where 1/θY Y = σY Y − σZYΣ−1ZZ σZY > 0. Hence

{{< math >}}
$$\begin{align} \beta
  &= \mathbf{\Sigma}_{ZZ}^{-1} \sigma_{ZY} \\
  &= - \theta_{ZY} / \theta_{YY}
\end{align}\tag{17.9}$$
{{< /math >}}

We have learned two things here:
- The dependence of Y on Z in (17.6) is in the mean term alone. Here
we see explicitly that zero elements in β and hence θZY mean that
the corresponding elements of Z are conditionally independent of Y ,
given the rest.
- We can learn about this dependence structure through multiple linear
regression.

Thus Θ captures all the second-order information (both structural and
quantitative) needed to describe the conditional distribution of each node
given the rest, and is the so-called “natural” parameter for the Gaussian
graphical model2 .

Another (different) kind of graphical model is the covariance graph or rel-
evance network, in which vertices are connected by bidirectional edges if the
covariance (rather than the partial covariance) between the corresponding
variables is nonzero. These are popular in genomics, see especially Butte
et al. (2000). The negative log-likelihood from these models is not convex,
making the computations more challenging (Chaudhuri et al., 2007).

### 17.3.1 Estimation of the Parameters when the Graph Structure is Known

Given some realizations of X, we would like to estimate the parameters
of an undirected graph that approximates their joint distribution. Suppose
first that the graph is complete (fully connected). We assume that we have
N multivariate normal realizations xi , i = 1, . . . , N with population mean
µ and covariance Σ. Let

{{< math >}}
$$\mathbf{S} = \frac{1}{N} \sum_{i=1}^N (x_i - \bar{x})(x_i - \bar{x})^T \tag{17.10}$$
{{< /math >}}

be the empirical covariance matrix, with x̄ the sample mean vector. Ignoring
constants, the log-likelihood of the data can be written as

{{< math >}}
$$\ell(\mathbf{\Theta}) = \log \det \mathbf{\Theta} -
  \operatorname{trace}(\mathbf{S}\mathbf{\Theta}) \tag{17.11}$$
{{< /math >}}

In (17.11) we have partially maximized with respect to the mean parameter
µ. The quantity −ℓ(Θ) is a convex function of Θ. It is easy to show that
the maximum likelihood estimate of Σ is simply S.

Now to make the graph more useful (especially in high-dimensional set-
tings) let’s assume that some of the edges are missing; for example, the
edge between PIP3 and Erk is one of several missing in Figure 17.1. As we
have seen, for the Gaussian distribution this implies that the correspond-
ing entries of Θ = Σ−1 are zero. Hence we now would like to maximize
(17.11) under the constraints that some pre-defined subset of the parame-
ters are zero. This is an equality-constrained convex optimization problem,
and a number of methods have been proposed for solving it, in particular
the iterative proportional fitting procedure (Speed and Kiiveri, 1986). This
and other methods are summarized for example in Whittaker (1990) and
Lauritzen (1996). These methods exploit the simplifications that arise from
decomposing the graph into its maximal cliques, as described in the previ-
ous section. Here we outline a simple alternate approach, that exploits the
sparsity in a different way. The fruits of this approach will become apparent
later when we discuss the problem of estimation of the graph structure.

The idea is based on linear regression, as inspired by (17.6) and (17.9).
In particular, suppose that we want to estimate the edge parameters θij for
the vertices that are joined to a given vertex i, restricting those that are not
joined to be zero. Then it would seem that the linear regression of the node
i values on the other relevant vertices might provide a reasonable estimate.
But this ignores the dependence structure among the predictors in this
regression. It turns out that if instead we use our current (model-based)
estimate of the cross-product matrix of the predictors when we perform
our regressions, this gives the correct solutions and solves the constrained
maximum-likelihood problem exactly. We now give details.

To constrain the log-likelihood (17.11), we add Lagrange constants for
all missing edges

{{< math >}}
$$\ell_C(\mathbf{\Theta}) = \log \det \mathbf{\Theta} -
  \operatorname{trace}(\mathbf{S}\mathbf{\Theta}) -
  \sum_{(j,k) \notin E} \gamma_{jk} \theta_{jk}\tag{17.12}$$
{{< /math >}}

The gradient equation for maximizing (17.12) can be written as

{{< math >}}
$$\mathbf{\Theta}^{-1} - \mathbf{S} - \mathbf{\Gamma} = 0 \tag{17.13}$$
{{< /math >}}

using the fact that the derivative of log det Θ equals Θ−1 (Boyd and Van-
denberghe, 2004, for example, page 641). Γ is a matrix of Lagrange param-
eters with nonzero values for all pairs with edges absent.

We will show how we can use regression to solve for Θ and its inverse
W = Θ−1 one row and column at a time. For simplicity let’s focus on the
last row and column. Then the upper right block of equation (17.13) can
be written as

{{< math >}}
$$w_{12} - s_{12} - \gamma_{12} = 0\tag{17.14}$$
{{< /math >}}

Here we have partitioned the matrices into two parts as in (17.7): part 1
being the first p − 1 rows and columns, and part 2 the pth row and column.
With W and its inverse Θ partitioned in a similar fashion, we have

{{< math >}}
$$\begin{pmatrix}
  \mathbf{W}_{11} & w_{12} \\ w_{12}^T & w_{22}
\end{pmatrix}
\begin{pmatrix}
  \mathbf{\Theta}_{11} & \theta_{12} \\ \theta_{12}^T & \theta_{22}
\end{pmatrix} =
\begin{pmatrix}
\mathbf{I} & 0 \\ 0^T & 1
\end{pmatrix}
\tag{17.15}$$
{{< /math >}}

This implies

{{< math >}}
$$\begin{align} w_{12}
  &= - \mathbf{W}_{11} \theta_{12} / \theta_{22} \tag{17.16}\\
  &= \mathbf{W}_{11} \beta \tag{17.17}
\end{align}$$
{{< /math >}}

where β = −θ12 /θ22 as in (17.9). Now substituting (17.17) into (17.14)
gives

{{< math >}}
$$\mathbf{W}_{11} \beta - s_{12} - \gamma_{12} = 0 \tag{17.18}$$
{{< /math >}}

These can be interpreted as the p − 1 estimating equations for the con-
strained regression of Xp on the other predictors, except that the observed
mean cross-products matrix S11 is replaced by W11 , the current estimated
covariance matrix from the model.

Now we can solve (17.18) by simple subset regression. Suppose there are
p−q nonzero elements in γ12 —i.e., p−q edges constrained to be zero. These
p − q rows carry no information and can be removed. Furthermore we can
reduce β to β ∗ by removing its p − q zero elements, yielding the reduced
q × q system of equation

{{< math >}}
$$\mathbf{W}_{11}^* \beta^* - s_{12}^* = 0 \tag{17.19}$$
{{< /math >}}

with solution β̂ ∗ = W11∗s12 . This is padded with p − q zeros to give β̂.

Although it appears from (17.16) that we only recover the elements θ12
up to a scale factor 1/θ22 , it is easy to show that

{{< math >}}
$$\frac{1}{\theta_{22}} = w_{22} - w_{12}^T \beta \tag{17.20}$$
{{< /math >}}

(using partitioned inverse formulas). Also w22 = s22 , since the diagonal of
Γ in (17.13) is zero.

This leads to the simple iterative procedure given in Algorithm 17.1 for
estimating both Ŵ and its inverse Θ̂, subject to the constraints of the
missing edges.

Note that this algorithm makes conceptual sense. The graph estimation
problem is not p separate regression problems, but rather p coupled prob-
lems. The use of the common W in step (b), in place of the observed
cross-products matrix, couples the problems together in the appropriate
fashion. Surprisingly, we were not able to find this procedure in the lit-
erature. However it is related to the covariance selection procedures of
Dempster (1972), and is similar in flavor to the iterative conditional fitting
procedure for covariance graphs, proposed by Chaudhuri et al. (2007).

{{< figure
  src="https://public.guansong.wang/eslii/ch17/eslii_fig_17_04.png"
  id="f1704"
  title="**图 17.4**：A simple graph for illustration, along with the empirical covariance matrix."
>}}

Here is a little example, borrowed from Whittaker (1990). Suppose that
our model is as depicted in Figure 17.4, along with its empirical covariance
matrix S. We apply algorithm (17.1) to this problem; for example, in the
modified regression for variable 1 in step (b), variable 3 is left out. The
procedure quickly converged to the solutions:

{{< math >}}
$$\hat{\mathbf{\Sigma}} = \begin{pmatrix}
  10.00 & 1.00 & 1.31 & 4.00 \\
  1.00 & 10.00 & 2.00 & 0.87 \\
  1.31 & 2.00 & 10.00 & 3.00 \\
  4.00 & 0.87 & 3.00 & 10.00 \\
\end{pmatrix}$$
$$\hat{\mathbf{\Sigma}}^{-1} = \begin{pmatrix}
  0.12 & -0.01 & 0.00 & -0.05 \\
  -0.01 & 0.11 & -0.02 & 0.00 \\
  0.00 & -0.02 & 0.11 & -0.03 \\
  -0.05 & 0.00 & -0.03 & 0.13
\end{pmatrix}$$
{{< /math >}}

Note the zeroes in Σ̂−1 , corresponding to the missing edges (1,3) and (2,4).
Note also that the corresponding elements in Σ̂ are the only elements dif-
ferent from S. The estimation of Σ̂ is an example of what is sometimes
called the positive definite “completion” of S.

### 17.3.2 Estimation of the Graph Structure

In most cases we do not know which edges to omit from our graph, and
so would like to try to discover this from the data itself. In recent years a
number of authors have proposed the use of L1 (lasso) regularization for
this purpose.

Meinshausen and Bühlmann (2006) take a simple approach to the problem:
rather than trying to fully estimate Σ or Θ = Σ−1 , they only estimate
which components of θij are nonzero. To do this, they fit a lasso regression
using each variable as the response and the others as predictors. The
component θij is then estimated to be nonzero if either the estimated coefficient
of variable i on j is nonzero, or the estimated coefficient of variable j on
i is nonzero (alternatively they use an and rule). They show that
asymptotically this procedure consistently estimates the set of nonzero elements
of Θ.

We can take a more systematic approach with the lasso penalty, following
the development of the previous section. Consider maximizing the penalized
log-likelihood

{{< math >}}
$$\log \det \mathbf{\Theta} - \operatorname{trace}(\mathbf{S}\mathbf{\Theta}) -
  \lambda \|\mathbf{\Theta}\|_1 \tag{17.21}$$
{{< /math >}}

where ||Θ||1 is the L1 norm—the sum of the absolute values of the elements
of Σ−1 , and we have ignored constants. The negative of this penalized
likelihood is a convex function of Θ.

It turns out that one can adapt the lasso to give the exact maximizer of
the penalized log-likelihood. In particular, we simply replace the modified
regression step (b) in Algorithm 17.1 by a modified lasso step. Here are the
details.

The analog of the gradient equation (17.13) is now

{{< math >}}
$$\mathbf{\Theta}^{-1} - \mathbf{S} -
  \lambda \cdot \operatorname{Sign}(\mathbf{\Theta}) = 0 \tag{17.22}$$
{{< /math >}}

Here we use sub-gradient notation, with Sign(θjk ) = sign(θjk ) if θjk 6= 0,
else Sign(θjk ) ∈ [−1, 1] if θjk = 0. Continuing the development in the
previous section, we reach the analog of (17.18)

{{< math >}}
$$\mathbf{W}_{11} \beta - s_{12} +
  \lambda \cdot \operatorname{Sign} \beta = 0 \tag{17.23}$$
{{< /math >}}

(recall that β and θ12 have opposite signs). We will now see that this system
is exactly equivalent to the estimating equations for a lasso regression.

Consider the usual regression setup with outcome variables y and pre-
dictor matrix Z. There the lasso minimizes

{{< math >}}
$$\frac{1}{2} (y - \mathbf{Z}\beta)^T(y - \mathbf{Z}\beta) +
  \lambda \cdot \|\beta\|_1 \tag{17.24}$$
{{< /math >}}

[see (3.52) on page 68; here we have added a factor 21 for convenience]. The
gradient of this expression is

{{< math >}}
$$\mathbf{Z}^T\mathbf{Z}\beta - \mathbf{Z}^T\mathbf{y} + \lambda \cdot \operatorname{Sign}(\beta) = 0 \tag{17.25}$$
{{< /math >}}

So up to a factor 1/N , ZT y is the analog of s12 , and we replace ZT Z by
W11 , the estimated cross-product matrix from our current model.

The resulting procedure is called the graphical lasso, proposed by Fried-
man et al. (2008b) building on the work of Banerjee et al. (2008). It is
summarized in Algorithm 17.2.

Friedman et al. (2008b) use the pathwise coordinate descent method
(Section 3.8.6) to solve the modified lasso problem at each stage. Here are
the details of pathwise coordinate descent for the graphical lasso algorithm.
Letting V = W11 , the update has the form

{{< math >}}
$$\hat{\beta} \leftarrow S\left( s_{12j} - \sum_{k \neq j} V_{kj} \hat{\beta}_k, \lambda \right) / V_{jj}\tag{17.26}$$
{{< /math >}}

for j = 1, 2, . . . , p − 1, 1, 2, . . . , p − 1, . . ., where S is the soft-threshold
operator:

{{< math >}}
$$S(x,t) = \operatorname(x) (|x| - t)_+ \tag{17.27}$$
{{< /math >}}

The procedure cycles through the predictors until convergence.

It is easy to show that the diagonal elements wjj of the solution matrix
W are simply sjj + λ, and these are fixed in step 1 of Algorithm 17.23 .

The graphical lasso algorithm is extremely fast, and can solve a moder-
ately sparse problem with 1000 nodes in less than a minute. It is easy to
modify the algorithm to have edge-specific penalty parameters λjk ; since
λjk = ∞ will force θ̂jk to be zero, this algorithm subsumes Algorithm 17.1.
By casting the sparse inverse-covariance problem as a series of regressions,
one can also quickly compute and examine the solution paths as a function
of the penalty parameter λ. More details can be found in Friedman et al.
(2008b).

Figure 17.1 shows the result of applying the graphical lasso to the flow-
cytometry dataset. Here the lasso penalty parameter λ was set at 14. In
practice it is informative to examine the different sets of graphs that are
obtained as λ is varied. Figure 17.5 shows four different solutions. The
graph becomes more sparse as the penalty parameter is increased.

{{< figure
  src="https://public.guansong.wang/eslii/ch17/eslii_fig_17_05.png"
  id="f1705"
  title="**图 17.5**：Four different graphical-lasso solutions for the flow-cytometry data."
>}}

Finally note that the values at some of the nodes in a graphical model can
be unobserved; that is, missing or hidden. If only some values are missing
at a node, the EM algorithm can be used to impute the missing values
(Exercise 17.9). However, sometimes the entire node is hidden or latent.
In the Gaussian model, if a node has all missing values, due to linearity
one can simply average over the missing nodes to yield another Gaussian
model over the observed nodes. Hence the inclusion of hidden nodes does
not enrich the resulting model for the observed nodes; in fact, it imposes
additional structure on its covariance matrix. However in the discrete model
(described next) the inherent nonlinearities make hidden units a powerful
way of expanding the model.


----------
### 本节练习

#### 练习 17.5

Consider the setup in Section 17.3.1 with no missing edges. Show
that

{{< math >}}
$$\mathbf{S}_{11}\beta - s_{12} = 0$$
{{< /math >}}

are the estimating equations for the multiple regression coefficients of the
last variable on the rest.

#### 练习 17.6

Recovery of $\hat{\mathbf{\Theta}}=\hat{\mathbf{\Sigma}}^{-1}$ from Algorithm 17.1. Use expression (17.16)
to derive the standard partitioned inverse expressions

{{< math >}}
$$\begin{align}
\theta_{12} &= - \mathbf{W}_{11}^{-1} w_{12} \theta_{22} \tag{17.41} \\
\theta_{22} &= 1 / (w_{22} - w_{12}^T \mathbf{W}_{11}^{-1} w_{12}) \tag{17.42}
\end{align}$$
{{< /math >}}


Since $\hat{\beta}=\mathbf{W}\_{11}^{-1}w_{12}$ , show that $\hat{\theta}\_{22}=1/(w_{22}-w_{12}^T\hat{\beta})$ and $\hat{\theta}\_{12}=-\hat{\beta}\hat{\theta}\_{22}$
Thus $\hat{\theta}\_{12}$ is a simply rescaling of $\hat{\beta}$ by $-\hat{\theta}\_{22}$.

#### 练习 17.7

Write a program to implement the modified regression procedure in
Algorithm 17.1 for fitting the Gaussian graphical model with pre-specified
edges missing. Test it on the flow cytometry data from the book website,
using the graph of Figure 17.1.

#### 练习 17.8

1. Write a program to fit the lasso using the coordinate descent procedure (17.26). Compare its results to those from the lars program or some other convex optimizer, to check that it is working correctly.
2. Using the program from (a), write code to implement the graphical lasso (Algorithm 17.2). Apply it to the flow cytometry data from the book website. Vary the regularization parameter and examine the resulting networks.

#### 练习 17.9

Suppose that we have a Gaussian graphical model in which some
or all of the data at some vertices are missing.

1. Consider the EM algorithm for a dataset of N i.i.d. multivariate ob-
   servations xi ∈ IRp with mean µ and covariance matrix Σ. For each
   sample i, let oi and mi index the predictors that are observed and
   missing, respectively. Show that in the E step, the observations are
   imputed from the current estimates of µ and Σ:
   {{< math >}}
   $$\begin{align} \hat{x}_{i,m_i} 
     &= \operatorname{E}(x_{i,m} | x_{i,o_i}, \theta) \\
     &= \hat{\mu}_{m_i} + \hat{\Sigma}_{m_i,\sigma_i}\hat{\Sigma}_{o_i,o_i}^{-1}
        (x_{i,o_i} - \hat{\mu}_{o_i}) \tag{17.43}
   \end{align}$$
   {{< /math >}}
   while in the M step, µ and Σ are re-estimated from the empirical
   mean and (modified) covariance of the imputed data:
   {{< math >}}
   $$\begin{align}
   \hat{\mu}_j &= \sum_{i=1}^N \hat{x}_{ij} / N \\
   \hat{\Sigma}_{jj'} &= \sum_{i=1}^N
     [(\hat{x}_{ij} - \hat{\mu}_j)(\hat{x}_{ij'} - \hat{\mu}_{j'}) + c_{i,jj'}] / N
   \tag{17.44}\end{align}$$
   {{< /math >}}
   where ci,jj ′ = Σ̂jj ′ if j, j ′ ∈ mi and zero otherwise. Explain the reason
   for the correction term ci,jj ′ (Little and Rubin, 2002).
2. Implement the EM algorithm for the Gaussian graphical model using
   the modified regression procedure from Exercise 17.7 for the M-step.
3. For the flow cytometry data on the book website, set the data for the
   last protein Jnk in the first 1000 observations to missing, fit the model
   of Figure 17.1, and compare the predicted values to the actual values
   for Jnk. Compare the results to those obtained from a regression of
   Jnk on the other vertices with edges to Jnk in Figure 17.1, using only
   the non-missing data.