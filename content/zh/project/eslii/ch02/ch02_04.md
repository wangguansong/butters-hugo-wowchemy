---
title: 2.4 统计决策理论
summary: >
  第 18-22 页。从统计理论的角度，以平方误差衡量，条件于 X 的 Y 期望是最优的预测。线性回归或最近邻域都是在寻找条件期望的合理近似方法，只是采取的近似手段不同。

date: 2018-09-03T09:20:07+08:00
lastmod: 2022-05-27T17:40:07+08:00

weight: 204

---

本节介绍一点为讨论统计模型提供基础框架的统计决策理论。首先我们考虑量化输出变量。在概率空间中，假设 $X\in\mathbb{R}^p$ 为一个取值为实数的输入随机向量，$Y\in\mathbb{R}$ 为一个数值输出变量，两者的联合概率分布记为 $\operatorname{Pr}(X,Y)$。模型的目标为寻找合适的方程 $f(X)$，输入 $X$ 的值后得到 $Y$ 的预测。通常需要一个 **损失函数（loss function）** 来衡量预测误差的大小，目前最方便并使用广泛的是 **平方误差损失函数（squared error loss）**：$L(Y,f(X))=(Y-f(X))^2$。据此我们得到了一个选择 $f$ 的标准：

{{< math >}}
$$\begin{align} \operatorname{EPE}(f)
&= \operatorname{E}(Y-f(X))^2 \tag{2.9}\\
&= \int[y-f(x)]^2 \operatorname{Pr}(dx,dy) \tag{2.10}
\end{align}$$
{{< /math >}}

即期望平方预测误差。当 $X$ 满足一些基本条件时[^1]，我们可以将 EPE 写为：

{{< math >}}
$$\operatorname{EPE}(f) = \operatorname{E}_X \operatorname{E}_{Y|X}
([Y - f(X)]^2 | X) \tag{2.11}$$
{{< /math >}}

在输入空间中每个点 $x$ 寻找最小化 EPE 的 $f(x)$，即可得到一个使 EPE 最小化的 $f$：

{{< math >}}
$$f(x) = \arg\min_c \operatorname{E}_{Y|X} ([Y - c]^2 | X=x) \tag{2.12}$$
{{< /math >}}

最小化的解为[^2]：

{{< math >}}
$$f(x) = \operatorname{E} (Y | X=x) \tag{2.13}$$
{{< /math >}}

这个条件期望也被称为 **回归函数（regression function）** 。在以平均平方误差衡量预测模型的损失时，在任意一点 $X=x$ 上的 $Y$ 的最优预测为对应的条件期望。

最优近邻方法直接在训练集上执行了这个理论。在每一个输入变量空间上的点 $x$，我们希望能够得到所有 $x_i=x$ 的样本点的对应的 $y_i$ 的平均值。然而通常样本中至多只有一个点可以满足 $x_i=x$，于是我们用邻域上的平均来代替：

{{< math >}}
$$\hat{f}(x) = \operatorname{Ave}(y_i | x_i \in N_k(x)) \tag{2.14}$$
{{< /math >}}

其中的 $\operatorname{Ave}$ 表示取平均值，$N_k(x)$ 表示在训练集 $\mathcal{T}$ 中包含 $k$ 个点的邻域。这里隐含了两个近似：

- 用数据样本上的平均近似概率空间上的期望；
- 在取条件期望时，用某个点的邻域 $X\in N_k(x)$ 来近似这个点 $X=x$ 的条件。

如果训练样本的个数 $N$ 足够大，样本的密度足够高，则 $x$ 的邻域中的点会足够接近 $x$，并且当 $k$ 足够大时，领域上的平均值也会更稳定。实际上，在关于联合概率分布 $\operatorname{Pr}(X,Y)$ 较弱的正则条件下，可以证明当 $N\rightarrow\infty$，$k\rightarrow\infty$，并且 $k/N\rightarrow\infty$时，$\hat{f}(x)\rightarrow\operatorname{E}(Y|X=x)$。既然如此，我们是否找到了一个万能的模型，皆大欢喜？并不是，通常我们不会有足够多的训练样本。且当线性假设或其他结构的假设是合理的时，我们可以得到一个比 k-最近邻更稳定的预测模型，当然怎样的假设是合理的还需要从训练样本中习得。除此之外，有时也会有更严重的问题。在下一章节（2.5）中会介绍，当输入变量空间的维度 $p$ 增大，某个点的 k-近邻区域也同样会被扩大。因此用邻域上的平局和某点的条件期望会相差甚远。上述的收敛结果仍然成立，只是收敛的速度会随着维度的增加而降低。

回到基础理论的讨论，线性回归也可以归纳在这个理论基础上。简单来说，线性模型即假设回归方程（式 2.13）为一个线性方程：

{{< math >}}
$$f(x) \approx x^T \beta \tag{2.15}$$
{{< /math >}}

这是一个基于模型的处理方式，即为回归方程（2.13）指定一个模型。将这个线性模型代入到等式 2.9 的 EPE 中，再取微分，我们可以得到参数 $\beta$ 的理论解：

{{< math >}}
$$\hat{\beta} = [\operatorname{E}(XX^T)]^{-1} \operatorname{E}(XY) \tag{2.16}$$
{{< /math >}}

注意到这里并没有对 $X$ 取条件期望，而是在整体概率空间上取期望。最小二乘法的解（等式 2.6）相当于用训练样本上的平均代替了等式 2.16 中的期望。

综上，k-近邻和最小二乘法都在用样本平均值来近似条件期望。但它们在模型假设上完全不同：

- 最小二乘法假设回归方程 $f(x)$ 可以比较好地用全局线性方程来近似。
- k-近邻方法假设 $f(x)$ 可以很好地用局部的常数来近似。

虽然后者看起来更合理，但我们已经提及了对这种普适性所要付出的代价。

很多本书中的现代统计学习方法是基于模型的，当然它们要比严格的线性模型灵活很多。例如，加性模型（additive model）对回归方程的假设是

{{< math >}}
$$f(X) = \sum_{j=1}^p f_j(X_j) \tag{2.17}$$
{{< /math >}}

这个结构保留了线性模型的相加结构，但每个构成方程 $f_j$ 都是没有结构限制的。事实上加性模型的最优估计使用了类似于 k-最近邻的方法来同时近似每个方程的条件期望，而这些方程只有一维输入变量。因此，附加了某个（通常不现实的）模型假设后，在此例中为加性模型假设，就解决了估计条件期望时输入变量空间的维度过高的问题。

另外，我们是否满足于使用式 2.11 作为评判标准？如果用 $L_1:\operatorname{E}|Y-f(X)|$ 来代替 $L_2$ 作为损失函数会如何？这时对 $EPE$ 最小化的解是条件概率中位数：

{{< math >}}
$$\hat{f}(x) = \operatorname{median}(Y | X=x) \tag{2.18}$$
{{< /math >}}

中位数是概率分布位置的另一种度量，相较条件期望，它的估计更稳健（robust）[^3]。$L_1$ 函数在其微分中有不连续的点，这让它没有被广泛采纳。在后面的章节会介绍其他有稳健性质的损失函数，但目前平方误差仍然凭借其方便的解析性质被广泛使用。

以上均为量化输出变量，那么对于分类输出变量如何处理？套路仍然是一样的，只是衡量预测误差的损失函数有所不同。模型的预测 $\hat{G}$ 的取值范围为 $\mathcal{G}$，可能分类的集合。损失函数可以写成一个 $K\times K$ 的矩阵 $\mathbf{L}$，且 $K=\operatorname{card}(G)$[^4]。矩阵 $\mathbb{L}$ 的对角线上全为 0，其他位置上为非负数。元素 $L(k,\ell)$ 代表将一个实际为类别 $\mathcal{G}\_k$ 的样本分类为 $\mathcal{G}\_\ell$ 的损失。我们经常使用 0-1 的损失函数，即所有的非对角线位置均为 1。类似地，期望预测误差为：

{{< math >}}
$$\text{EPE} = \operatorname{E}[L(G, \hat{G}(X))] \tag{2.19}$$
{{< /math >}}

如同量化输出变量的式 2.9，这里的期望是基于联合概率分布 $\operatorname{Pr}(G,X)$。同样在一定的条件下，$\text{EPE}$ 可被写作：

{{< math >}}
$$\text{EPE} = \operatorname{E}_X \sum_{k=1}^K
L[\mathcal{G}_k, \hat{G}(X)] \operatorname{Pr}(\mathcal{G}_k | X) \tag{2.20}$$
{{< /math >}}

同理，这个最小化也可以被在每个点上求最小化替代：

{{< math >}}
$$\hat{G}(x)=\arg\min_{g \in \mathcal{G}}
\sum_{k=1}^K L[\mathcal{G}_k, g] \operatorname{Pr}(\mathcal{G}_k | X=x) \tag{2.21}$$
{{< /math >}}

如果使用 0-1 损失函数，上式可简化为：

{{< math >}}
$$\hat{G}(x) = \arg\min_{g \in \mathcal{G}}
[1 - \operatorname{Pr}(g | X=x) ] \tag{2.22}$$
{{< /math >}}

或写为[^5]：

{{< math >}}
$$\hat{G}(x) = \mathcal{G}_k \text{ if } \operatorname{Pr}(\mathcal{G}_k | X=x)
= \max_{g \in \mathcal{G}} \operatorname{Pr}(g | X=x) \tag{2.23}$$
{{< /math >}}

上式的解也被称为 **贝叶斯分类器（Bayes classifier）**，从直觉上很容易理解这个等式，即根据离散的条件概率分布 $\operatorname{Pr}(G|X)$，对于某个点赋予其条件概率最大的分类。[图 2.5](#figure-f0205) 展示了上一节模拟示例的贝叶斯最优决策边界。贝叶斯分类器的错误率也称为贝叶斯错误率。

{{< figure
  src="https://public.guansong.wang/eslii/ch02/eslii_fig_02_05.png"
  id="f0205"
  title="**图 2.5**：使用与图 2.1、2.2、2.3 同样模拟数据的最优贝叶斯决策边界。由于数据的生成模型已知，所以可以确定地计算出这个边界。（[练习 2.2](#练习-22)）"
>}}

我们再一次注意到 k-近邻分类器，在最近邻域上的少数服从多数投票，直接地实现式 2.23。只是在某个点上的条件概率被近似为这个点的邻域上的条件概率，并且概率被近似估计为样本的分类比例。

对于二分类问题，我们可以将分类输出变量 $G$ 编码为取值 0-1 的哑变量 $Y$，然后使用前半节中的平方误差损失函数。如果 $G_1$ 对应着 $Y=1$，则 $\hat{f}(X)=\operatorname{E}(Y|X)=\operatorname{Pr}(G_1|X)$。类似地，对于 K 分类问题， $\operatorname{E}(Y_k|X)=\operatorname{Pr}(G_k|X)$。因此贝叶斯分类器也可理解为使用多个 0-1 输出变量做回归，然后按照最大的预测或拟合值来确定分类。这个理论是正确的，但实践上使用各种不同的回归模型可能有不同的问题。例如，如果使用线性模型，则 $\hat{f}(X)$ 不一定是正数。在[第四章]({{< relref "../ch04/_index.md" >}})我们会介绍多种基于模型的 $\operatorname{Pr}(G|X)$ 估计方法。

----------

### 本节练习

#### 练习 2.1

假设 $K$ 个类型的每一个都有一个对应的目标向量 $t_k$，即一个只在第 $k$ 个位置为一其他位置都为零的向量。求证：如果 $\hat{y}$ 的所有元素和为一，那么按 $\hat{y}$ 中的最大值元素进行分类等价于按与之最接近的目标向量 $\arg\min_k\|t_k-\hat{y}\|$ 进行分类。

#### 练习 2.2

推导计算图 2.5 中模拟例子中的贝叶斯决策边界。

{{< spoiler text="**解 2.2**" >}}

由式 2.22 可知贝叶斯分类规则可写为：

$$\hat{G}(x) = \arg\max_{g \in \mathcal{G}} \operatorname{Pr}(g | X=x)$$

根据贝叶斯公式：

$$\begin{align} \operatorname{Pr}[g|X=x]
&= \frac{\operatorname{Pr}[G=g, X=x]}{\operatorname{Pr}[X=x]} \\\\
&= \frac{\operatorname{Pr}[X=x|g]\operatorname{Pr}[G=g]}{\operatorname{Pr}[X=x]}
\end{align}$$

这个二分类的例子中，先验概率 $\operatorname{Pr}[g]$ 同为 0.5。这里的特征变量 $X=x$ 取值在 $\mathbf{R}^2$ 上。注意严格来说对于连续变量 $X=x$ 的概率为 0，需要从概率密度函数层面去证明。这里仅做参考。所以在这里贝叶斯分类规则就只需要比较当类别为蓝色或橙色时 $X=x$ 的条件概率（密度）。[第 2.3.3 节]({{< relref "../ch02/ch02_03.md" >}})给出了模拟数据的生成模型，因此这个条件概率是可计算的。

假设 $m_k^{(0)}, k=1,\dots,10$ 为蓝色类别来自 $\mathcal{N}((1,0)^T, \mathbf{I})$ 分布的 10 个成分高斯分布的均值向量，则某个蓝色点的特征向量 $X$ 来自于 $\mathcal{N}(m_k^{(0)}, \mathbf{I}/5)$ 的概率都均匀地为 1/10。所以：

$$\begin{align}
f_0(x) &= \operatorname{Pr}[X=x|\text{蓝色}]
        = \frac{1}{10} \sum_{k=1}^{10} \Phi(x; m_k^{(0)}, \mathbf{I}/5) \\\\
f_1(x) &= \operatorname{Pr}[X=x|\text{橙色}]
        = \frac{1}{10} \sum_{k=1}^{10} \Phi(x; m_k^{(1)}, \mathbf{I}/5)
\end{align}$$

这里的 $\Phi$ 为多元高斯分布的概率密度函数；$m_k^{(0)}$ 和 $m_k^{(1)}$ 是（在上帝视角下）已知的概率分布参数。所以 $f_1(x)-f_0(x)=0$ 在 $\mathbb{R}^2$ 对应出的曲线即为贝叶斯决策边界。

{{< /spoiler >}}

----------

### 参考资料

- [Optimal Decision Boundaries](https://mathformachines.com/posts/decision/index.html)：关于最优（贝叶斯）决策边界，以及 R 代码。其中第二个例子（混合正态分布）就是本书中的混合高斯模拟数据。
- [Beautiful Plots: The Decision Boundary](https://www.tvhahn.com/posts/beautiful-plots-decision-boundary/)：Python 绘制决策边界的例子。



[^1]: 这里所说的基本条件，使概率空间 $(X,Y)$ 满足其联合概率分布函数可以分解成条件概率函数和边际概率函数的乘积，即 $\text{Pr}(X,Y) = \text{Pr}(Y|X)\text{Pr}(X)$，这里 $\text{Pr}(Y|X) = \text{Pr}(Y|X) / \text{Pr}(X)$。译者理解这个条件是“随机变量$Y$对于随机变量$X$是绝对连续的”。
[^2]: 目标函数可写为 $E\_{Y|X}(Y^2|X=x) - 2c E\_{Y|X}(Y|X=x) + c^2$，这是一个 $c$ 的一元二次方程，即可得到使其最小的 $c$ 值。
[^3]: 译者认为将 robust 翻译做“鲁棒”是非常搞笑的做法。
[^4]: 集合的势（cardinality），可以理解为一个有限集合中元素的个数。
[^5]: 译者认为等式 2.23 的写法不如这样更直观：$\hat{G}(x) = \text{arg}\max\_{g\in\mathcal{G}}\text{Pr}(g|X=x)$。