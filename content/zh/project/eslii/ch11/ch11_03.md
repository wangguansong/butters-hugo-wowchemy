---
title: 11.3 神经网络
summary: >
  第 392-395 页。神经网络模型的本质是一种非线性的统计模型。本节以单隐层反向传播网络为例，介绍了神经网络的基本结构。

date: 2019-02-16T16:55:00+08:00
lastmod: 2019-02-16T16:55:00+08:00

weight: 1103

---

**神经网络（neural network）** 已发展成为囊括了很大一类模型和学习方法的术语。本节介绍最普遍使用的普通（vanilla）神经网络，有时被称为单隐层（single hidden layer）反向传播（back-propagation）网络，或单层感知器（perceptron）。神经网络得到了非常大的关注，使其貌似神奇而又神秘。但如本节所述，它与上节中的投影寻踪回归模型一样，只是非线性的统计模型。

{{< figure
  id="f1102"
  src="https://public.guansong.wang/eslii/ch11/eslii_fig_11_02.png"
  title="**图 11.2**：单隐层（single hidden layer）前馈（feed-forward）神经网络的示意图。"
>}}

神经网络是一个二阶段回归或分类模型，通常可表述为[图 11.2](#figure-f1102) 中的网络示意图。这个网络哦可适用于回归和分类问题。对回归问题，通常 $K=1$，并且在顶部只有一个输出单元 $Y_1$。然而这些网路模型可以无缝地处理多个数量的输出变量，因此本节直接处理通用的场景。

对 $K$ 类别的分类问题，在顶端有 $K$ 个单元，其中第 $k$ 个单元是对类别 $k$ 的概率的建模。这里有 $K$ 个目标变量 $Y_k$，$k=1,\dots,K$，每个变量是第 $k$ 个类别的 0-1 编码变量。

用输入变量的线性组合创建衍生特征变量 $Z_m$，然后在用 $Z_m$ 的线性组合的函数来对目标变量 $Y_k$ 建模：

$$\begin{align}
Z_m &= \sigma(\alpha_{0m} + \alpha_m^T X), m = 1,\dots,M \\\\
T_k &= \beta_{0k} + \beta_k^T Z, k = 1,\dots,K \\\\
f_k(X) &= g_k(T), k = 1,\dots,K
\end{align}\tag{11.5}$$

其中 $Z=(Z_1,Z_2,\dots,Z_M)^T$，$T=(T_1,T_2,\dots,T_k)^T$。

{{< figure
  id="f1103"
  src="https://public.guansong.wang/eslii/ch11/eslii_fig_11_03.png"
  title="**图 11.3**：在神经网络的隐藏层中普遍使用的 S 函数 $\sigma(v)=1/(1+e^{-v})$ 的曲线图（红色曲线）。图中还包括了 $\sigma(xv)$ 的曲线，其中 $s=\frac{1}{2}$（蓝色曲线）和 $s=10$（紫色曲线）。尺度参数 $s$ 控制了激活率，可见较大的 $s$ 取值对应着在 $v=0$ 点处的硬激活。注意 $\sigma(s(v-v_0))$ 可将激活门槛从 $0$ 移至 $v_0$。"
>}}

通常将激活函数 $\sigma(v)$ 选为 **S 函数（sigmoid）** $\sigma(v)=1/(1+e^{-v})$；[图 11.3](#figure-f1103) 展示了 $1/(1+e^{-v})$ 的曲线图。有时会用高斯径向基函数（第六章）作为 $\sigma(v)$，这就会得到一个 **径向基函数网络（radial basis function network）**。

有时在如[图 11.2](#figure-f1102) 的神经网络示意图中，会有额外的偏差（bias）单元进入到隐藏层或输出层。若将常数 “1” 视为一个额外的输入特征，那么这个偏差单元捕捉的是模型 11.5 中的截距项 $\alpha_{0m}$ 和 $\beta_{0k}$。

输出函数 $g_k(T)$ 可对输出变量向量 $T$ 进行最终的一次变换。在回归问题中，通常选用恒等函数 $g_k(T)=T_k$。在 $K$ 类别的分类问题中，在早期也使用恒等函数，但在后续由 softmax（归一化指数）函数所替代：

$$g_k(T) = \frac{e^{T_k}}{\sum_{\ell=1}^K e^{T_\ell}} \tag{11.6}$$

这也当然就是在多项对数几率模型中（[第 4.4 节]({{< relref "../ch04/ch04_04.md" >}})）使用的变换，可得出和为一的正估计值。[第 4.2 节]({{< relref "../ch04/ch04_02.md" >}})讨论了线性激活函数的其他问题，特别是潜在严重的屏蔽（masking）效应。

由于 $Z_m$ 的取值无法直接观测到，在网络中部计算衍生特征 $Z_m$ 的单元被称为隐藏单元。一般来说可以有不止一个隐藏层，如在本章末尾的例子所演示的。可以将 $Z_m$ 视为原始输入变量 $X$ 的基展开；那么神经网络便是使用这些输入变量的变化的标准线性模型，或线性多项对数几率模型。但它对第五章讨论的基展开方法有一个重要的改进；这里的基函数参数是从数据中学习得出的。

注意如果 $\sigma$ 为恒等函数，那么整个模型便退回到输入变量的一个线性模型。因此，在回归和分类问题中，神经网络都可以看作是对线性模型的一个非线性的推广。引入了非线性变换 $\sigma$ 后，极大地增加了线性模型可覆盖的模型。从图 11.3 中可见 S 函数的激活率取决于 $\alpha_m$ 的范数，如果 $\\|\alpha_m\\|$ 非常小，那么这个单元实际运行在其激活函数近似于线性的区间上。

同时注意到单隐层的神经网络模型与之前介绍的投影寻踪模型有着完全一样的形式。区别在于 PPR 模型使用的是非参数函数 $g_m(v)$，而神经网络使用的是基于 $\sigma(v)$ 的简单得多的函数，在其输入中只有三个自由的参数。详细来说，若将神经网络视为一个 PPR 模型，则有：

$$\begin{align} g_m(w_m^T X)
&= \beta_m \sigma(\alpha_{0m} + \alpha_m^T X) \\\\
&= \beta_m \sigma(\alpha_{0m} + \\|\alpha_m^T\\|(w_m^T X))
\tag{11.7}\end{align}$$

其中 $w_m=\alpha_m/\\|\alpha_m\\|$ 为第 $m$ 个单位向量。由于函数 $\sigma_{\beta,\alpha_0,s}(v)=\beta\sigma(\alpha_0+sv)$ 的复杂度要低于一个更通用的非参数函数 $g(v)$，自然地在神经网络中可能使用 20 或 100 个这种函数，而 PPR 模型通常使用更少的项（例如 $M=5$ 或 $M=10$）。

最后指出“神经网络”名字的由来，是因为这个方法最先是作为人脑的模型发展而来的。每个单元代表了一个神经元（neuron），并且它们之间的连接（[图 11.2](#figure-f1102) 中的连线）代表了神经突触（synapse）。在早期的模型中，当传入一个单元的总信号超出一个特定的门槛后，这个神经元才会被激活。在上述的模型中，这相当于使用一个阶梯函数作为 $\sigma(Z)$ 和 $g_m(T)$。后来意识到了神经网络是非线性统计建模的一个有效工具，而阶梯函数的平滑性无法满足为此进行的最优化过程。因此阶梯函数被替换为一个更平滑的阈值函数，即[图 11.3](#figure-f1103) 中的 S 函数。