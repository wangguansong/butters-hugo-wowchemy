---
title: 11.7 示例：邮政编码数据
summary: >
  第 404-408 页。在手写数字识别的分类问题中演示神经网络模型。在寻找最优模型的过程中，一个方向是扩大备选模型的范围，也就是让模型可以模拟出更复杂的函数结构；另一个方向是根据具体的常识缩小搜寻的范围，比如对模型的系数后结构加以限制。译者在本节有较多不理解之处，待回溯。

date: 2019-02-24T15:52:00+08:00
lastmod: 2019-02-24T15:52:00+08:00
draft: false
math: true

type: book
weight: 1107

authors: ["Butters"]
tags: ["译文"]
categories: ["统计学习基础（译注）"]

---

本节的例子是一个字符识别任务：手写数字的分类。这个问题多年以来在机器学习和神经网络的学术社区得到很多关注，并且仍然是该领域中的一个基准问题。[图 11.9](#figure-f1109) 展示了标准化的手写数字的一些示例，它们是美国邮政局（U.S. Postal Service）从信封上自动扫描所得。原始的扫描数字是二进制文件，并且有不同的大小和方向；这里展示的图片经过了去倾斜化和大小标准化的处理，最后的结果是 $16\times 16$ 的灰度图片（Le Cun et al., 1990）。这 256 个像素点的值就作为神经网络分类器的输入变量。

{{< figure
  id="f1109"
  src="https://public.guansong.wang/eslii/ch11/eslii_fig_11_09.png"
  title="**图 11.9**：邮政编码数据的训练样本示例。每个图片是用 $16\times 16$ 的 8 位灰度代表的一个手写数字。"
>}}

一个黑盒（black box）[^10]的神经网络模型在这个模式识别任务中并不太适用，一部分是因为用像素表达的图片缺乏某种不变性（比如图片小幅旋转会改变像素的值）。因此神经网络模型在这个问题的多个例子上的早期尝试得出的误分类率在 4.5% 左右。本节将介绍一些为克服这些问题而调整神经网络的探索性工作（Le Cun, 1989），这些努力最终发展成为神经网络模型的最先进水平（Le Cun et al., 1998）[^1] [^2]。

尽管目前的数字数据库中已有几万个训练和测试样本，本节为了突出演示（不同模型结构的）效果，有意使用了不太大的数据样本。样本来自于对一些真实手写数字的扫描图片，以及通过随机水平平移而生成的一些追加的图片。更多细节可参考 Le Cun (1989)。训练集中有 320 个数字，测试集中有 160 个数字。

数据的拟合使用了五个不同的神经网络模型：

- 网络模型 1：无隐藏层，等价于多项对数几率回归。
- 网络模型 2：单个隐藏层，12 个完全连接的隐藏单元。
- 网络模型 3：两个隐藏层，局部连接。
- 网络模型 4：两个隐藏层，局部连接，权重共享。
- 网络模型 5：两个隐藏层，局部连接，两层的权重共享。

{{< figure
  id="f1110"
  src="https://public.guansong.wang/eslii/ch11/eslii_fig_11_10.png"
  title="**图 11.10**：邮政编码例子中使用的五个神经网络模型的结构示意图。"
>}}

[图 11.10](#figure-f1110) 中描绘了这些模型。例如网络模型 1 中有 256 个输入变量，$16\times 16$ 的每个输入像素对应着一个变量，并且有 10 个输出单元对应着 0-9 的每个数字。预测值 $\hat{f}\_k(X)$ 代表着一个图片 $x$ 的数字类别为 $k$，$k=0,1,2,\dots,9$，的估计概率值。

所有网络模型都是 S 函数的输出单元[^3]，并且都基于平方和的误差函数进行拟合。第一个网络模型没有隐藏层，因此它几乎等价于一个线性多项回归模型（练习 11.4）。网络模型 2 是一个有 12 个如之前介绍的隐藏单元的单隐层网络[^4]。

所有网络模型的训练误差都为 0%，因为在所有的场景中参数个数都大于训练观测样本量。[图 11.11](#figure-f1111) 展示了测试误差随着训练轮回的增加而变化。线性网络（网络模型 1）很快就开始出现过拟合，而其他模型的测试误差表现稳定在依次更优的水平上。

{{< figure
  id="f1111"
  src="https://public.guansong.wang/eslii/ch11/eslii_fig_11_11.png"
  title="**图 11.11**：测试集表现作为训练轮回个数的函数曲线，包括应用在邮政编码数据上的表 11.1 列出的五个网络模型（Le Cun, 1989）。"
>}}

其他三个网络模型包含了一些能体现出神经网络模型的能力和灵活性的一些附加特性。它们在网络模型上添加了由具体问题自然产生的约束，这样可以在生成更复杂的连接结构的同时却有更少的参数。

网络模型 3 使用了局部连接：也就是说每个隐藏单元只与其下面一层中的一小组单元有连接。在第一个隐藏层中（$8\times 8$ 的行列），每个单元以输入层的一个 $3\times 3$ 的区域作为输入变量；第一个隐藏层中相邻的单元的接收区域有一行或一列的重叠，即有两个像素的距离。在第二个隐藏层中，输入为 $5\times 5$ 的区域，相邻的单元的接收区域同样有两个像素的距离[^5]。所有其他连接的权重设为零。局部连接令每个单元从下一层中提取局部的特征，这很大程度地减少了权重的总数。网络模型 3 比 网络模型 2 有更多的隐藏单元，但却有更少的连接和权重个数（1226 对比 3214），并达到了相似的表现。

网络模型 4 和网络模型 5 包括了带有权重分享的局部连接。在图片的不同部分上所有的提取局部特征的映射都采用了一样的运算，也就是共享了同样的权重。在网络模型 4 中，第一个隐藏层为两个 $8\times 8$ 的行列，如在网络模型 3 中一样，每个单元接收来自 $3 \times 3$ 区域上的输入。不过在同一个 $8 \times 8$ 特征矩阵上的每个单元都共享了同样的九个权重（但会有不同的偏差参数）。这使得图片的不同部分上的特征提取是由同一个线性转换完成的，因而这种网络结构有时被称为 **卷积神经网络（convolutional neural network，CNN）**。网络模型 4 在第二个隐藏层中没有权重共享，所以与网络模型 3 结构一样[^6]。误差函数 $R$ 对一个共享权重的梯度是被这个权重在其控制的所有连接上的 $R$ 的梯度之和。

|            | 网络结构     | 连接数 | 权重数 | 正确率 |
|------------|-------------|------|--------|-------|
| 网络模型 1： | 单层网络     | 2570 | 2570   | 80.0% |
| 网络模型 2： | 双层网络     | 3214 | 3214   | 87.0% |
| 网络模型 3： | 局部连接     | 1226 | 1226   | 88.5% |
| 网络模型 4： | 有约束网络 1 | 2266 | 1132   | 94.0% |
| 网络模型 5： | 有约束网络 2 | 5194 | 1060   | 98.4% |

**表11.1**：手写数字分类列子中，五个不同神经网络模型在测试集上的表现（Le Cun, 1989）。

{{% callout note %}}
{{< spoiler text="计算连接数和权重数" >}}

计算连接数和权重数

- 网络模型 1
  - 连接数/权重数：$(16^2+1)\cdot 10$
- 网络模型 2
  - 连接数/权重数：$(16^2+1)\cdot 12+(12+1)\cdot 10$
- 网络模型 3
  - 连接数/权重数：$(3^2+1)\cdot 8^2+(5^2+1)\cdot 4^2+(4^2+1)\cdot 10$
- 网络模型 4
  - 连接数：$(3^2+1)\cdot 8^2\cdot 2+(5^2\cdot 2+1)\cdot 4^2+(4^2+1)\cdot 10$
  - 权重数：$(3^2+8^2)\cdot 2+(5^2\cdot 2+1)\cdot 4^2+(4^2+1)\cdot 10$
- 网络模型 5
  - 连接数：$(3^2+1)\cdot 8^2\cdot 2+(5^2\cdot 2+1)\cdot 4^2\cdot 4+(4^2\cdot 4+1)\cdot 10$
  - 权重数：$(3^2+8^2)\cdot 2+(5^2\cdot 2+4^2)\cdot 4+(4^2\cdot 4+1)\cdot 10$

{{< /spoiler >}}
{{% /callout %}}


表 11.1 给出了每个网络模型的连接数、权重数和在测试集上的最优表现。可见网络模型 4 比网络模型 3 有更多的连接但更少的权重，并且更好的测试集表现。网络模型 5 在第二个隐藏层有四个 $4\times 4$ 的特征矩阵，每个单元与下一层中 $5\times 5$ 的局部区域连接[^7]。在这些特征矩阵中都存在权重共享。可见网络模型 5 表现最好，误差率只有 1.6%，与之相比“普通”的网络模型 2 的误差率为 13%。网络模型 5 的结构设计很巧妙，是经过多年的试验得出的成果，其灵感来自对手写样式应该会在数字图片的多个部分出现的事实的认知。这样的和类似的神经网络模型在当时（1990 年代早期）比其它任何学习方法在邮政编码问题上的表现都好。这个例子也说明了神经网络并不是一个如有时被宣称的全自动工具。与所有的统计模型一样，专业领域的知识能够也应该被用来改善模型的表现。

这个模型后来被[第 13.3.3 节]({{< relref "../ch13/ch13_03.md" >}})将介绍的切线距离（tangent distance）方法（Simard et al., 1993）超越，这个方法直接地考虑到了自然的平移不变性（affine invariance）。目前数字识别数据成为了每个新的学习方法的测试台，研究人员为降低错误率做出了很多的努力。在本书成书之际，在从标准的 NIST[^8] 数据库中得到的大量数据上（60,000 个训练样本，10,000个测试样本），最佳的误差率如下（Le Cun et al., 1998）：

- $1.1\%$：切线距离的 1 近邻分类器（第 13.3.3 节）
- $0.8\%$：a degree-9 polynomial SVM（第 12.3 节）
- $0.8\%$：*LeNet-5* ，一个上述的卷积神经网络的更复杂版本；
- $0.7\%$：提升 *LeNet-4* 。提升方法见[第 8 章]({{< relref "../ch08/_index.md" >}})。*LeNet-4* 是 *LeNet-5* 的前一个版本。

Le Cun et al. (1998) 中给出了一个更大的模型表现对比表格，这也看出有很多研究者在非常努力地在做降低这些测试误差率的工作。文章中给出的误差率估计的标准误差为 0.1%，这是基于一个 $N=10,000$ 和 $p\approx 0.01$ 的二项分布平均[^9]。这意味着误差率之间 0.1%~0.2% 的差别在统计意义上可视为等价的。在现实中这个估计的标准误差会更高，因为在不同方法的参数调节中会隐含地使用到测试数据。

[^1]: 原文脚注 1：这个例子中的图表是对 Le Cun (1989) 的再现。
[^2]: 文献链接：[Le Cun (1989)](http://yann.lecun.com/exdb/publis/pdf/lecun-89.pdf); [Le Cun et al. (1998)](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)。
[^3]: 神经网络模型在多类型的输出单元应该是 softmax？
[^4]: 原文中说隐藏单元是“of the kind described above”，“above”指的是什么？
[^5]: 这里关于隐藏层单元的局部连接结构内容按原文翻译，不过译者对其有疑问。首先，如果第一个隐藏层中相邻的两个单元在输入层对应的区域有两个单元的距离，那么要生成 $8 \times 8$ 的行列需要输入层的维度为 $17 \times 17$；第二，如果第二个隐藏层中的单元对应了第一个隐藏层上 $5 \times 5$ 的区域，那么要从 $8 \times 8$ 得到 $4 \times 4$，相邻单元对应的区域应该只有一个单元的距离。译者查看了文章 (Le Cun 1989) 也是差不多的描述。
[^6]: 第二个隐藏层中的单元对应了两个下一层的 $5 \times 5$ 的输入区域, 所以共有 50 个输入变量（不算截距）。
[^7]: 两个 $5 \times 5$？
[^8]: 美国国家标准与技术研究院（The National Institute of Standards and Technology）维护了大量的数据库，其中包括了手写字符数据库；http://www.nist.gov/srd/。
[^9]: 如何理解？
[^10]: “黑盒”指的是一个通用的即插即用式的神经网络方法，不需要对具体问题做调整。