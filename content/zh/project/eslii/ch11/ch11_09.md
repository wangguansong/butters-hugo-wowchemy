---
title: 11.9 贝叶斯神经网络和 NIPS 2003 挑战赛
summary: >
  第 409-414 页。Neal and Zhang (2006) 在 NIPS 2003 的分类问题挑战赛中利用贝叶斯神经网络方法赢得了比赛。本节用竞赛的数据集，对比了一系列不同的方法。

date: 2019-02-28T16:08:00+08:00
lastmod: 2019-02-28T16:08:00+08:00
linktitle: 11.9 BNN 和 NIPS 2003 挑战赛

weight: 1109

---

在 2003 年举办了一个分类问题的竞赛，给参与者提供了五个有标签的训练数据集。它是为神经信息处理系统（Neural Information Processing Systems，NIPS）研讨会组织的。每个数据集都是二类别的分类问题，它们有不同的样本量并且来自不同的领域（见表 11.2）。同时也提供了一个验证集上的特征度量。

参与者设计并应用统计学习方法来对这些数据集进行预测，并可在为期 12 周的时间内将对验证集的预测结果提交至网站。得到这个反馈后，参与者需要提交在另外一个测试集上的预测，然后得到各自的结果。最后会公开验证集上的类别标签，参与者有一周的时间来综合训练集和验证集来训练算法，并将最终的预测提交到竞赛网站。共有 75 个小组参与了竞赛，最终提交了验证集和测试集结果的分别有 20 和 16 个小组。

这个竞赛侧重于对特征的提取。数据中添加了一些伪造的“探测变量”：即一些分布与真实特征相似但与分类标签独立的噪声特征。表 11.2 列出了每个数据集中添加的探测变量相对于特征个数的百分比。因此每个学习方法需要想办法识别出探测变量，并且降低或消除它们的影响。

| 数据 | 领域 | 特征类型 | p | 探测变量百分比 | $N_\text{tr}$ | $N_\text{val}$ | $N_\text{te}$ |
|-----|-----|---------|---|--------------|---------------|----------------|---------------|
| Arcene  | 质谱测定 | 密集 | 10,000  | 30 | 100  | 100  | 700  |
| Dexter  | 文本分类 | 稀疏 | 20,000  | 50 | 300  | 300  | 2000 |
| Dorothea| 药物研发 | 稀疏 | 100,000 | 50 | 800  | 350  | 800  |
| Gisette | 数字识别 | 密集 | 5000    | 30 | 6000 | 1000 | 6500 |
| Madelon | 虚拟数据 | 密集 | 500     | 96 | 2000 | 600  | 1800 |

**表 11.2**：NIPS 2003 挑战赛数据集。列名 $p$ 表示特征的个数。Dorothea 数据集的特征为二元变量。$N_\text{tr}$、$N_\text{val}$ 和 $N_\text{te}$ 分别为训练集、验证集和测试集的样本个数。

在衡量结果中使用了很多度量，包括了测试集上的正确分类百分比、ROC 曲线下的面积和对所有的分类器两两对比后的综合得分。竞赛的结果很令人感兴趣，具体可见 Guyon et al. (2007)。其中最值得关注的结果当然是最后的胜出小组：Neal and Zhang (2006)。在最终的比较中，他们在五个数据集上有三个排名第一，在两外两个排名分别为第五和第七。

Neal and Zhang (2006) 在获胜的方案中使用了一系列特征选择预处理的步骤，然后再利用了贝叶斯神经网络（Bayesian neural network）、狄利克雷扩散树（Dirichlet diffusion tree）以及这些方法的结合。本节集中于贝叶斯神经网络方法，并尝试辨别出他们的方法可以成功的重要因素。我们重现了他们的程序并且将结果与提升神经网络、提升树模型和其他相关方法进行比较。

### 11.9.1 贝叶斯、提升和自助聚合

首先来简要回顾一下贝叶斯的推断方法和它在神经网络中的应用。给定训练集 $\mathbf{X}\_\text{tr}$ 和 $\mathbf{y}\_\text{tr}$，假设样本的模型有参数 $\theta$；Neal and Zhang (2006) 使用了双隐层的神经网络，其输出节点为二元分类结果的概率 $\operatorname{Pr}(Y|X,\theta)$。给定一个先验分布 $\operatorname{Pr}(\theta)$，参数的后验分布为：

$$\operatorname{Pr}(\theta|\mathbf{X}\_\text{tr}, \mathbf{y}\_\text{tr}) = \frac
{\operatorname{Pr}(\theta)
\operatorname{Pr}(\mathbf{y}\_\text{tr}|\mathbf{X}\_\text{tr}, \theta)}
{\int \operatorname{Pr}(\theta)
\operatorname{Pr}(\mathbf{y}\_\text{tr}|\mathbf{X}\_\text{tr}, \theta) d\theta}
\tag{11.19}$$

对于一个特征为 $X_\text{new}$ 的测试样本，分类标签 $Y_\text{new}$ 的预测分布为：

$$\begin{align}
& \operatorname{Pr}(Y_\text{new} |
  X_\text{new},\mathbf{X}\_\text{tr}, \mathbf{y}\_\text{tr}) \\\\
= &\int \operatorname{Pr}(Y_\text{new}|X_\text{new}, \theta)
  \operatorname{Pr}(\theta | \mathbf{X}\_\text{tr}, \mathbf{y}\_{tr}) d\theta
\end{align}\tag{11.20}$$

（类比于表达式 8.24）。由于难以处理表达式 11.20 中的积分，所以会用复杂的马尔可夫链蒙特卡罗（Markov Chain Monte Carlo，MCMC）方法来从后验分布 $\operatorname{Pr}(Y_\text{new} | X_\text{new}$ 抽样。生成几百个 $\theta$ 的取值，然后在这些取值上进行简单的平均来估计积分。Neal and Zhang (2006) 在所有参数中使用了扩散（diffuse）高斯先验。文中使用的特定 MCMC 方法被称为 **混合蒙特卡罗（hybrid Monte Carlo）**，而这可能是这个方法成功的关键。它包含了一个辅助动量向量（auxiliary momentum vector）并实施了汉密尔顿的演变（Hamiltonian dynamics），其中潜在函数是目标密度函数。这样做是为了避免出现随机行走；相继的备选（模型）在样本空间上以较大的步伐移动。它们倾向于有更低的相关性，因此会更快速地收敛到目标概率分布上。

Neal and Zhang (2006) 同时也尝试了对特征不同形式的预处理：

1. 利用 t 检验的单变量筛选
2. 自动的相关性测定（ARD）

在第二个方法（ARD）中，第一个隐藏层的每个单元的第 $j$ 个特征的权重（或系数）都有一个共同的先验方差 $\sigma_j^2$ 和先验均值零。然后计算每个方差 $\sigma_j^2$ 的后验概率，舍弃掉后验方差集中在较小取值上的特征。

因此这个方法成功的关键可能是它的以下三个特点：

1. 特征选择和预处理；
2. 神经网络模型；
3. 利用 MCMC 的模型贝叶斯推断。

根据 Neal and Zhang (2006) 文中所述，特点 1 中的特征筛选完全是为了提高计算的效率；因为在很大数量的特征上进行 MCMC 过程是很慢的。然而并不需要用特征选择来避免过拟合。后验的平均（表达式 11.20）自动地解决了这个问题。

我们想要理解贝叶斯方法成功的原因。在我们看来，现代的贝叶斯方法的能力并不是来自于将它们作为了一个正式的推断过程；大部分人都不会相信在一个高维度复杂的神经网络模型上的先验概率真的是正确的。而是因为贝叶斯或 MCMC 方法是对模型空间上相关的部分进行抽样，并对高概率模型的预测取平均的一个有效的方式。

自助聚合（bagging）和提升方法（boosting）是非贝叶斯的方法，但它们都与贝叶斯模型中的 MCMC 有相似之处。贝叶斯方法将数据固定，将当前对后验分布的估计作为参数的随机分布。自主聚合以独立同分布的形式来打乱数据，然后重新拟合模型从而得出一个模型参数估计值的集合。最后在预测时，计算来自不同自主聚合样本的模型的预测值的简单平均。提升方法与自助聚合类似，但它的模型是每个单独的基学习器的加性模型，这些基学习器的训练样本并不是独立同分布的。这些模型都可以写为以下的形式：

$$\hat{f}(\mathbf{x}\_\text{new}) = \sum_{\ell=1}^L w_\ell
\operatorname{E}(Y_\text{new} | \mathbf{x}\_\text{new}, \hat{\theta}\_\ell)
\tag{11.21}$$

在所有的模型中 $\hat{\theta}\_\ell$ 是模型参数的集合。对于贝叶斯模型，$w_\ell=1/L$，从后验分布对 $\theta_\ell$ 抽样后取平均值来估计后验的均值 11.21。对于自助聚合，同样有 $w_\ell=1/L$，而 $\hat{\theta}\_\ell$ 为在训练集的自助聚合重取样上拟合的参数值。对于提升方法，所有的权重都等于 1，但 $\hat{\theta}\_\ell$ 通常是相继以非随机的方式得出的，从而不断地改进当前的拟合。

### 11.9.2 表现比较

鉴于上述的相似性，我们在表 11.2 中五个数据集上比较了贝叶斯神经网络、提升树模型、提升神经网络、随机森林和自助聚合神经网络。作者在以前的工作中没有使用过自助聚合或提升版本的神经网络。由于贝叶斯神经网络在竞赛中的成功以及自助聚合和提升方法在树模型中的良好表现，我们决定在这里进行尝试。通过对神经模型进行自助聚合和提升，我们可以评估模型的选择以及选择模型的策略。

以下为待比较的学习方法的一些细节：

- 贝叶斯神经网络。结果取自于 Neal and Zhang (2006)，使用了他们的贝叶斯方法来拟合神经网络。模型有两个隐藏层，各有 20 和 8 个单元。我们只是为了计时而重新运行了一些网络模型。
- 提升树模型。我们使用了 R 语言的 `gbm` 程序包（版本 1.5-7）。不同数据集有不同的树深度和收缩因子。我们在每个提升循环步骤中一致地对数据进行了 80% 的自助抽样（默认为 50%）。收缩因子在 $0.001$ 和 $0.1$ 之间。树深度在 $2$ 和 $9$ 之间。
- 提升神经网络。由于提升方法通常对弱学习器最有效，我们对有两个或四个单元的单隐层神经网络进行提升，拟合使用了 R 语言的 `nnet` 程序包（版本 7.2-36）。
- 随机森林。我们使用了 R 语言的 `randomForest` 程序包（版本 4.5-16），其中的参数使用了默认的设置。
- 自助聚合神经网络。我们使用了与上面的贝叶斯神经网络相同的结构（两个隐含层，各有 20 和 8 个单元），拟合使用了 Neal 的 C 语言程序包“Flexible Bayesian Modeling”（2004-11-10 发布）以及 Matlab 的 `neural-net` 工具（版本 5.1）。

这个分析工作由 Nicholas Johnson 完成，完整的分析细节可见 Johnson (2008)[^1]。[图 11.12](#figure-f1112) 和表 11.3 展示了一些结果。

{{< figure
  id="f1112"
  src="https://public.guansong.wang/eslii/ch11/eslii_fig_11_12.png"
  title="**图 11.12**：不同学习方法在五个问题上的表现，使用了单变量筛选的特征集（左图）以及由自动相关性测定的缩减特征集（右图）。每个图顶端的误差横条之间的长度等于两种误差率之差的一个标准差。在大多数问题中，有几个方法彼此落在这个标准差的范围中。"
>}}

| 方法 | 筛选特征集 | | ARD 缩减特征集 | |
|-----|------|------|------|------|
|     | **平均排名** | **平均用时** | **平均排名** | **平均用时** | 
| 贝叶斯神经网络 | 1.5 | 384(138)  | 1.6 | 600(186)   |
| 提升树 | 3.4 | 3.03(2.5) | 4.0 | 34.1(32.4) |
| 提升神经网络 | 3.8 | 9.4(8.6)  | 2.2 | 35.6(33.5) |
| 随机森林 | 2.7 | 1.9(1.7)  | 3.2 | 11.2(9.3)  |
| 自助聚合神经网络 | 3.6 | 3.5(1.1)  | 4.0 | 6.4(4.4)   |

**表 11.3**：不同方法的表现。表中数值是在五个问题上的测试误差排名的平均（越低越好），以及计算用时的均值和标准差（单位为分钟）。

图和表中展示的方法有贝叶斯神经网络、提升神经网络、自助聚合神经网络、提升树模型和随机森林，使用了基于筛选的特征集合和基于缩减（reduced）的特征集合[^2]。每个图顶部的误差横条标志着两个误差率之差的一个标准差范围。贝叶斯神经网络再一次成为了胜者，不过在一些数据集中测试误差率之间的差别在统计意义上并不显著。在所有与之竞争的方法中，在使用筛选特征集时随机森林的表现最好，而在使用缩减特征集时提升神经网络表现最好，几乎与贝叶斯神经网络相同。

提升神经网络要优于提升树模型，说明了神经网络模型更适用于这些特定的问题中。具体来讲，在这些场景中单独的特征可能不能很好地预测结果，而特征的线性组合可更好地预测结果。不过这个说法却并不能解释随机森林的出奇表现，我们对此也感到意外。

由于缩减特征集实际上是由贝叶斯神经网络模型得出，只有在使用筛选特征集时的方法才是正规的独立的过程。然而这也说明了更好的内部特征选择有可能会改善提升神经网络模型的总体表现。

表中也展示了每个方法所需要的大概训练耗时。这时非贝叶斯的方法有明显的优势。

总的来说，贝叶斯神经网络在表现上的优势可能有以下的原因：

1. 在这五个问题中，神经网络模型更适用；
2. MCMC 方法为考察参数空间上重要的区域并在得出的模型上按其拟合质量进行平均提供了一个有效的方法。

贝叶斯方法可很好地用在平滑参数化的模型，比如神经网络；但还不清楚它是否也适用于不平滑的模型中，比如树模型。


[^1]: 原文脚注 3：我们也感谢 Isabella Guyon 在准备本节的结果中给予的帮助。
[^2]: 指的是 Neal and Zhang (2006) 对特征不同形式的预处理方式，前者是利用 t 检验的单变量筛选，后者是自动的相关性测定（ARD）对特征进行缩减。
