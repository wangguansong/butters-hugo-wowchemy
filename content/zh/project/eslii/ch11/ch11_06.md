---
title: 11.6 示例：模拟数据
summary: >
  第 401-404 页。在模拟数据例子中，演示单隐层神经网络模型，以及隐藏单元个数和权重衰减参数对模型效果的影响。

date: 2019-02-18T21:07:00+08:00
lastmod: 2019-02-18T21:07:00+08:00

weight: 1106

---

数据的生成模型为两种加性误差模型 $Y=f(X)+\varepsilon$：

{{< math >}}
$$\begin{align}
\text{S 函数之和：} & Y = \sigma(a_1^T X) + \sigma(a_2^T X) + \varepsilon_1 \\
\text{径向（Radial）：} & Y = \prod_{m=1}^{10} \phi(X_m) + \varepsilon_2
\end{align}$$
{{< /math >}}

其中的 $X^T=(X_1,X_2,\dots,X_p)$，每个 $X_j$ 为标准高斯分布变量，在第一个模型中 $p=2$，在第二个模型中 $p=10$。

S 函数模型的真实参数为 $a_1=(3,3)^T$，$a_2=(3,-3)^T$；径向模型中 $\phi(t)=(1/2\pi)^{1/2}\exp(-t^2/2)$。$\varepsilon_1$ 和 $\varepsilon_1$ 都是高斯误差，为其选定的方差可使得两个模型的信噪比都为 4：

{{< math >}}
$$\frac{\operatorname{Var}(\operatorname{E}(Y|X))}
       {\operatorname{Var}(Y-\operatorname{E}(Y|X))} =
  \frac{\operatorname{Var}(f(X))}
       {\operatorname{Var}(\varepsilon)} \tag{11.18}$$
{{< /math >}}

训练集大小为 100，测试集大小为 10,000。拟合不同隐藏单元数量的带有权重衰减的神经网络模型，并记录 10 个随机初始权重下的平均测试误差 $\operatorname{E}_\text{Test}(Y-\hat{f}(X))^2$。虽然这里只生成了一个训练集，但普通的训练集通常也会得出一致的结果。[图 11.6](#figure-f1106) 展示了测试误差。注意其中的零隐藏单元模型指的就是线性最小二乘回归。神经网络模型完美地适用于 S 函数之和的数据模型，而且两个单元的模型也确实表现最佳，达到了接近贝叶斯错误率的误差。（回溯：使用平方误差的回归问题中的贝叶斯错误率是误差的方差；图中给出的是测试误差与贝叶斯错误率的相对比值）。然而需要注意，随着隐藏单元的增多，过拟合问题很快显现出来，在一些初始权重下模型甚至要差于线性模型（零隐藏单元）。即使是两个隐藏单元的模型，十个初始权重配置中也有两个的表现没有好于线性模型，这也体现了使用多个初始值的重要性。

{{< figure
  id="f1106"
  src="https://public.guansong.wang/eslii/ch11/eslii_fig_11_06.png"
  title="**图 11.6**：模拟数据例子中，测试误差相对于贝叶斯误差率（水平虚线）的比值的箱形图。左图的真实函数是两个 S 函数的和，右图的是一个径向函数。图中展示的是不同单元个数的单隐层神经网络在 10 个不同初始权重下的测试误差。"
>}}

在某种程度上来说，径向函数是神经网络最难处理的情况，因为它有球体的对称性而没有偏好的方向。从[图 11.6](#figure-f1106) 的右侧可见，这个场景中的表现确实不好，测试误差远高于贝叶斯误差率（注意右图和左图的纵坐标尺度不同）。实际上，一个常数拟合（比如样本平均）可达到 5 的相对误差（当信噪比为 4 时），可见神经网络的表现（随着单元个数增加）更加差于一个均值模型。

在这个例子中，权重衰减参数固定为 0.0005，这是一个轻微程度的正则化。[图 11.6](#figure-f1106) 中左侧的结果说明对更多数量的隐藏单元需要更强的正则化。

{{< figure
  id="f1107"
  src="https://public.guansong.wang/eslii/ch11/eslii_fig_11_07.png"
  title="**图 11.7**：模拟数据例子中，测试误差相对于贝叶斯误差率的比值的箱形图。真实函数是两个 S 函数的和。图中展示的是不同单元个数的单隐层神经网络在 10 个不同初始权重下的测试误差。两个图分别代表着没有权重衰减（左侧）和强权重衰减 $\lambda=0.1$（右侧）。"
>}}

在[图 11.7](#figure-f1107) 中，重复 S 函数之和的数据模型的模拟实验，其左侧为没有权重衰减，右侧为较强的权重衰减（$\lambda=0.1$）。缺少了权重衰减后，过拟合问题在较大数量的隐藏单元的模型中变得更加严重。$\lambda=0.1$ 的权重衰减在所有不同个数隐藏单元的模型中都得出了良好的结果，并且看起来没有出现由单元个数增加而带来的过拟合现象。最后，[图 11.8](#figure-f1108) 展示了十个隐藏单元的神经网络模型在权重衰减参数的很大取值范围中的测试误差。参数值 $0.1$ 大致是最优的取值。

{{< figure
  id="f1108"
  src="https://public.guansong.wang/eslii/ch11/eslii_fig_11_08.png"
  title="**图 11.8**：模拟数据例子中，测试误差（相对于贝叶斯误差率的比值）的箱形图。真实函数是两个 S 函数的和。图中展示的是不同权重衰减参数值的单隐层神经网络在 10 个不同初始权重下的测试误差。"
>}}

总的来说，这里要选择两个自由参数：权重衰减参数 $\lambda$ 和 隐藏单元个数 $M$。一个训练的策略是先将某一个参数固定在对应着最弱约束模型的取值上，从而确保模型可适用的函数足够丰富，然后用交叉验证来选择另一个参数。这个例子中，最弱约束的取值分别为零权重衰减和十个隐藏单元。对比[图 11.7](#figure-f1107) 的左图和[图 11.8](#figure-f1108)，可见测试误差对权重衰减参数的取值更敏感[^1]，因此首选对这个参数进行交叉验证。

[^1]: 原文为“更不敏感”。