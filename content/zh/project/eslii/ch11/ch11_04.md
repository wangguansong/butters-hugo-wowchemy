---
title: 11.4 神经网络的拟合
summary: >
  第 395-397 页。反向传播（BP）的拟合方法本质上是一个只用到一阶更新的梯度下降过程。神经网络的结构使这个过程具有局部性，因此可利用并行的计算结构。BP 的更新是一种批量学习，所以也可以进行在线训练。但由于只用到一阶梯度，BP 的收敛过程可能会非常慢。

date: 2019-02-17T13:26:00+08:00
lastmod: 2019-02-17T13:26:00+08:00

weight: 1104

---

神经网络模型中的未知参数通常被称为 **权重（weights）**；需要寻找使模型很好拟合训练集的权重取值。将全部权重的集合记为 $\theta$，其中包括：

{{< math >}}
$$\begin{align}
\{\alpha_{0m}, \alpha_m; m=1,2,\dots,M\} & & M(p+1)\text{ 个权重} \\
\{\beta_{0k}, \beta_k; k=1,2,\dots,K\} & & K(M+1)\text{ 个权重}
\end{align}\tag{11.8}$$
{{< /math >}}

对于回归问题，使用误差平方和作为拟合的测度（误差函数）：

{{< math >}}
$$R(\theta) = \sum_{k=1}^K \sum_{i=1}^N (y_{ik} - f_k(x_i))^2 \tag{11.9}$$
{{< /math >}}

对于分类问题，使用平方误差或交叉熵（偏差，deviance）：

{{< math >}}
$$R(\theta) = -\sum_{i=1}^N \sum_{k=1}^K y_{ik} \log f_k(x_i) \tag{11.10}$$
{{< /math >}}

其对应的分类函数则为 $G(x)=\arg\max_kf_k(x)$。在使用了 softmax 激活函数和交叉熵误差函数后，神经网络模型在隐藏单元中与线性对数几率回归模型完全一致，并且可用最大似然法来估计所有的参数。

$R(\theta)$ 的全局最小化解通常并不合适，因为它很可能是一个过拟合的解。一般会需要某种正则化处理：这可通过一个惩罚项来直接实现，或者间接地通过早停来实现。下一节会介绍具体细节。

最小化 $R(\theta)$ 的通用方法为梯度下降，在这个场景中称为**反向传播（back-propagation）**。由于模型的特定组成结构，可很容易地使用微分的链式法则推导出梯度。可以通过前向和后向地扫过网络来进行计算，只需要记录每个单元局部的相关数值。

以下详细地描述了平方误差损失下的反向传播。如式 11.5 所定义，令 $z_{mi}=\sigma(\alpha_{0m}+\alpha_m^Tx_i)$，以及 $z_i=(z_{1i},z_{2i},\dots,z_{Mi})^T$。则有：

{{< math >}}
$$\begin{align} R(\theta)
&\equiv \sum_{i=1}^N R_i \\
&= \sum_{i=1}^N \sum_{k=1}^K (y_{ik} - f_k(x_i))^2
\tag{11.11}\end{align}$$
{{< /math >}}

以及导数：

{{< math >}}
$$\begin{align}
\frac{\partial R_i}{\partial \beta_{km}}
&= -2 (y_{ik} - f_k(x_i)) g_k'(\beta_k^T z_i) z_{mi} \\
\frac{\partial R_i}{\partial \alpha_{m\ell}}
&= -\sum_{k=1}^K 2 (y_{ik} - f_k(x_i)) g_k'(\beta_k^T z_i)
\beta_{km} \sigma'(\alpha_m^T x_i) x_{i\ell}
\end{align}\tag{11.12}$$
{{< /math >}}

得到这些导数后，在第 $(r+1)$ 步迭代中的梯度下降更新的表达式为：

{{< math >}}
$$\begin{align}
\beta_{km}^{(r+1)} &= \beta_{km}^{(r)} - \gamma_r \sum_{i=1}^N 
\frac{\partial R_i}{\partial \beta_{km}^{(r)}} \\
\alpha_{m\ell}^{(r+1)} &= \alpha_{m\ell}^{(r)} -
\gamma_r \sum_{i=1}^N \frac{\partial R_i}{\partial \alpha_{m\ell}^{(r)}}
\end{align}\tag{11.13}$$
{{< /math >}}

其中的 $\gamma_r$ 为**学习率**（learning rate），将在稍后介绍。

现将式 11.12 写为：

{{< math >}}
$$\begin{align}
\frac{\partial R_i}{\beta_{km}} &= \delta_{ki} z_{mi} \\
\frac{\partial R_i}{\alpha_{m\ell}} &= s_{mi} x_{i\ell}
\end{align}\tag{11.14}$$
{{< /math >}}

数值 $\delta_{ki}$ 和 $s_{mi}$ 分别为当前模型在输出层和隐藏层的单元上的“误差”。由定义可见它们满足：

{{< math >}}
$$s_{mi} = \sigma'(\alpha_m^T x_i) \sum_{k=1}^K \beta_{km} \delta_{ki}
\tag{11.15}$$
{{< /math >}}

这也被称为 **反向传播等式（back-propagation equations）**。利用这个等式，式 11.13 中的更新可通过两次传递算法来实现。在 **前向传递（forward pass）** 中，固定当前的权重并通过式 11.5 计算预测值 $\hat{f}\_k(x_i)$。在 **后向传递（backward pass）** 中，计算出误差 $\delta\_{ki}$，然后再通过式 11.15 的反向传播得出误差 $s_{mi}$。最后使用两组误差，通过表达式 11.14 计算出式 11.13 更新过程中的梯度。

这个两次传递的过程就是所谓的反向传播。其也曾被称为 **delta 规则**（Widrow and Hoff, 1960）。交叉熵与误差平方和的计算过程一样，其推导过程见[练习 11.3](#练习-113)。

反向传播的优势在于它的简单和局部化的特性。在反向传播算法中，每个隐藏单元只与和其有连接的单元传递和接收信息。因此，算法可以在并行结构的计算机上快速地实现。

式 11.13 中的更新是一种 **批量学习（batch learning）**，其参数的更新是在所有训练样本上的求和。这种学习过程也可以在线（online）进行：每次处理一个观测样本，依次根据每个训练样本更新梯度，对所有的训练样本进行多次的循环。在这个过程中，式 11.13 中的总和就变成了一个单次的加法。一个 **训练轮回（training epoch）** 指对整体训练集的一次扫描。在线的训练使网络模型可以处理非常大的训练集，并可以随着新观测样本的到达更新权重。

批量学习的学习率 $\gamma_r$ 通常取一个常数，但也可以在每一次更新中用线搜索（line search）最小化误差函数来得出最优的取值。在线学习中的 $\gamma_r$ 应随着循环次数 $r\rightarrow\infty$ 而递减至零。这种学习是 **随机逼近（stochastic approximation）** 的一个形式（Robbins and Munro, 1951）；根据这方面的理论，当 $\gamma_r\rightarrow 0$、$\sum_r\gamma_r=\infty$ 并且 $\sum_r\gamma_r^2<\infty$ 时，可保证结果收敛（$\gamma_r=1/r$ 是一个满足条件的例子）。

反向传播的计算可能会非常慢，因此也通常不被使用。比如牛顿方法的二阶方法同样在这里也不令人满意，因为 $R$ 的二阶导数（海森，Hessian）矩阵可能会非常大。有更好的拟合方法，比如共轭梯度（conjugate gradient）方法和变尺度法（variable metric）方法。它们避免了对二阶导数矩阵的直接计算，同时仍可以达到快速的收敛。

----------

### 本节练习

#### 练习 11.3

Derive the forward and backward propagation equations for the
cross-entropy loss function.
