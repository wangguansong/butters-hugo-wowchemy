---
title: 11.2 投影寻踪回归
summary: >
  第 389-392 页。投影寻踪是一个泛逼近器，用输入变量线性组合的非线性函数可逼近任意的连续函数。它的本质思想与神经网络模型一致。

date: 2019-02-16T11:13:00+08:00
lastmod: 2022-04-01T11:59:00+08:00

weight: 1102

---

在一个普通的监督学习问题中，假设有一个 $p$ 维的输入向量 $X$，和一个目标变量 $Y$。令 $w_m$，$m=1,2,\dots,M$ 为未知参数的单位 $p$ 维向量。投影寻踪回归（projection pursuit regression，PPR）模型的形式为：

$$f(X) = \sum_{m=1}^M g_m(w_m^T X) \tag{11.1}$$

这是一个对衍生特征 $V_m=w_m^TX$ 的加性模型，而对输入变量本身则不是。其中未指定函数 $g_m$ 的形式，可用某个灵活的（flexible）平滑方法（见下文）连同 $w_m$ 的方向一同来估计出。

函数 $g_m(w_m^TX)$ 称为 $\mathbb{R}^p$ 上的一个**岭函数（ridge function）**。它只可在由向量 $w_m$ 所定义的方向上变化。数值变量 $V_m=w_m^TX$ 为 $X$ 在单位向量 $w_m$ 上的投影，要寻找使模型较好拟合的 $w_m$，这就是这个方法的名称“投影寻踪”的由来。[图 11.1](#figure-f1101) 展示了岭函数的几个例子。左图的例子为 $w=(1/\sqrt{2})(1,1)^T$，故这个函数只在 $X_1+X_2$ 的方向上变化。右图的例子为 $w=(1,0)^T$。

{{< figure
  id="f1101"
  src="https://public.guansong.wang/eslii/ch11/eslii_fig_11_01.png"
  title="**图 11.1**：两个岭函数的透视图。左图：$g(V)=1/[1+\exp(-5(V-0.5))]$，其中 $V=(X_1+X_2)/2$。右图：$g(V)=(V+0.1)\sin(1/(V/3+0.1))$，其中 $V=X_1$。"
>}}

表达式 11.1 中的 PPR 模型通用性很强，因为通过构建线性组合的非线性函数的操作可生成非常广泛的模型类型。例如，乘积项 $X_1\cdot X_2$ 就可写为 $[(X_1+X_2)^2-(X_1-X_2)^2]/4$，更高阶的乘积也可类似地被表达出。

实际上，如果 $M$ 取任意大，那么选择合适的 $g_m$ 后 PPR 模型可任意程度地逼近 $\mathbb{R}^p$ 上的任意连续函数。这样一类模型被称为 **泛逼近器（universal approximator）**。不过这种通用性也有相应的代价。对拟合模型的解释通常比较困难，因为每个输入变量在模型中的作用方式都很复杂而且是多方位的。所以，PPR 模型在预测中非常有用，却对产生对数据的一个可理解模型不太有帮助。一个例外是当 $M=1$ 时，这个模型在计量经济学中称为 **单一指数模型（single index model）**。它比线性回归模型的通用性稍强，并有相似的可解释性。

给定训练集 $(x_i,y_i)$，$i=1,2,\dots,N$，怎样拟合 PPR 模型？通过寻找函数 $g_m$ 和方向向量 $w_m$，$m=1,2,\dots,M$来最小化误差函数：

$$\sum_{i=1}^N \left[ y_i - \sum_{m=1}^M g_m(w_m^T x_i) \right]^2 \tag{11.2}$$

类似于其他的平滑问题，需要或显式或隐式地对 $g_m$ 添加复杂度约束，以避免得出过拟合解。

先考虑只有一个项的情况（$M=1$，并省略下标）。给定方向向量 $w$ 后，可生成衍生变量 $v_i=w^Tx_i$。那么这是一个一维的平滑问题，可适应任意的散点图平滑器，比如平滑样条，来得出 $g$ 的估计。

另一方面，给定了函数 $g$，需要对 $w$ 来最小化表达式 11.2。这可用高斯牛顿方法来解决。这是一个拟牛顿法（quasi-Newton）方法，其中舍弃了 $g$ 的二阶导数的海森（Hessian）矩阵部分。这可简单的推导如下。令 $w_\text{old}$ 为对 $w$ 的当前估计，则有：

$$g(w^T x_i) \approx g(w_\text{old}^T x_i) +
g'(w_\text{old}^T x_i)(w - w_\text{old})^T x_i
\tag{11.3}$$

可得出：

$$\begin{align}
& \sum_{i=1}^N [y_i - g(w^T x_i)]^2 \\\\
\approx & \sum_{i=1}^N g'(w_\text{old}^T x_i)^2 \left[
\left(w_\text{old}^T x_i +
\frac{y_i - g(w_\text{old}^T x_i)}
{g'(w_\text{old}^T x_i)}\right) -w^T x_i
\right]^2 \tag{11.4}\end{align}$$

为了最小化上式的右侧，以 $w_\text{old}^Tx_i+(y_i-g(w_\text{old}^Tx_i))/g'(w_\text{old}^Tx_i)$ 为目标变量，使用权重 $g'(w_\text{old}^Tx_i)^2$，对输入变量进行无截距（偏差）项的最小二乘回归。从而得出更新系数向量 $w_\text{new}$。

循环进行 $g$ 和 $w$ 的估计的两个步骤，直至结果收敛。如果 PPR 模型有不止一项，则可用一个前向分段的方式来构建，在每一段中添加一对 $(w_m,g_m)$。

以下为一些实施中的细节问题。

- 虽然原则上可以使用任意的平滑方法，但最好是使用可得到导数的方法。局部回归和平滑样条就是这样的方法。
- 在每个步骤之后，可用第九章中介绍的回修方法重新调整之前步骤得出的 $g_m$。虽然这样可能会使最终的项数更少，但并不清楚是否会改善预测表现。
- 理论上也可重新调整 $w_m$，但通常不这样做（也是为避免过多的计算）。
- 项的个数 $M$ 通常也从前向分段的过程中估计出。当新增项无法明显改善模型拟合程度后，则停止构建模型的迭代过程。也可以使用交叉验证来确定 $M$ 取值。

投影寻踪的思想在很多其他场景中也有应用，例如密度估计（Friedman et al., 1984; Friedman, 1987）。具体可见[第 14.7 节]({{< relref "../ch14/ch14_07.md" >}})介绍的 ICA 以及其与探索投影寻踪（exploratory projection pursuit）的关联。然而投影寻踪回归模型并没有在统计学领域中被广泛地使用，可能是因为在它出现时（1981）的计算机能力无法满足它的计算需求。但它确实是一个重要的认知上的进步，在神经网络领域中又再次重生，本章的其余部分将加以介绍。