---
title: 6.7 径向基函数与核函数
summary: >
  第 212-214 页。基函数展开的方法的灵活性在于大量的基函数，核函数方法的灵活性在于局部性，径向基函数则结合了上述两者。

date: 2018-11-22T19:50:00+08:00
lastmod: 2022-06-15T15:22:00+08:00

weight: 607

---

在第五章中，函数被表述为基函数的拓展：$f(x)=\sum_{j=1}^M \beta_j h_j(x)$。使用基函数展开建模的灵活性在于选择合适的基函数族， 然后再通过筛选、正则化、或两者并举来控制函数表达式的复杂度。有些类型的基函数存在着定义在局部的元素；例如，B-样条即定义在 $\mathbb{R}$ 的局部。若想要在特定的局部获得更灵活的模型，则需要在该区域上的表达式增加更多的基函数（在 B-样条例子中则是增加更多结点）。$\mathbb{R}$ 的局部基函数的张量积可生成 $\mathbb{R}^p$ 上的局部基函数。并不是所有基函数都是局部的。例如，样条中的截断幂次基函数，或神经网络（第十一章）中的 S 形基函数 $\sigma(\alpha_0+\alpha x)$。尽管如此，由于系数的特定的符号和取值可能会彼此消除某些全局效应，其组成的函数 $f(x)$ 仍可表现出局部性。例如，对截断幂次基函数，存在着等价的 B-样条基函数，使得两者生成的函数空间是一样的；在这种场景中，全局效应被完全消除。

核函数方法的灵活性在于在目标点 $x_0$ 的局部区域拟合简单的模型。局部性是通过一个加权核函数 $K_\lambda$ 实现的，每个样本被赋予权重 $K_\lambda(x_0, x_i)$。

径向基函数则结合了上述两种思想，将核函数 $K_\lambda(\xi,x)$ 作为一个基函数。这引入了模型：

$$\begin{align}
f(x) &= \sum_{j=1}^M K_{\lambda_j}(\xi_j, x) \beta_j \\\\
  &= \sum_{j=1}^M D\left(\frac{\\|x-\xi_j\\|}{\lambda_j}\right) \beta_j
\tag{6.28}\end{align}$$

其中的每个基函数成分被一个位置或 **原型（prototype）** 参数 $\xi_j$ 和一个尺度参数 $\lambda_j$ 索引。$D$ 的常见选择为标准高斯密度函数。有很多种获得参数 $\\{\lambda_j,\xi_j,\beta_j\\},j=1,\dots,M$ 的方法。简单起见，这里对回归问题着重于最小二乘方法，并使用高斯核函数。

* 对所有的参数进行平方和的最优化：
  $$\begin{align}
  \min_{\\{\lambda_j, \xi_j, \beta_j\\}\_1^M}
  \sum_{i=1}^N \Bigg( & y_i - \beta_0 - \\\\
  & \sum_{j=1}^M
  \beta_j \exp\bigg\\{
    -\frac{(x_i-\xi_j)^T(x_i-\xi_j)}{\lambda_j^2}
  \bigg\\}\Bigg)
  \tag{6.29}\end{align}$$
  这个模型一般被称为径向基（RBF）网络，可视为是第十一章中的 S 状神经网络的一个替代方法；参数 $\xi_j$ 和 $\lambda_j$ 起到了权重的作用。这个准则函数是非凸的，有多个局部最小点，其最优化的算法与神经网络中使用的类似。

* 对不同 $\beta_j$ 分别估计 $\\{\lambda_j,\xi_j\\}$。给定后者，则前者的估计是一个简单的最小二乘问题。通常会通过一个无监督的方式只从 $X$ 的分布来选择核函数的参数 $\lambda_j$ 和 $\xi_j$。其中一种方法是对训练样本 $x_i$ 拟合一个高斯混合密度模型，同时得到多个中心 $\xi_j$ 和尺度 $\lambda_j$。另一个更特殊的方法是通过聚类方法来定位原型 $\xi_j$，并将 $\lambda_j=\lambda$ 作为一个超（hyper）参数。这类方法明显的问题是条件分布 $\operatorname{Pr}(Y|X)$ 和特别是 $E(Y|X)$ 对选择集中的位置没有任何影响。相应地，其好处是比较容易实现。

{{< figure
  id="f0616"
  src="https://public.guansong.wang/eslii/ch06/eslii_fig_06_16.png"
  title="**图 6.16**：$\mathbb{R}$ 上的固定宽度的高斯径向基函数可能造成空洞（上图）。重标准化的高斯径向基函数可避免这个问题，其产生的基函数在某些方面与 B-样条类似。"
>}}

尽管常数 $\lambda_j=\lambda$ 的假设可缩减参数集的大小，但这可能会在空间上形成 **空洞（holes）**。如[图 6.16](#figure=f0616) 中上图所示，在 $\mathbb{R}^p$ 中的一些区域上，没有任一核函数有相应的支撑集。使用 **重标准化（renormalized）** 的径向基函数可解决这个问题（下图）：

$$h_j(x) = \frac
{D(\\|x-\xi_j\\|/\lambda)}{\sum_{k=1}^M D(\\|x-\xi_k\\|/\lambda)}
\tag{6.30}$$

等式 6.2 中的 $\mathbb{R}^p$ 上的 Nadaraya-Watson 核函数回归估计可被视为用重标准化径向基函数的展开：

$$\begin{align}
\hat{f}(x_0)
&= \sum_{i=1}^N y_i \frac{K_\lambda(x_0, x_i)}{\sum_{i=1}^N K_\lambda(x_0, x_i)} \\\\
&= \sum_{i=1}^N y_i h_i(x_0)
\tag{6.31}\end{align}$$

其中的基函数 $h_i$ 位置在每个样本处，系数为 $y_i$；即 $\xi_i=x_i$，$\hat{\beta}\_i=y_i$，$i=1,\dots,N$。

注意展开式 6.31 与径向基函数引入的正则化问题解 5.50（[第 5.8 节]({{< relref "../ch05/ch05_08.md" >}})，第 169 页）之间的相似之处，将现代的“核方法”与局部拟合方法关联了起来。