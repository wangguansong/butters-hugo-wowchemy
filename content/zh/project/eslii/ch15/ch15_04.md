---
title: 15.4 随机森林的分析 😱
summary: >
  第 597-602 页。对随机森林偏差和方差的分解。

date: 2022-05-31T20:00:00+08:00
math: true

type: book
weight: 1504

authors: ["Butters"]
tags: ["译文"]
categories: ["统计学习基础（译注）"]

---

本节我们将分析随机森林中实现额外随机性的内在机制。在以下讨论中我们将聚焦在回归问题和平方误差损失函数，原因是这样可以更清晰地阐述主要观点，而 0-1 损失函数下的偏差和方差会更复杂（见[第 7.3.1 节]({{< relref "../ch07/ch07_03.md" >}})。此外，即使是在分类问题中，我们也可以将随机森林的平均视为是类别后验概率的一个估计，而偏差和方差来描述它也是合理的。

### 15.4.1 方差和去相关的作用

随机森林回归估计量的极限形式为（$B\rightarrow\infty$）：

$$\hat{f}\_{rf}(x) = \operatorname{E}\_{\Theta|\mathbf{Z}}T(x;\Theta(\mathbf{Z}))\tag{15.4}$$

在这里我们直接清楚地表明了它对训练集 $\mathbf{Z}$ 的依赖。然后考虑在某个目标点 $x$ 处的估计。根据式 15.1 可见：

$$\operatorname{Var}\hat{f}\_{rf}(x)=\rho(x)\sigma^2(x)\tag{15.5}$$

在这个式子里：
- $\rho(x)$ 为在随机森林平均当中任意一对树模型之间的**样本**相关系数。
  $$\rho(x)=\operatorname{corr}[T(x;\Theta_1(\mathbf{Z})), T(x;\Theta_2(\mathbf{Z}))]\tag{15.6}$$
  其中 $\Theta_1(\mathbf{Z})$ 和 $\Theta_2(\mathbf{Z})$ 分别为随机森林中随机抽取的一对树模型，它们都是分别在 $\mathbf{Z}$ 的随机抽样上训练的。
- $\sigma^2(x)$ 为任意单个随机抽取的树模型的样本方差。
  $$\sigma^2(x)=\operatorname{Var}T(x;\Theta(\mathbf{Z}))\tag{15.7}$$

这里的 $\rho(x)$ 容易与另一个相关性的定义混淆，即在某个**给定**的随机森林集成模型中的树模型拟合值之间的平均相关系数。后者是将拟合的树模型写为 $N$ 维向量，计算这些向量两两之间相关系数的平均值，它是条件于（或给定）样本数据上的。后者的条件相关系数与取平均的过程并不直接有关系，而且 $\rho(x)$ 中出现的 $x$ 也为我们提示了两者的不同。相反地，$rho(x)$ 是一对随机森林的树模型在 $x$ 处取值的理论相关系数，它的样本需要通过重复地从总体中抽取训练样本 $\mathbf{Z}$ 以及从随机森林中抽取一对树模型。在统计学的术语中，它是在 $\mathbf{Z}$ 和 $\Theta$ 的样本分布（sampling distribution）下所计算的相关性。

更准确地说，式 15.6 和 15.7 中的取平均计算所针对的随机变动，包含了以下两个部分：
- 条件于 $\mathbf{Z}$ 的随机变动：由样本的自助抽样以及在每次分割进行的特征变量选取。
- $\mathbf{Z}$ 本身的随机变动。

而实际上，在点 $x$ 处的一对树模型拟合值的条件协方差矩阵为 0，这是因为自助抽样和特征选取都是独立同分布的（i.i.d.）；见[练习 15.5](#练习-155)。

接下来的演示说明将基于这个模拟模型：

$$Y=\frac{1}{\sqrt{50}}\sum\_{j=1}^{50}X_j+\epsilon\tag{15.8}$$

其中所有的 $X_j$ 和 $\epsilon$ 都是独立同分布的高斯分布。我们使用了 500 组大小为 100 的的训练集，以及一组大小为 600 的测试位置点的集合。由于回归树模型是对 $\mathbf{Z}$ 非线性的，接下来呈现的结果可能会根据模型的结构（测试集中 $x$ 点的不同）而有所不同。

[图 15.9](#figure-f1509) 展示了一对树模型之间的相关系数 15.6 会随着 $m$ 的下降而下降。如果一对树模型共同的分割变量越少，它们在不同的训练集 $\mathbf{Z}$ 下对某个 $x$ 的预测结果就越不可能相似。

{{< figure
  src="https://public.guansong.wang/eslii/ch15/eslii_fig_15_09.png"
  id="f1509"
  title="**图 15.9**：随机森林回归算法中的一对树模型之间的相关系数对 $m$ 的函数。箱线图表现的是在（测试集）600 个随机抽取的目标点 $x$ 处的相关系数。"
>}}

{{< figure
  src="https://public.guansong.wang/eslii/ch15/eslii_fig_15_10.png"
  id="f1510"
  title="**图 15.10**：模拟数据结果。左图为随机森林中一个树模型方差平均数对 $m$ 的函数。“Within $\mathbf{Z}$”指的是总体方差中样本内贡献的方差的平均数，它是式 15.9 中来自于自助抽样和分割变量选取的那部分方差。“Total”（总体方差）同时包含了 $\mathbf{Z}$ 的样本随机变动。水平线为单个全深度（fully grown）树模型（不进行自助抽样）的方差平均数。右图展示了均方误差、平方偏差、和集成的方差，三者对 $m$ 的函数。请注意方差的纵坐标在图的右侧（与左侧的尺度相同，但位置不同）。水平线为一个全深度（fully grown）树模型的平均平方偏差。"
>}}

在[图 15.10](#figure-f1510) 的左图中，我们考察单个树模型预测的方差 $\operatorname{Var}T(x;\Theta(\mathbf{Z}))$（图中是在模拟模型中随机生成的 600 个目标点 $x$ 上的方差的平均）。这是总体方差，可利用一些条件方差的性质将其分解成两部分（[练习 15.5](#练习-155)）：

$$\begin{align}
\operatorname{Var}\_{\Theta,\mathbf{Z}}T(x;\Theta(\mathbf{Z})) =&
 \operatorname{Var}\_{\mathbf{Z}}\operatorname{E}\_{\Theta|\mathbf{Z}}T(x;\Theta(\mathbf{Z})) +\\\\
& \operatorname{E}\_{\mathbf{Z}}\operatorname{Var}\_{\Theta|\mathbf{Z}}T(x;\Theta(\mathbf{Z})) \\\\
\text{Total Variance} =&
 \operatorname{Var}\_{\mathbf{Z}} \hat{f}\_{rf}(x) +\\\\
&  \text{within-}\mathbf{Z}\text{ Variance}
\end{align}\tag{15.9}$$

第二项为样本内（within-$\mathbf{Z}$）方差，来自于（随机森林的）随机化操作，随着 $m$ 的降低而升高。第一项实际就是随机森林集成的样本方差（右图中的 Variance 线），随着 $m$ 的降低而降低。单个树模型的方差在 $m$ 的取值范围内并没有特别大的变化，因此基于 15.5，（随机森林）集成后的方差要远低于单个树的方差。

### 15.4.2 偏差

与自助聚合相同，随机森林的偏差和其中任意单个树模型 $T(x;\Theta(\mathbf{Z}))$ 的偏差一样：

$$\begin{align}
\operatorname{Bias} &= \mu(x) - \operatorname{E}\_{\mathbf{Z}}\hat{f}\_{rf}(x)\\\\
  &= \mu(x) - \operatorname{E}\_{\mathbf{Z}}\operatorname{E}\_{\Theta|\mathbf{Z}}T(x;\Theta(\mathbf{Z}))
\tag{15.10}\end{align}$$

而且这也一般（在绝对值上）会高于 $\mathbf{Z}$ 上训练的一个未剪枝的树模型，因为随机化和降维的样本空间带来了额外的约束。因此自助聚合或随机森林在预测结果上的改进完全是因为方差的缩减。

对偏差的研究依赖于未知的真实（回归）函数。[图 15.10](#figure-f1510) 的右图展示了上面的加性模型模拟数据（500 次实现估计）的平方偏差。尽管在不同的模型中偏差曲线的形状和取值可能不同，但一般的趋势是随着 $m$ 降低偏差会升高。图中展示了均方误差曲线，对 $m$ 的选择就又是一个经典的偏差方差权衡。在所有的 $m$ 取值下，随机森林的平方偏差都大于单个树模型（水平线）。

这些模式与岭回归有相似之处（[第 3.4.1 节]({{< relref "../ch03/ch03_04.md" >}})。岭回归适用的场景是当（线性模型中）有很多的特征变量而它们的系数大小接近；岭回归将它们的系数向零收缩，并且强相关变量的系数向彼此靠近。即使训练集的大小可能限制了所有的特征变量被包括在模型中，但岭回归的正则化使模型更稳定，并且可以让所有的变量都能起到作用（即使其系数最终可能为零）。较小 $m$ 的随机森林在进行类似的平均。每一个相关的特征变量都有作为分割变量的机会，然后集成的平均操作降低了任意单个变量的贡献。由于模拟示例 15.8 是一个依赖了所有特征变量的线性模型，岭回归可以达到一个更低的均方误差（大约为 0.45，$\operatorname{df}(\lambda_\text{opt}）\approx 29$）。

### 15.4.3 自适应最近邻

随机森林分类器与 k 最近邻分类器（[第 13.3 节]({{< relref "../ch13/ch13_03.md" >}})）有很多共同之处；实际上是一个加权版本的最近邻。由于每个树模型都训练到全深度，那么对某个特定的 $\Theta^\*$，$T(x;\Theta^\*(\mathbf{Z}))$ 则是训练集中某个观测样本的输出变量值。树模型的训练算法就是在寻找一个到达那个观测样本的“最优”路径，它从那些备选的自变量中选择区分度最高的一部分。取平均的过程就是在对这些观测样本的输出变量加权重，也就是最终对预测结果的投票。因此，通过随机森林的投票机制，那些与目标点接近的观测点可获得权重（等价于一个核函数），然后组合形成一个分类的决策。

[图 15.11](#figure-f1511) 在混合数据上演示了 3 最近邻和随机森林生成的决策边界的相似性。

{{< figure
  src="https://public.guansong.wang/eslii/ch15/eslii_fig_15_11.png"
  id="f1511"
  title="**图 15.11**：混合数据上的随机森林和 3 最近邻的对比。随机森林中的单个树模型自然地沿着坐标系方向分割区域，产生的决策区域也基本是按坐标系方向划分的。"
>}}

----------
### 本节练习

#### 练习 15.5

[^2]: 原文脚注 4：注意当节点内的输出变量值相同时（pure node），不会再进行分割，因此在一个终节点内可能有不止一个观测样本。