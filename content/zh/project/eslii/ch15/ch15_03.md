---
title: 15.3 随机森林的细节
summary: >
  第 592-597 页。随机森林可以使用自助外样本近似交叉验证、生成样本相似图、和变量重要性。

date: 2022-05-31T16:30:00+08:00

weight: 1503

---

我们简略说明随机森林在分类问题和回归问题上的区别。在分类问题中，随机森林从每个树模型得到一个类别的投票，然后分类给票数最多的类别（参考[第 8.7 节]({{< relref "../ch08/ch08_07.md" >}})中在自助聚合中类似的讨论）。在回归问题中，只需要对每个树模型在目标点 $x$ 处的预测值做回归即可，如式 15.2。另外，模型的发明者的建议如下：

- 在分类问题中，默认的 $m$ 值为 $\lfloor\sqrt{p}\rfloor$，最小节点大小为 1。
- 在回归问题中，默认的 $m$ 值为 $\lfloor p/3 \rfloor$，最小节点大小为 5。

在实践中，这些参数的最优取值会依赖于具体的问题，并且可被视为待调节的参数。在[图 15.3]({{< relref "../ch15/ch15_02.md" >}}) 中，$m=6$ 的表现就要好于默认值 $\lfloor 8/3 \rfloor = 2$。

### 15.3.1 自助外样本

随机森林的一个重要特性是对 **自助外样本（out-of-bag，oob）** 的使用：

> 对每一个样本点 $z_i=(x_i,y_i)$，在构建它的随机森林预测值时只对使用了不包含 $z_i$ 的自助抽样样本的那些树模型取平均。

自助外误差估计与 N 折交叉验证得到的结果几乎一致（见[练习 15.2](#练习-152)）。因此与其他非线性估计方法不同的是，随机森林可以在拟合过程的同时就完成了交叉验证。当自助外误差趋于稳定后，就可终止训练过程。

[图 15.4](#figure-f1504) 展示了垃圾邮件数据上的自助外（oob）误分类误差率，并与误差率做对比。
尽管在例子中使用到了 2500 个树模型的平均，从图中看大概 200 个树模型就足够了。

{{< figure
  src="https://public.guansong.wang/eslii/ch15/eslii_fig_15_04.png"
  id="f1504"
  title="**图 15.4**：垃圾邮件训练集上计算的自助外（oob）误差率与测试集的测试误差率的对比。"
>}}

### 15.3.2 变量重要性

随机森林也可以构建 **变量重要性（variable importance）** 示意图，其做法与梯度提升模型（[第 10.13 节]({{< relref "../ch10/ch10_13.md" >}})）完全一致。在每个树模型的每一次分割中，分割准则的改进大小就组成了这个分割变量重要性测度，为每个变量各自汇总在所有树模型中产生的重要性。[图 15.5](#figure-f1505) 为在垃圾邮件数据中如此计算出的变量重要性；可以与梯度提升方法所对应的[图 10.6]({{< relref "../ch10/ch10_08.md" >}})做比较。。提升方法完全忽略掉了一些变量，而随机森林没有忽略。对备选分割变量的随机选择增加了任意单个变量进入随机森林模型的机会，而在提升方法中则没有这样的机制。

{{< figure
  src="https://public.guansong.wang/eslii/ch15/eslii_fig_15_05.png"
  id="f1505"
  title="**图 15.5**：垃圾邮件数据上训练的随机森林分类模型的变量重要性示意图。左图是基于分割的基尼（Gini）系数，与梯度提升一样。变量重要性的排序与梯度提升中的排序基本吻合（图 10.6）。右图使用了自助外（oob）样本的随机置换操作来计算变量重要性，它的重要性分布得更均匀。"
>}}

随机森林也可以使用自助外（oob）样本来构建一个不同的变量重要性测度，用来测量每个变量的预测能力。具体操作是当训练了第 $b$ 个树模型后，将自助外（oob）样本传入到这个树模型中，然后计算预测的准确率。然后将 oob 样本中的第 $j$ 个变量进行随机的置换（randomly permuted），然后再一次计算预测的准确率。将所有树模型中这个随机置换所带来的准确率下降取平均，这个平均数就作为随机森林中变量 $j$ 的重要性的一个测度。[图 15.5](#figure-f1505) 的右图就是按最大值的百分比给出的重要性测度。虽然两种方法计算出的变量排序很相似，不过右图中变量之间的重要性测度差异更小。随机置换的操作实际上是消除了一个变量所起到的作用，这与在线性模型中将一个系数置为零是类似的（[练习 15.7](#练习-157)）。这并不等同于去除这个变量对预测结果的影响，因为如果去除这个变量后再次拟合模型，其他变量可能会作为这个变量的代理变量而起作用。

### 15.3.3 样本相似图

**样本相似图（proximity plot）** 是随机森林的独特产出之一。[图 15.6](#figure-f1506) 展示了在[第 2.3.3 节]({{< relref "../ch02/ch02_03.md" >}}) 中定义的混合数据中的样本相似图。在拟合随机森林的过程中，可以在训练集上累积出一个 $N\times N$ 的相似性矩阵。在每个树模型中，若任意一对自助外（oob）观测点同时出现在一个终节点，它们的相似性就增加一。然后利用多维尺度分析（[第 14.8 节]({{< relref "../ch14/ch14_08.md" >}})）将这个相似性矩阵表达为两个维度。这里的思想是，虽然数据可能是多维度的、包含了多种类型的变量、或其他复杂情况，样本相似图可以给出一个在随机森林分类器视角下哪些样本实际上更接近的指示。

{{< figure
  src="https://public.guansong.wang/eslii/ch15/eslii_fig_15_06.png"
  id="f1506"
  title="**图 15.6**：（左图）：混合数据拟合的随机森林分类器的样本相似图。（右图）：混合数据拟合的随机森林分类器的决策边界和训练集数据。有六个点同时在每个图中进行了标记。"
>}}

即使是来自不同的数据，随机森林的样本相似图也通常看起来是差不多的，这对它们是否真的有用带来了疑问。它们一般呈现星形，每个类别一个角；分类的效果越好，这种形状就越明显。

由于所使用的混合数据是二维的，我们可以把样本相似图中的点映射到原始坐标系中，这样可以更好地理解它们代表的含义。看起来类别比较单一的区域中的点映射在星形的极点上，而决策边界附近的点映射在星形的中心附近。考虑到相似性矩阵的构建方法，这也就在意料之中。单一类别区域中邻近的点通常会处在同一个节点中，因为当一个终节点只有一个类别时，随机森林中的树模型算法就不会继续进行分割了。另一方面，若两个点距离相近但却分属不同的类别，它们就只是有时可能处于同一个终节点而不会总是在同一个终节点。

### 15.3.4 随机森林和过拟合

当特征变量的数量较大，而其中有价值的变量所占比例较小时，较小 $m$ 值的随机森林模型可能表现不好。在每次分割的时候，有价值的特征变量被选中的概率可能会比较小。[图 15.7](#figure-f1507) 展示了可说明这个问题的模拟例子。图标题以及[练习 15.3] 提供了更多的细节。在每对结果的顶部标记了随机森林树模型在任意一个分割中选中（至少）一个有价值变量的超几何分布概率值。（在这个模拟中，有价值的变量的取值尺度相同）。随着这个概率变小，提升方法和随机森林之间的差距变大。当有价值的变量数量增加时，随机森林的表现对噪声变量数量的增加却出乎意料地稳健。例如若有 6 个有价值变量和 100 个噪声变量，假设 $m=\sqrt{6+100}\approx 10$，则任一分割中选取了有价值变量俄概率为 0.46。根据[图 15.7](#figure-f1507) 所示，这并不会影响随机森林对比提升方法的表现。这种稳健性大多是由于（最终的）误分类损失对每个单个树模型中概率估计的偏差和方差相对地不敏感。在[下一节]({{< relref "../ch15/ch15_04.md" >}})中会更多考虑回归问题中的随机森林。

{{< figure
  src="https://public.guansong.wang/eslii/ch15/eslii_fig_15_07.png"
  id="f1507"
  title="**图 15.7**：在噪声（干扰项）变量数量增大的问题中，随机森林和梯度提升的对比。在每个场景中，真实的决策边界依赖于两个变量，而所包含的噪声干扰项变量个数逐渐增加。随机森林模型中使用默认的参数值 $m=\sqrt{p}$。每对结果的顶部给出了在任意分割时选入了至少一个有价值变量的概率。每对结果是基于 50 次实现，训练集大小为 300，测试集大小为 500。具体见练习 15.3。"
>}}

另外有一种说法认为随机森林“无法过拟合”。有一点是没有疑问的，就是提高 $B$ 不会造成随机森林过拟合；与自助聚合相似，随机森林的估计（式 15.2）以 $\Theta$ 的 $B$ 次实现的平均来逼近期望：

{{< math >}}
$$\hat{f}_{rf}(x) = \operatorname{E}_{\Theta} T(x; \Theta)
  = \lim_{B \to \infty} \hat{f}(x)_{rf}^B \tag{15.3}$$
{{< /math >}}

这里的 $\Theta$ 的分布是条件于训练集数据。然而，这个极限可能会对数据过拟合；全深度（fully grown）的树模型的平均可能会得到一个过复杂的模型，并带来不必要的方差。Segal (2004) 演示了在随机森林中控制单个树模型的深度可以稍微提升模型的效果。我们的经验是使用全深度的树模型一般不会带来太大问题，并且会少一个待调整的参数。

[图 15.8](#figure-f1508) 在一个简单回归例子中展示了深度控制的小幅度效果。分类模型对方差更不敏感，这种过拟合的效应几乎不会出现在随机森林分类模型中。

{{< figure
  src="https://public.guansong.wang/eslii/ch15/eslii_fig_15_08.png"
  id="f1508"
  title="**图 15.8**：在随机森林回归中，树模型的大小对误差的影响。在这个例子中，真实的（决策边界）曲面是 12 个特征变量其中两个变量的加性结构函数，外加一个方差为一的高斯噪声。这里通过最小节点大小来控制树模型的深度；最小节点大小越小，树模型越深。"
>}}

----------

### 本节练习

#### 练习 15.2

Show that as the number of bootstrap samples B gets large, the
oob error estimate for a random forest approaches its N -fold CV error
estimate, and that in the limit, the identity is exact.

#### 练习 15.3

Consider the simulation model used in Figure 15.7 (Mease and Wyner, 2008).
Binary observations are generated with probabilities

{{< math >}}
$$\operatorname{Pr}(Y=1|X) = q + (1 - 2q) \cdot I \left[
  \sum_{j=1}^J X_j > J/2 \right] \tag{15.11}$$
{{< /math >}}

where $X\sim U[0,1]^p$, $0\leq q\leq\frac{1}{2}$, and $J\leq p$ is some
predefined (even) number. Describe this probability surface, and give the
Bayes error rate.

#### 练习 15.7

Suppose we fit a linear regression model to $N$ observations with response $y_i$
and predictors $x_{i1},\dots,x_{ip}$. Assume that all variables are standardized
to have mean zero and standard deviation one. Let RSS be the mean-squared
residual on the training data, and $\hat{\beta}$ the estimated coefficient.
Denote by $RSS_j^*$ the mean-squared residual on the training data using the
same $\hat{\beta}$, but with the $N$ values for the jth variable randomly permuted
before the predictions are calculated. Show that

{{< math >}}
$$\operatorname{E}_P [RSS_j^* - RSS] = 2 \hat{\beta}_j^2 \tag{15.13}$$
{{< /math >}}

where $\operatorname{E}_P$
denotes expectation with respect to the permutation distribution.
Argue that this is approximately true when the evaluations are done using
an independent test set.