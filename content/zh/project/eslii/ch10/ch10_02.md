---
title : 10.2 加性模型的提升拟合
summary: >
  第 341-342 页。将每个基学习器视为基函数，那么提升方法可理解为是一个加性模型的拟合。

date: 2019-01-19T19:00:00+08:00
lastmod: 2022-06-14T14:30:00+08:00

weight: 1002

---

提升方法非常有效的原因并不神秘，其关键在于等式（10.1）。提升方法可以看作是对一组基函数的加性展开（additive expansion）的拟合。这里的基函数就是每个分类器 $G_m(x)\in\\{-1,1\\}$。更一般地，基函数展开的形式如下

$$f(x) = \sum_{m=1}^M \beta_m b(x; \gamma_m) \tag{10.3}$$

这里的 $\beta_m,m=1,2,\dots,M$ 为展开系数，$b(x;\gamma)\in\mathcal{R}$ 通常为输入多维向量 $x$ 的简单的函数，含有一组参数 $\gamma$。在第五章有更多关于基函数拓展的细节。

类似这样的加性展开是本书中很多统计学习技术的核心：

- 单隐层神经网络（第十一章），$b(x;\gamma)=\sigma(\gamma_0+\gamma_1^Tx)$，其中的 $\sigma(t)=1/(1+e^{−t})$ 为 S（sigmoid）函数，$\gamma$ 为输出变量的线性组合的参数。
- 信号处理中普遍使用小波函数（[第 5.9.1 节]({{< relref "../ch05/ch05_09.md" >}})），$\gamma$ 是控制母小波函数位置和尺度变化的参数。
- 多变量自适应回归样条（[第 9.4 节]({{< relref "../ch09/ch09_04.md" >}})）使用了截幂（truncated-power）样条基函数，$\gamma$ 为控制节点的变量和位置的参数。
- 树模型中，$\gamma$ 参数中包含了内部节点处的分割变量和分割点，以及终节点的预测。

这些模型的拟合通常是对损失函数，例如平方误差或似然损失函数，在训练集上的平均求最小化：

$$\min_{\\{\beta_m, \gamma_m\\}\_1^M} \sum_{i=1}^N
L \left( y_i, \sum_{m=1}^M\beta_m b(x_i; \gamma_m) \right)
\tag{10.4}$$

对于很多损失函数 $L(y,f (x))$ 和基函数 $b(x;\gamma)$，这种拟合的数值最优化过程的计算非常复杂。然而，如果拟合单个基函数的子优化问题（表达式 10.5）存在快速的解法，那么整体的优化通常可以找到另一种简单的解法。

$$\min_{\beta, \gamma} \sum_{i=1}^N L(y_i, \beta b(x_i; \gamma))
\tag{10.5}$$