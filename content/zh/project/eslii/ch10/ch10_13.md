---
title: 10.13 模型解释
summary: >
  第 367-370 页。梯度提升方法中可用相对重要性来衡量自变量与输出变量的相关程度排序；可用部分依赖图来直观地展示自变量子集对近似函数的影响。

date: 2019-01-31T23:30:00+08:00
lastmod: 2022-06-14T15:53:00+08:00

weight: 1013

---

单个的决策树模型的可解释性很高。整个模型可完整地表达为一个很容易可视化的简单二维图（二叉树）。树模型的线性组合（式 10.28）不再有这个重要的特性，也因此必须以不同的方式进行解释。

### 10.13.1 自变量的相对重要性

在数据挖掘的应用中，输入自变量与结果的相关程度几乎都不相同。对输出变量有重要影响的通常只有其中的几个；而大部分变量都是不相关的，可以直接舍弃。所以通常要了解每个输入变量在预测输出变量上的相对重要性或贡献度。

对于一个单个的决策树模型 $T$，Breiman etal. (1984) 提出了每个自变量 $X_\ell$ 的相关程度的一个度量：

{{< math >}}
$$\mathcal{I}_\ell^2(T) = \sum_{t=1}^{J-1}
\hat{\imath}^2 I(v(t) = \ell) \tag{10.42}$$
{{< /math >}}

求和是对树模型的 $J-1$ 个内部节点。在每个这样的节点 $t$ 上，会选定一个用来将这个节点所对应的区域分割成两部分的输入变量 $X_{v(t)}$；在每个部分中分别用常数来拟合输出变量的取值。相对于用一个常数来拟合整体区域，基于某个变量分割所带来的平方误差损失改进为 $\hat{\imath}\_t^2$，而在这个节点所选的特定变量就是可得出最大估计改进的变量。变量 $X_\ell$ 的平方相对重要性就是在所有选择它为分割变量的内部节点上的平方改进之和。

这个重要性的度量可很容易地推广至加性树模型的展开中（式 10.28）；也就是在这些树模型上的平均：

{{< math >}}
$$\mathcal{I}_\ell^2 = \frac{1}{M} \sum_{m=1}^{M}
\mathcal{I}_\ell^2(T_m) \tag{10.43}$$
{{< /math >}}

由于取平均带来的稳定效应，这个度量要比其在单个树模型中对应的度量（式 10.42）要可靠得多。同时，由于采用了收缩（[第 10.12.1 节]({{< relref "../ch10/ch10_12.md#10121-收缩" >}})），更不太可能出现因为彼此高度相关而被其他输入变量屏蔽（masking）掉重要的变量的问题。注意到式 10.42 和式 10.43 都是相关程度的平方；实际的相关程度为它们对应的平方根。因为这些度量是相对的，所以通常会会将最大值设置为 100 并将其他值相应地进行缩放。[图 10.6]({{< relref "../ch10/ch10_08.md#figure-f1006" >}})展示了在预测垃圾邮件和正常邮件中的 57 个输入变量的相对重要性。

对于 $K$ 类别的分类问题，需要分别推导出 $K$ 个模型 $f_k(x),k=1,2,\dots,K$，每个模型都是一些树模型的求和：

{{< math >}}
$$f_k(x) = \sum_{m=1}^M T_{km}(x) \tag{10.44}$$
{{< /math >}}

在这个场景中，可将式 10.43 推广为：

{{< math >}}
$$\mathcal{I}_{\ell k}^2 = \frac{1}{M} \sum_{m=1}^M
\mathcal{I}_\ell^2(T_{km}) \tag{10.45}$$
{{< /math >}}

其中的 $\mathcal{I}\_{\ell k}$ 为变量 $X_\ell$ 在区分类别 $k$ 和其他类别时的相关程度。$X_\ell$ 的总体相关程度就是在所有类别上的平均：

{{< math >}}
$$\mathcal{I}_\ell^2 =  \frac{1}{K} \sum_{k=1}^K
\mathcal{I}_{\ell k}^2 \tag{10.46}$$
{{< /math >}}

[图 10.23]({{< relref "../ch10/ch10_14.md#figure-f1023" >}}) 和[图 10.24]({{< relref "../ch10/ch10_14.md#figure-f1024" >}}) 演示了平均或各自的相对重要性的应用。

### 10.13.2 部分依赖图

找出了最相关的变量之后，接下来是要理解近似函数 $f(X)$ 与输入变量联合取值的依赖关系特征。画出函数 $f(X)$ 对其输入参数的曲线图形，可给出函数对输入变量联合取值依赖关系的一个全面的概括。

但遗憾的是这种可视化方法只能限制在低维度的曲线。一个或两个参数的连续或离散函数，都很容易以很多不同方式来展示；本书中有很多这样的图形。对于维度略高的函数，可以条件于除去一或两个参数的一组特定取值，然后考察函数对这一或两个参数的曲线，生成一组格状（trellis）的曲线图（Becker et al., 1996）。[^1]

对于多于两或三个变量，对更高维度参数函数的可视化更加困难。有时一个实用的替代方法是查看一系列曲线图，其中每个图展示近似函数 $f(X)$ 对一小组选定的输入变量的部分依赖（partial dependence）。尽管这样的一系列图不太能给出这个近似函数的一个综合的描述，但通常会给出一些有用的线索，尤其是在 $f(x)$ 被低阶的交互项（式 10.40）所主导时。

考虑输入自变量 $X^T=(X_1,X_2,\dots,X_p)$ 中的 $l<p$ 个变量的子向量 $X_\mathcal{S}$，索引为 $\mathcal{S}\subset\\{1,2,\dots,p\\}$。令 $\mathcal{C}$ 为补集，即 $\mathcal{S}\cup\mathcal{C}=\\{1,2,\dots,p\\}$。一个一般的函数 $f(X)$ 原则上会依赖于其所有的输入变量：$f(X) = f(X_\mathcal{S}, X_\mathcal{C})$。$f(X)$ 对 $X_\mathcal{S}$ 的平均（average）或部分依赖（partial dependence）的一种定义方式为：

{{< math >}}
$$f_\mathcal{S}(X_\mathcal{S}) = \operatorname{E}_{X_\mathcal{C}}
f(X_\mathcal{S}, X_\mathcal{C}) \tag{10.47}$$
{{< /math >}}

这是 $f$ 的一个边际平均，例如在 $X_\mathcal{S}$ 中的变量与 $X_\mathcal{C}$ 中的变量之间没有强交互作用时，它可做为所选变量子集对 $f(X)$ 影响的一个实用的描述。

部分依赖函数可用于解释任意的“黑盒”式学习方法。它们可用下式估计：

{{< math >}}
$$\bar{f}_\mathcal{S}(X_\mathcal{S}) = \frac{1}{N} \sum_{i=1}^N
f(X_\mathcal{S}, x_{i\mathcal{C}}) \tag{10.48}$$
{{< /math >}}

其中的 $\\{x_{1\mathcal{C}},x_{2\mathcal{C}},\dots,x_{N\mathcal{C}}\\}$ 为 $X_\mathcal{C}$ 在训练集上发生的取值。这需要对 $X_\mathcal{S}$ 的每组联合取值都要遍历一遍数据，来获得 $\bar{f}\_\mathcal{S}(X_\mathcal{S})$ 的取值。这即使在中等大小的数据集上也会计算量非常大。幸好的是在决策树模型中，$\bar{f}\_\mathcal{S}(X_\mathcal{S})$（表达式 10.48）可从树模型自身快速地计算出，而不需要涉及到数据（[练习 10.11](#练习-1011)）。

需要强调的是式 10.47 中定义的部分依赖函数代表的是已包含了其他变量 $X_\mathcal{C}$ 的（平均）效果后，$X_\mathcal{S}$ 对 $f(X)$ 的影响。它们并不是在忽略了 $X_\mathcal{C}$ 的影响时 $X_\mathcal{S}$ 对 $f(X)$ 的影响。后者是由条件期望所描述：

{{< math >}}
$$\tilde{f}_\mathcal{S}(X_\mathcal{S}) =
\operatorname{E}(f(X_\mathcal{S}, X_\mathcal{C}) | X_\mathcal{S}) \tag{10.49}$$
{{< /math >}}

而且这是在所有只包含 $X_\mathcal{S}$ 的函数中的对 $f(X)$ 的最佳的最小二乘近似。只有当 $X_\mathcal{S}$ 和 $X_\mathcal{C}$ 彼此独立的时候，数值 $\tilde{f}\_\mathcal{S}(X_\mathcal{S})$ 和 $\bar{f}\_\mathcal{S}(X_\mathcal{S})$ 才会相等，而这是不太可能出现的。例如，如果所选变量子集的影响碰巧为完全加性的：

{{< math >}}
$$f(X) = h_1(X_\mathcal{S}) + h_2(X_\mathcal{C}) \tag{10.50}$$
{{< /math >}}

那么式 10.47 得出的是 $h_1(X_\mathcal{S})$ 加上一个常数。如果影响碰巧为完全乘性（multiplicative）：

{{< math >}}
$$f(X) = h_1(X_\mathcal{S}) \cdot h_2(X_\mathcal{C}) \tag{10.51}$$
{{< /math >}}

那么式 10.47 得出的是 $h_1(X_\mathcal{S})$ 乘以一个常数。另一方面，式 10.49 在任一情况下都不会得出 $h_1(X_\mathcal{S})$。实际上，式 10.49 可能在对 $f(X)$ 完全没有影响的变量子集上得出很强的影响关系。

通过观察提升树模型的近似（表达式 10.28）在选定变量子集上的部分依赖图，可帮助理解这个函数的一些特征。[第 10.8 节]({{< relref "../ch10/ch10_08.md" >}})和[第 10.14 节]({{< relref "../ch10/ch10_14.md" >}})做了一些演示。被计算机绘图和人类感知所限制，子集 $X_\mathcal{S}$ 大小必须有限（$l\approx1,2,3$）。当然会有非常多这样的子集，不过只有从通常会小得多的高相关程度自变量集合中选出的子集才会有参考意义。同时，那些对 $f(X)$ 的影响近似为加性或乘性的变量子集会在图中最明显。

对于 $K$ 类别的分类问题，会对每个类别有一个模型，共用 $K$ 个不同的模型（表达式 10.44）。每个模型与对应的概率（表达式 10.21）的关系是：

{{< math >}}
$$f_k(X) = \log p_k(X) - \frac{1}{K} \sum_{l=1}^K \log p_l(X) \tag{10.52}$$
{{< /math >}}

因此，每个 $f_k(X)$ 是其对应概率在对数尺度上的单调递增函数。每个对应的 $f_k(X)$（式 10.44）的最相关的自变量（式 10.45）的部分依赖图可展现出相应的输入变量是如何影响到特定类别的对数几率的。

----------

### 本节练习

#### 练习 10.3

Show that the marginal average (10.47) recovers additive and
multiplicative functions (10.50) and (10.51), while the conditional expec-
tation (10.49) does not.

#### 练习 10.11

Show how to compute the partial dependence function fS (XS )
in (10.47) efficiently.

#### 练习 10.12

Referring to (10.49), let $\mathcal{S}=\\{1\\}$ and $\mathcal{C}=\\{2\\}$,
with $f(X_1,X_2)=X_1$. Assume $X_1$ and $X_2$ are bivariate Gaussian,
each with mean zero, variance one, and $\operatorname{E}(X_1X_2)=\rho$.
Show that $\operatorname{E}(f(X_1,X_2)|X_2)=\rho X_2$, even
though $f$ is not a function of $X_2$.

[^1]: 原文脚注 1：R 中的 `lattice` 程序包。