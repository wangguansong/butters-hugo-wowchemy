---
title: 10.11 提升方法中树模型的合理大小
summary: >
  第 361-364 页。提升方法中的树模型大小体现了自变量中交互项的阶数。经验来说可选择 4 至 8 之间的取值，或直接令树大小为 6。

date: 2019-01-30T20:15:00+08:00
lastmod: 2022-06-14T15:36:00+08:00
linktitle: 10.11 提升树模型的合理大小
draft: false
math: true

type: book
weight: 1011

authors: ["Butters"]
tags: ["译文"]
categories: ["统计学习基础（译注）"]

---

提升方法一直被视为一个用于结合模型（比如树模型）的方法。如此以来，生成树模型的算法成为了构建提升方法待结合的模型的前提过程。在这样的场景中，可以如同往常一样在构建每个树模型的过程中分别估计其最优的大小（[第 9.2 节]({{< relref "../ch09/ch09_02.md" >}})）。先构建一个非常大（过于大）的树模型，然后自下而上地对其进行剪枝到所估计出的最优终节点个数。这个做法隐含地将每个树模型都假设为展开式 10.28 中的最后一个。可能对除了展开式的最后一个树模型外的其他树模型来说，这明显是不合适的假设。这导致了，尤其在前期循环中，一些树模型过大。这也会严重地影响模型的表现并增加了计算负担。

为避免这个问题，一个最简单的方法是限制所有的树模型都是同样的大小 $J_m = J$，$\forall m$。在每步循环中，都构建 $J$ 个终节点的回归树模型。因此 $J$ 成为了提升方法整体流程中的一个超（meta）参数，可根据在数据上最大化估计的模型表现来进行调优。

为了理解 $J$ 的取值范围，可考察一下“目标”函数的性质：

$$\eta = \underset{f}{\arg\min} \operatorname{E}\_{XY} L(Y, f(X)) \tag{10.39}$$

其中的期望是对 $(X,Y)$ 在样本总体的联合分布计算的。目标函数 $\eta(x)$ 是在未来数据上的预测误差（risk）最小的函数。这是模型所要近似的函数。

$\eta{X}$ 的一个有关的性质是其坐标变量 $X^T=(X_1,X_2,\dots,X_p)$ 相互之间影响的程度。这可通过其方差分析（analysis of variance, ANOVA）展开来衡量：

$$\begin{align}
\eta(X) =& \sum_j \eta_j(X_j) + \sum_{jk} \eta_{jk}(X_j, X_k)\\\\
& + \sum_{jkl} \eta_{jkl}(X_j, X_k, X_l) + \dots
\end{align}\tag{10.40}$$

表达式 10.40 中的第一个求和是对所有只有一个自变量 $X_j$ 的函数。特定的函数 $\eta_j(X_j)$ 是在损失准则下协同地产生对 $\eta(X)$ 最优近似的那些函数。每个这样的 $\eta_j(X_j)$ 被称为 $X_j$ 的“主效应”（main effect）。第二个求和是对在主效应基础上额外添加到展开式后可最佳拟合 $\eta(X)$ 的那些有两个变量的函数。这些被称为每对变量 $(X_j,X_k)$ 的二阶交互。第三个求和代表了三阶交互，以此类推。在实践中遇到的很多问题通常由低阶交互效应所主导。在这样的场景中，一些包含了较多高阶交互效应的模型往往准确度不高，比如大型的决策树模型。

树模型产生的函数近似的交互程度由树模型大小 $J$ 所控制。也就是说，树模型中不包含高于 $J-1$ 阶的交互效应。由于提升模型是树模型的相加（表达式 10.28），这个限制对提升模型也生效。令 $J=2$（一次分割的树桩模型）则得到的是只有主效应的提升模型；令 $J=3$，则也允许有两个变量的交互效应，以此类推。这说明所选的 $J$ 取值应当反应了 $\eta(x)$ 中主导交互项的阶数。当然一般这是未知的，但在大多数场景中这个值应该不大。[图 10.9](#figure-f1009) 用模拟数据的例子（表达式 10.2）演示了交互项阶数（$J$ 的选择）的影响。数据生成函数是加性的（二次项的求和），所以 $J>2$ 的提升模型会引入不必要的方差，因而会有较大的测试误差。[图 10.10](#figure-f1010) 对比了基于树桩模型的提升方法中的坐标函数和真实的函数。

{{< figure
  id="f1009"
  src="https://public.guansong.wang/eslii/ch10/eslii_fig_10_09.png"
  title="**图 10.9**：基于图 10.2 中所使用的例子（表达式 10.2），展示了不同树模型大小的提升方法。由于数据生成模型是加性的，树桩模型的表现最佳。提升算法中使用了算法 10.3 中的二项分布偏差损失；图中也展示了算法 10.1 中的自适应提升（AdaBoost）作为对比。"
>}}

{{< figure
  id="f1010"
  src="https://public.guansong.wang/eslii/ch10/eslii_fig_10_10.png"
  title="**图 10.10**：在图 10.9 的模拟例子的提升树桩模型中估计出的坐标函数。同时展示了真实的二次函数作为比较。"
>}}

尽管在很多应用中 $J=2$ 不满足实际要求，但也不太可能需要 $J>10$。从目前的经验来说，在提升方法中 $4\leq J\leq 8$ 的效果良好，模型结果对 $J$ 在这个区间内的特定取值相对的不敏感。当然也可对 $J$ 的取值进行微调，在几个不同的值中选择在验证样本中损失最低的一个。然而，这样也很少会在 $J\simeq 6$ 的基础上有显著的改进。