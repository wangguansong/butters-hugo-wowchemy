---
title: 14.9  非线性降维和局部多维尺度分析
summary: >
  第 572-576 页。

date: 2022-07-29T17:00:00+08:00
linktitle: 14.9  非线性降维和局部 MDS

weight: 1409

---

Several methods have been recently proposed for nonlinear dimension re-
duction, similar in spirit to principal surfaces. The idea is that the data lie
close to an intrinsically low-dimensional nonlinear manifold embedded in a
high-dimensional space. These methods can be thought of as “flattening”
the manifold, and hence reducing the data to a set of low-dimensional co-
ordinates that represent their relative positions in the manifold. They are
useful for problems where signal-to-noise ratio is very high (e.g., physical
systems), and are probably not as useful for observational data with lower
signal-to-noise ratios.

{{< figure
  id="f1444"
  src="https://public.guansong.wang/eslii/ch14/eslii_fig_14_44.png"
  title="**图 14.44**："
>}}

The basic goal is illustrated in the left panel of Figure 14.44. The data
lie near a parabola with substantial curvature. Classical MDS does not pre-
serve the ordering of the points along the curve, because it judges points
on opposite ends of the curve to be close together. The right panel shows
the results of local multi-dimensional scaling, one of the three methods for
non-linear multi-dimensional scaling that we discuss below. These meth-
ods use only the coordinates of the points in p dimensions, and have no
other information about the manifold. Local MDS has done a good job of
preserving the ordering of the points along the curve.

We now briefly describe three new approaches to nonlinear dimension
reduction and manifold mapping.

Isometric feature mapping (ISOMAP) (Tenenbaum et al., 2000) con-
structs a graph to approximate the geodesic distance between points along
the manifold. Specifically, for each data point we find its neighbors—points
within some small Euclidean distance of that point. We construct a graph
with an edge between any two neighboring points. The geodesic distance
between any two points is then approximated by the shortest path be-
tween points on the graph. Finally, classical scaling is applied to the graph
distances, to produce a low-dimensional mapping.

Local linear embedding (Roweis and Saul, 2000) takes a very different ap-
proach, trying to preserve the local affine structure of the high-dimensional
data. Each data point is approximated by a linear combination of neigh-
boring points. Then a lower dimensional representation is constructed that
best preserves these local approximations. The details are interesting, so
we give them here.

1. For each data point x i in p dimensions, we find its K-nearest neigh-
bors N (i) in Euclidean distance.
2. We approximate each point by an affine mixture of the points in its
neighborhood:
$$\min_{W_{ik}} \\|x_i - \sum_{k\in\mathcal{N}(i)} w_{ik} x_k\\|^2\tag{14.102}$$
over weights w ik satisfying w ik = 0, k ∈/ N (i), k=1 w ik = 1. w ik
is the contribution of point k to the reconstruction of point i. Note
that for a hope of a unique solution, we must have K < p.
3. Finally, we find points y i in a space of dimension d < p to minimize
$$\sum_{i=1}^N \\|y_i - \sum_{k=1}^N w_{ik} y_k\\|^2 \tag{14.103}$$
with w ik fixed.

In step 3, we minimize

$$\operatorname{tr}[
  (\mathbf{Y}-\mathbf{W}\mathbf{Y})^T(\mathbf{Y}-\mathbf{W}\mathbf{Y})]
= \operatorname{tr}[
  \mathbf{Y}^T(\mathbf{I}-\mathbf{W})^T(\mathbf{I}-\mathbf{W})\mathbf{Y}
]$$
$$\tag{14.104}$$

where W is N × N ; Y is N × d, for some small d < p. The solutions Ŷ
are the trailing eigenvectors of M = (I − W) T (I − W). Since 1 is a trivial
eigenvector with eigenvalue 0, we discard it and keep the next d. This has
the side effect that 1 T Y = 0, and hence the embedding coordinates are
mean centered.

Local MDS (Chen and Buja, 2008) takes the simplest and arguably the
most direct approach. We define N to be the symmetric set of nearby pairs
of points; specifically a pair (i, i ′ ) is in N if point i is among the K-near

$$\begin{align}
S_L(z_1,z_2,\dots,z_N) &=
  \sum_{(i,i')\notin \mathcal{N}} (d_{ii'} - \\|z_i-z_{i'}\\|)^2 \\\\ &+
  \sum_{(i,i')\notin \mathcal{N}} w \cdot (D - \\|z_i-z_{i'}\\|)^2
\tag{14.105}\end{align}$$

Here D is some large constant and w is a weight. The idea is that points
that are not neighbors are considered to be very far apart; such pairs are
given a small weight w so that they don’t dominate the overall stress func-
tion. To simplify the expression, we take w ∼ 1/D, and let D → ∞.
Expanding (14.105), this gives

$$\begin{align}
S_L(z_1,z_2,\dots,z_N) &=
  \sum_{(i,i')\notin \mathcal{N}} (d_{ii'} - \\|z_i-z_{i'}\\|)^2 \\\\ &+
  \tau \sum_{(i,i')\notin \mathcal{N}} \\|z_i-z_{i'}\\|
\tag{14.106}\end{align}$$

where τ = 2wD. The first term in (14.106) tries to preserve local structure
in the data, while the second term encourages the representations z i , z i ′
for pairs (i, i ′ ) that are non-neighbors to be farther apart. Local MDS
minimizes the stress function (14.106) over z i , for fixed values of the number
of neighbors K and the tuning parameter τ .

The right panel of Figure 14.44 shows the result of local MDS, using k = 2
neighbors and τ = 0.01. We used coordinate descent with multiple starting
values to find a good minimum of the (nonconvex) stress function (14.106).
The ordering of the points along the curve has been largely preserved,

{{< figure
  id="f1445"
  src="https://public.guansong.wang/eslii/ch14/eslii_fig_14_45.png"
  title="**图 14.45**："
>}}

Figure 14.45 shows a more interesting application of one of these meth-
ods (LLE) 15 . The data consist of 1965 photographs, digitized as 20 × 28
grayscale images. The result of the first two-coordinates of LLE are shown
and reveal some variability in pose and expression. Similar pictures were
produced by local MDS.

In experiments reported in Chen and Buja (2008), local MDS shows su-
perior performance, as compared to ISOMAP and LLE. They also demon-
strate the usefulness of local MDS for graph layout. There are also close
connections between the methods discussed here, spectral clustering (Sec-
tion 14.5.3) and kernel PCA (Section 14.5.4).