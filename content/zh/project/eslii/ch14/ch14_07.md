---
title: 14.7 独立成分分析和探索投影寻踪
summary: >
  第 557-570 页。

date: 2022-04-01T11:00:00+08:00
draft: true 
math: true

type: book
weight: 1407
linktitle: 14.7 ICA 和 EPP

authors: ["Butters"]
tags: ["译文"]
categories: ["统计学习基础（译注）"]

---

多元数据通常可被看成是对一个隐含数据来源的多个间接的测量，而这个数据来源一般无法被直接测量。以下为一些例子：

- 教育性和心理学的测试使用对问卷的回答来测量被试主体隐含的智力水平和其他心理能力。
- 脑电图扫描通过放置在头部不同位置的传感器记录到的电磁信号来间接地测量大脑不同部位的神经活动。
- 股票的交易价格不停地波动，反映了许多无法测量因子的影响，例如市场信息、外部影响、以及其他难以识别或测量的驱动因素。


Factor analysis is a classical technique developed in the statistical liter-
ature that aims to identify these latent sources. Factor analysis models
are typically wed to Gaussian distributions, which has to some extent hin-
dered their usefulness. More recently, independent component analysis has
emerged as a strong competitor to factor analysis, and as we will see, relies
on the non-Gaussian nature of the underlying sources for its success.

### 14.7.1 Latent Variables and Factor Analysis

奇异值分解 $\mathbf{X}=\mathbf{U}\mathbf{D}\mathbf{V}^T$（式 14.54）

The singular-value decomposition √ X = UDV(14.54) has√ a latent variable
representation. Writing S = N U and A = DV T / N , we have X =
SA T , and hence each of the columns of X is a linear combination of the
columns of S. Now since U is orthogonal, and assuming as before that the
columns of X (and hence U) each have mean zero, this implies that the
columns of S have zero mean, are uncorrelated and have unit variance. In
terms of random variables, we can interpret the SVD, or the corresponding
principal component analysis (PCA) as an estimate of a latent variable
model

$$\begin{matrix}
X_1 &=& a_{11}S_1 + a_{12}S_2 + \cdots + a_{1p}S_p \\\\
X_2 &=& a_{21}S_1 + a_{22}S_2 + \cdots + a_{2p}S_p \\\\
\vdots & & \vdots \\\\
X_p &=& a_{p1}S_1 + a_{p2}S_2 + \cdots + a_{pp}S_p
\end{matrix}\tag{14.78}$$

or simply X = AS. The correlated X j are each represented as a linear
expansion in the uncorrelated, unit variance variables S ℓ . This is not too
satisfactory, though, because given any orthogonal p × p matrix R, we can
write

$$\begin{align} X
&= \mathbf{A} S \\\\
&= \mathbf{A}\mathbf{R}^T\mathbf{R}S \\\\
&= \mathbf{A}^\* S^\*
\tag{14.79}\end{align}$$

and Cov(S ∗ ) = R Cov(S) R T = I. Hence there are many such decom-
positions, and it is therefore impossible to identify any particular latent
variables as unique underlying sources. The SVD decomposition does have
the property that any rank q < p truncated decomposition approximates
X in an optimal way.

The classical factor analysis model, developed primarily by researchers in
psychometrics, alleviates these problems to some extent; see, for example,
Mardia et al. (1979). With q < p, a factor analysis model has the form

$$\begin{matrix}
X_1 &=& a_{11}S_1 + a_{12}S_2 + \cdots + a_{1q}S_q + \varepsilon_1 \\\\
X_2 &=& a_{21}S_1 + a_{22}S_2 + \cdots + a_{2q}S_q + \varepsilon_2 \\\\
\vdots & & \vdots \\\\
X_p &=& a_{p1}S_1 + a_{p2}S_2 + \cdots + a_{pq}S_q + \varepsilon_p
\end{matrix}\tag{14.80}$$

or X = AS + ε. Here S is a vector of q < p underlying latent variables or
factors, A is a p × q matrix of factor loadings, and the ε j are uncorrelated
zero-mean disturbances. The idea is that the latent variables S ℓ are com-
mon sources of variation amongst the X j , and account for their correlation
structure, while the uncorrelated ε j are unique to each X j and pick up the
remaining unaccounted variation. Typically the S ℓ and the ε j are modeled
as Gaussian random variables, and the model is fit by maximum likelihood.
The parameters all reside in the covariance matrix

$$\mathbf{\Sigma} = \mathbf{A}\mathbf{A}^T + \mathbf{D}\_\varepsilon \tag{14.81}$$

where D ε = diag[Var(ε 1 ), . . . , Var(ε p )]. The S ℓ being Gaussian and un-
correlated makes them statistically independent random variables. Thus a
battery of educational test scores would be thought to be driven by the
independent underlying factors such as intelligence, drive and so on. The
columns of A are referred to as the factor loadings, and are used to name
and interpret the factors.

Unfortunately the identifiability issue (14.79) remains, since A and AR T
are equivalent in (14.81) for any q × q orthogonal R. This leaves a certain
subjectivity in the use of factor analysis, since the user can search for ro-
tated versions of the factors that are more easily interpretable. This aspect
has left many analysts skeptical of factor analysis, and may account for its
lack of popularity in contemporary statistics. Although we will not go into
details here, the SVD plays a key role in the estimation of (14.81). For ex-
ample, if the Var(ε j ) are all assumed to be equal, the leading q components
of the SVD identify the subspace determined by A.

Because of the separate disturbances ε j for each X j , factor analysis can
be seen to be modeling the correlation structure of the X j rather than the
covariance structure. This can be easily seen by standardizing the covari-
ance structure in (14.81) (Exercise 14.14). This is an important distinction
between factor analysis and PCA, although not central to the discussion
here. Exercise 14.15 discusses a simple example where the solutions from
factor analysis and PCA differ dramatically because of this distinction.

### 14.7.2 独立成分分析

**独立成分分析（independent component analysis，ICA）** 模型

The independent component analysis (ICA) model has exactly the same
form as (14.78), except the S ℓ are assumed to be statistically indepen-
dent rather than uncorrelated. Intuitively, lack of correlation determines
the second-degree cross-moments (covariances) of a multivariate distribu-
tion, while in general statistical independence determines all of the cross-
moments. These extra moment conditions allow us to identify the elements
of A uniquely. Since the multivariate Gaussian distribution is determined
by its second moments alone, it is the exception, and any Gaussian inde-
pendent components can be determined only up to a rotation, as before.
Hence identifiability problems in (14.78) and (14.80) can be avoided if we
assume that the S ℓ are independent and non-Gaussian.

Here we will discuss the full p-component model as in (14.78), where the
S ℓ are independent with unit variance; ICA versions of the factor analysis
model (14.80) exist as well. Our treatment is based on the survey article
by Hyvärinen and Oja (2000).

We wish to recover the mixing matrix A in X = AS. Without loss
of generality, we can assume that X has already been whitened to have
Cov(X) = I; this is typically achieved via the SVD described above. This
in turn implies that A is orthogonal, since S also has covariance I. So
solving the ICA problem amounts to finding an orthogonal A such that
the components of the vector random variable S = A T X are independent
(and non-Gaussian).

{{< figure
  id="f1437"
  src="https://public.guansong.wang/eslii/ch14/eslii_fig_14_37.png"
  title="**图 14.37**："
>}}

Figure 14.37 shows the power of ICA in separating two mixed signals.
This is an example of the classical cocktail party problem, where differ-
ent microphones X j pick up mixtures of different independent sources S ℓ
(music, speech from different speakers, etc.). ICA is able to perform blind
source separation, by exploiting the independence and non-Gaussianity of
the original sources.

Many of the popular approaches to ICA are based on entropy. The dif-
ferential entropy H of a random variable Y with density g(y) is given by

$$H(Y) = -\int g(y) \log g(y) dy \tag{14.82}$$

A well-known result in information theory says that among all random
variables with equal variance, Gaussian variables have the maximum en-
tropy. Finally, the mutual information I(Y ) between the components of the
random vector Y is a natural measure of dependence:

$$I(Y) = \sum_{j=1}^p H(Y_j) - H(Y) \tag{14.83}$$

The quantity I(Y ) is called the Kullback–Leibler Q p distance between the
density g(y) of Y and its independence version j=1 g j (y j ), where g j (y j )
is the marginal density of Y j . Now if X has covariance I, and Y = A T X
with A orthogonal, then it is easy to show that

$$\begin{align} I(Y)
&= \sum_{j=1}^p H(Y_j) - H(X) - \log |\det\mathbf{A}| \tag{14.84}\\\\
&= \sum_{j=1}^p H(Y_j) - H(X) \tag{14.85}
\end{align}$$

Finding an A to minimize I(Y ) = I(A T X) looks for the orthogonal trans-
formation that leads to the most independence between its components. In
light of (14.84) this is equivalent to minimizing the sum of the entropies of
the separate components of Y , which in turn amounts to maximizing their
departures from Gaussianity.

For convenience, rather than using the entropy H(Y j ), Hyvärinen and
Oja (2000) use the negentropy measure J(Y j ) defined by

$$J(Y_j) = H(Z_j) - H(Y_j) \tag{14.86}$$

where Z j is a Gaussian random variable with the same variance as Y j . Ne-
gentropy is non-negative, and measures the departure of Y j from Gaussian-
ity. They propose simple approximations to negentropy which can be com-
puted and optimized on data. The ICA solutions shown in Figures 14.37–
14.39 use the approximation

$$J(Y_j) \approx [\operatorname{E}G(Y_j) - \operatorname{E}G(Z_j)]^2 \tag{14.87}$$

where G(u) = a 1 log cosh(au) for 1 ≤ a ≤ 2. When applied to a sample
of x i , the expectations are replaced by data averages. This is one of the
options in the FastICA software provided by these authors. More classical
(and less robust) measures are based on fourth moments, and hence look for
departures from the Gaussian via kurtosis. See Hyvärinen and Oja (2000)
for more details. In Section 14.7.4 we describe their approximate Newton
algorithm for finding the optimal directions.

{{< figure
  id="f1438"
  src="https://public.guansong.wang/eslii/ch14/eslii_fig_14_38.png"
  title="**图 14.38**："
>}}

{{< figure
  id="f1439"
  src="https://public.guansong.wang/eslii/ch14/eslii_fig_14_39.png"
  title="**图 14.39**："
>}}

In summary then, ICA applied to multivariate data looks for a sequence
of orthogonal projections such that the projected data look as far from
Gaussian as possible. With pre-whitened data, this amounts to looking for
components that are as independent as possible.

ICA starts from essentially a factor analysis solution, and looks for rota-
tions that lead to independent components. From this point of view, ICA is
just another factor rotation method, along with the traditional “varimax”
and “quartimax” methods used in psychometrics.

#### Example: Handwritten Digits

We revisit the handwritten threes analyzed by PCA in Section 14.5.1. Fig-
ure 14.39 compares the first five (standardized) principal components with
the first five ICA components, all shown in the same standardized units.
Note that each plot is a two-dimensional projection from a 256-dimensional
space. While the PCA components all appear to have joint Gaussian distri-
butions, the ICA components have long-tailed distributions. This is not too
surprising, since PCA focuses on variance, while ICA specifically looks for
non-Gaussian distributions. All the components have been standardized,
so we do not see the decreasing variances of the principal components.

For each ICA component we have highlighted two of the extreme digits,
as well as a pair of central digits and displayed them in Figure 14.40.
This illustrates the nature of each of the components. For example, ICA
component five picks up the long sweeping tailed threes.

{{< figure
  id="f1440"
  src="https://public.guansong.wang/eslii/ch14/eslii_fig_14_40.png"
  title="**图 14.40**："
>}}

#### Example: EEG Time Courses

ICA has become an important tool in the study of brain dynamics—the
example we present here uses ICA to untangle the components of signals
in multi-channel electroencephalographic (EEG) data (Onton and Makeig,
2006).

Subjects wear a cap embedded with a lattice of 100 EEG electrodes,
which record brain activity at different locations on the scalp. Figure 14.41 11
(top panel) shows 15 seconds of output from a subset of nine of these elec-
trodes from a subject performing a standard “two-back” learning task over
a 30 minute period. The subject is presented with a letter (B, H, J, C, F, or
K) at roughly 1500-ms intervals, and responds by pressing one of two but-
tons to indicate whether the letter presented is the same or different from
that presented two steps back. Depending on the answer, the subject earns
or loses points, and occasionally earns bonus or loses penalty points. The
time-course data show spatial correlation in the EEG signals—the signals
of nearby sensors look very similar.

The key assumption here is that signals recorded at each scalp electrode
are a mixture of independent potentials arising from different cortical ac-
tivities, as well as non-cortical artifact domains; see the reference for a
detailed overview of ICA in this domain.

{{< figure
  id="f1441"
  src="https://public.guansong.wang/eslii/ch14/eslii_fig_14_41.png"
  title="**图 14.41**："
>}}

The lower part of Figure 14.41 shows a selection of ICA components.
The colored images represent the estimated unmixing coefficient vectors â j
as heatmap images superimposed on the scalp, indicating the location of
activity. The corresponding time-courses show the activity of the learned
ICA components.

For example, the subject blinked after each performance feedback signal
(colored vertical lines), which accounts for the location and artifact signal
in IC1 and IC3. IC12 is an artifact associated with the cardiac pulse. IC4
and IC7 account for frontal theta-band activities, and appear after a stretch
of correct performance. See Onton and Makeig (2006) for a more detailed
discussion of this example, and the use of ICA in EEG modeling.

### 14.7.3 探索投影寻踪

Friedman and Tukey (1974) proposed exploratory projection pursuit, a
graphical exploration technique for visualizing high-dimensional data. Their
view was that most low (one- or two-dimensional) projections of high-
dimensional data look Gaussian. Interesting structure, such as clusters or
long tails, would be revealed by non-Gaussian projections. They proposed
a number of projection indices for optimization, each focusing on a differ-
ent departure from Gaussianity. Since their initial proposal, a variety of
improvements have been suggested (Huber, 1985; Friedman, 1987), and a
variety of indices, including entropy, are implemented in the interactive
graphics package Xgobi (Swayne et al., 1991, now called GGobi). These
projection indices are exactly of the same form as J(Y j ) above, where
Y j = a Tj X, a normalized linear combination of the components of X. In
fact, some of the approximations and substitutions for cross-entropy coin-
cide with indices proposed for projection pursuit. Typically with projection
pursuit, the directions a j are not constrained to be orthogonal. Friedman
(1987) transforms the data to look Gaussian in the chosen projection, and
then searches for subsequent directions. Despite their different origins, ICA
and exploratory projection pursuit are quite similar, at least in the repre-
sentation described here.

### 14.7.4 A Direct Approach to ICA :scream:

Independent components have by definition a joint product density

$$f_S(s) = \prod_{j=1}^p f_j(x_j) \tag{14.88}$$

so here we present an approach that estimates this density directly us-
ing generalized additive models (Section 9.1). Full details can be found in
Hastie and Tibshirani (2003), and the method is implemented in the R
package ProDenICA, available from CRAN.

In the spirit of representing departures from Gaussianity, we represent
each f j as

$$f_j(s_j) = \phi(s_j)e^{g_j(s_j)} \tag{14.89}$$

a tilted Gaussian density. Here φ is the standard Gaussian density, and
g j satisfies the normalization conditions required of a density. Assuming
as before that X is pre-whitened, the log-likelihood for the observed data
X = AS is


$$\ell(\mathbf{A}, \\{g_j\\}\_1^p; \mathbf{X}) =
\sum_{i=1}^N\sum_{j=1}^p [\log\phi_j(a_j^T x_i) + g_j(a_j^T x_i)] \tag{14.90}$$

which we wish to maximize subject to the constraints that A is orthogonal
and that the g j result in densities in (14.89). Without imposing any further
restrictions on g j , the model (14.90) is over-parametrized, so we instead
maximize a regularized version

$$\sum_{j=1}^p \left[
\frac{1}{N}\sum_{i=1}^N [\log\phi(a_j^T x_j) + g_j(a_j^T x_i)] -
\int\phi(t)e^{g_j(t)} dt -
\lambda_j \int \\{g'''_j(t)\\}^2 (t) dt
\right]$$
$$\tag{14.91}$$

We have subtracted two penalty terms (for each j) in (14.91), inspired by
Silverman (1986, Section 5.4.4):
• The first enforces the density constraint φ(t)e ĝ j (t) dt = 1 on any
solution ĝ j .
• The second is a roughness penalty, which guarantees that the solution
ĝ j is a quartic-spline with knots at the observed values of s ij = a Tj x i .

It can further be shown that the solution densities f ˆ j = φe ĝ j each have
mean zero and variance one (Exercise 14.18). As we increase λ j , these
solutions approach the standard Gaussian φ.

We fit the functions g j and directions a j by optimizing (14.91) in an
alternating fashion, as described in Algorithm 14.3.

----------
#### Algorithm 14.3 Product Density ICA Algorithm: ProDenICA

1. Initialize A (random Gaussian matrix followed by orthogonalization).
2. Alternate until convergence of A:
(a) Given A, optimize (14.91) w.r.t. g j (separately for each j).
(b) Given g j , j = 1, . . . , p, perform one step of a fixed point algo-
rithm towards finding the optimal A.
----------

Step 2(a) amounts to a semi-parametric density estimation, which can
be solved using a novel application of generalized additive models. For
convenience we extract one of the p separate problems,

$$\frac{1}{N}\sum_{i=1}^N [\log\phi(s_i) + g_j(s_i)] -
\int\phi(t)e^{g_j(t)} dt -
\lambda_j \int \\{g'''_j(t)\\}^2 (t) dt$$
$$\tag{14.92}$$

Although the second integral in (14.92) leads to a smoothing spline, the
first integral is problematic, and requires an approximation. We construct
a fine grid of L values s ∗ ℓ in increments ∆ covering the observed values s i ,
and count the number of s i in the resulting bins:

$$y_\ell^\* = \frac{\\# s_i \in (s_\ell^\*-\Delta/2, s_\ell^\*+\Delta/2)}{N}\tag{14.93}$$

Typically we pick L to be 1000, which is more than adequate. We can then
approximate (14.92) by

$$\sum_{\ell=1}^L \left\\{ y_i^\* [\log(\phi(s_\ell^\*)) + g(s_\ell^\*)] - \Delta\phi(s_\ell^\*) e^{g(s_\ell^\*)} \right\\} -
\lambda\int g'''^2(s) ds$$
$$\tag{14.94}$$

This last expression can be seen to be proportional to a penalized Poisson
log-likelihood with response y ℓ ∗ /∆ and penalty parameter λ/∆, and mean
µ(s) = φ(s)e g(s) . This is a generalized additive spline model (Hastie and
Tibshirani, 1990; Efron and Tibshirani, 1996), with an offset term log φ(s),
and can be fit using a Newton algorithm in O(L) operations. Although
a quartic spline is called for, we find in practice that a cubic spline is
adequate. We have p tuning parameters λ j to set; in practice we make
them all the same, and specify the amount of smoothing via the effective
degrees-of-freedom df(λ). Our software uses 5df as a default value.

Step 2(b) in Algorithm 14.3 requires optimizing (14.91) with respect to
A, holding the ĝ j fixed. Only the first terms in the sum involve A, and
since A is orthogonal, the collection of terms involving φ do not depend on
A (Exercise 14.19). Hence we need to maximize

$$\begin{align} C(\mathbf{A})
&= \frac{1}{N} \sum_{j=1}^p\sum_{i=1}^N \hat{g}\_j(a_j^T x_i) \tag{14.95}\\\\
&= \sum_{j=1}^p C_j(a_j)
\end{align}$$

C(A) is a log-likelihood ratio between the fitted density and a Gaussian,
and can be seen as an estimate of negentropy (14.86), with each ĝ j a con-
trast function as in (14.87). The fixed point update in step 2(b) is a modified
Newton step (Exercise 14.20)

1. For each j update
$$a_j \leftarrow \operatorname{E}\\{\\} - \operatorname{E}[\hat{g}''\_j]\tag{14.96}$$
where E represents expectation w.r.t the sample x i . Since ĝ j is a fitted
quartic (or cubic) spline, the first and second derivatives are readily
available.
2. Orthogonalize A using the symmetric square-root transformation
(AA T ) − 2 A. If A = UDV T is the SVD of A, it is easy to show that
this leads to the update A ← UV T .


#### Example: Simulations

{{< figure
  id="f1442"
  src="https://public.guansong.wang/eslii/ch14/eslii_fig_14_42.png"
  title="**图 14.42**："
>}}

Figure 14.42 shows the results of a simulation comparing ProDenICA to
FastICA, and another semi-parametric competitor KernelICA (Bach and
Jordan, 2002). The left panel shows the 18 distributions used as a basis
of comparison. For each distribution, we generated a pair of independent
components (N = 1024), and a random mixing matrix in IR 2 with condition
number between 1 and 2. We used our R implementations of FastICA, using
the negentropy criterion (14.87), and ProDenICA. For KernelICA we used
the authors MATLAB code. 12 Since the search criteria are nonconvex, we
used five random starts for each method. Each of the algorithms delivers
an orthogonal mixing matrix A (the data were pre-whitened), which is
available for comparison with the generating orthogonalized mixing matrix
A 0 . We used the Amari metric (Bach and Jordan, 2002) as a measure of
the closeness of the two frames:

$$\begin{align} d(\mathbf{A}\_0,\mathbf{A})
&= \frac{1}{2p}\sum_{i=1}^p \left( \frac{\sum_{j=1}^p |r_{ij}|}{\max_j |r_{ij}|} - 1 \right) \\\\ &+
\frac{1}{2p}\sum_{j=1}^p \left( \frac{\sum_{i=1}^p |r_{ij}|}{\max_i |r_{ij}|} - 1 \right)
\end{align}\tag{14.97}$$

where r ij = (A o A −1 ) ij . The right panel in Figure 14.42 compares the
averages (on the log scale) of the Amari metric between the truth and the
estimated mixing matrices. ProDenICA is competitive with FastICA and
KernelICA in all situations, and dominates most of the mixture simulations.

----------
### 本节练习

#### 练习 14.14

#### 练习 14.15

#### 练习 14.18

#### 练习 14.19

#### 练习 14.20