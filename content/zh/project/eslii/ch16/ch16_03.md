---
title: 16.3 Learning Ensembles
summary: >
  第 616-623 页。

date: 2022-04-01T11:00:00+08:00
draft: true 
math: true

type: book
weight: 1603

authors: ["Butters"]
tags: ["译文"]
categories: ["统计学习基础（译注）"]

---

The insights learned from the previous sections can be harnessed to produce
a more effective and efficient ensemble model. Again we consider functions
of the form

$$f(x)=\alpha_0+\sum_{T_k\in\mathcal{T}} \alpha_k T_k(x)\tag{16.8}$$

where T is a dictionary of basis functions, typically trees. For gradient
boosting and random forests, |T | is very large, and it is quite typical for the
final model to involve many thousands of trees. In the previous section we
argue that gradient boosting with shrinkage fits an L 1 regularized monotone
path in this space of trees.

Friedman and Popescu (2003) propose a hybrid approach which breaks
this process down into two stages:
• A finite dictionary T L = {T 1 (x), T 2 (x), . . . , T M (x)} of basis functions
is induced from the training data;
• A family of functions f λ (x) is built by fitting a lasso path in this
dictionary:

$$\begin{align}
&\alpha(\lambda)=\\\\
&\arg\min_\alpha \sum_{i=1}^N L[y_i,\alpha_0+\sum_{m=1}^M\alpha_m T_m(x_i)] +
\lambda\sum_{m=1}^M|\alpha_m|
\end{align}\tag{16.9}$$

In its simplest form this model could be seen as a way of post-processing
boosting or random forests, taking for T L the collection of trees produced
by the gradient boosting or random forest algorithms. By fitting the lasso
path to these trees, we would typically use a much reduced set, which would
save in computations and storage for future predictions. In the next section
we describe modifications of this prescription that reduce the correlations in
the ensemble T L , and improve the performance of the lasso post processor.

As an initial illustration, we apply this procedure to a random forest
ensemble grown on the spam data.

Figure 16.6 shows that a lasso post-processing offers modest improve-
ment over the random forest (blue curve), and reduces the forest to about
40 trees, rather than the original 1000. The post-processed performance
matches that of gradient boosting. The orange curves represent a modified
version of random forests, designed to reduce the correlations between trees
even more. Here a random sub-sample (without replacement) of 5% of the
training sample is used to grow each tree, and the trees are restricted to be
shallow (about six terminal nodes). The post-processing offers more dra-
matic improvements here, and the training costs are reduced by a factor
of about 100. However, the performance of the post-processed model falls
somewhat short of the blue curves.

{{< figure
  src="https://public.guansong.wang/eslii/ch16/eslii_fig_16_06.png"
  id="f1606"
  title="**图 16.6**："
>}}
Application of the lasso post-processing (16.9) to the spam data.
The horizontal blue line is the test error of a random forest fit to the spam data,
using 1000 trees grown to maximum depth (with m = 7; see Algorithm 15.1).
The jagged blue curve is the test error after post-processing the first 500 trees
using the lasso, as a function of the number of trees with nonzero coefficients.
The orange curve/line use a modified form of random forest, where a random
draw of 5% of the data are used to grow each tree, and the trees are forced to
be shallow (typically six terminal nodes). Here the post-processing offers much
greater improvement over the random forest that generated the ensemble.

### 16.3.1 Learning a Good Ensemble

Not all ensembles T L will perform well with post-processing. In terms of
basis functions, we want a collection that covers the space well in places
where they are needed, and are sufficiently different from each other for
the post-processor to be effective.

Friedman and Popescu (2003) gain insights from numerical quadrature
and importance sampling. They view the unknown function as an integral

$$f(x)=\int \beta(\gamma)b(x;\gamma)d\gamma\tag{16.10}$$

where γ ∈ Γ indexes the basis functions b(x; γ). For example, if the basis
functions are trees, then γ indexes the splitting variables, the split-points
and the values in the terminal nodes. Numerical quadrature amounts to
finding a set of M evaluation P M points γ m ∈ Γ and corresponding weights
α m so that f M (x) = α 0 + m=1 α m b(x; γ m ) approximates f (x) well over
the domain of x. Importance sampling amounts to sampling γ at random,
but giving more weight to relevant regions of the space Γ. Friedman and
Popescu (2003) suggest a measure of (lack of) relevance that uses the loss
function (16.9):

$$Q(\gamma)=\min_{c_0,c_1}\sum_{i=1}^N
L(y_i, c_0+c_1 b(x_i;\gamma))\tag{16.11}$$

evaluated on the training data.

If a single basis function were to be selected (e.g., a tree), it would be
the global minimizer γ ∗ = arg min γ∈Γ Q(γ). Introducing randomness in the
selection of γ would necessarily produce less optimal values with Q(γ) ≥
Q(γ ∗ ). They propose a natural measure of the characteristic width σ of the
sampling scheme S,

$$\sigma = \operatorname{E}_\mathcal{S}[Q(\gamma)-Q(\gamma^*))]\tag{16.12}$$

• σ too narrow suggests too many of the b(x; γ m ) look alike, and similar
to b(x; γ ∗ );
• σ too wide implies a large spread in the b(x; γ m ), but possibly con-
sisting of many irrelevant cases.

Friedman and Popescu (2003) use sub-sampling as a mechanism for intro-
ducing randomness, leading to their ensemble-generation algorithm 16.2.

----------
#### 算法 16.2 ISLE Ensemble Generation.
1. $f_0(x) = \operatorname{arg}\min_c \sum_{i=1}^N L(y_i;c)$
2. For m = 1 to M do
   1. $\gamma_m = \operatorname{arg}\min_\gamma \sum_{i\in S_m(\eta)}L(y_i,f_{m-1}(x_i)+b(x_i;\gamma))$
   2. $f_m(x) = f_{m-1}(x) + \nu b(x;\gamma_m)$
3. $\mathcal{T}_{ISLE} = \\{b(x;\gamma_1),b(x;\gamma_2),\dots,b(x;\gamma_M)\\}$
----------

S m (η) refers to a subsample of N · η (η ∈ (0, 1]) of the training obser-
vations, typically without replacement. Their simulations suggest picking
η ≤ 2 1 , and for large N picking η ∼ 1/ N . Reducing η increases the
randomness, and hence the width σ. The parameter ν ∈ [0, 1] introduces
memory into the randomization process; the larger ν, the more the pro-
cedure avoids b(x; γ) similar to those found before. A number of familiar
randomization schemes are special cases of Algorithm 16.2:

Bagging has η = 1, but samples with replacement, and has ν = 0. Fried-
man and Hall (2007) argue that sampling without replacement with
η = 1/2 is equivalent to sampling with replacement with η = 1, and
the former is much more efficient.

Random forest sampling is similar, with more randomness introduced by
the selection of the splitting variable. Reducing η < 1/2 in algo-
rithm 16.2 has a similar effect to reducing m in random forests, but
does not suffer from the potential biases discussed in Section 15.4.2.

Gradient boosting with shrinkage (10.41) uses η = 1, but typically does
not produce sufficient width σ.

Stochastic gradient boosting (Friedman, 1999) follows the recipe exactly.

The authors recommend values ν = 0.1 and η ≤ 2 1 , and call their combined
procedure (ensemble generation and post processing) Importance sampled
learning ensemble (ISLE).

Figure 16.7 shows the performance of an ISLE on the spam data. It does
not improve the predictive performance, but is able to produce a more
parsimonious model. Note that in practice the post-processing includes
the selection of the regularization parameter λ in (16.9), which would be
chosen by cross-validation. Here we simply demonstrate the effects of post-
processing by showing the entire path on the test data.

{{< figure
  src="https://public.guansong.wang/eslii/ch16/eslii_fig_16_07.png"
  id="f1607"
  title="**图 16.7**："
>}}
Importance sampling learning ensemble (ISLE) fit to the spam
data. Here we used η = 1/2, ν = 0.05, and trees with five terminal nodes. The
lasso post-processed ensemble does not improve the prediction error in this case,
but it reduces the number of trees by a factor of five.

{{< figure
  src="https://public.guansong.wang/eslii/ch16/eslii_fig_16_08.png"
  id="f1608"
  title="**图 16.8**："
>}}
Demonstration of ensemble methods on a regression simulation
example. The notation GBM (0.1, 0.01) refers to a gradient boosted model, with
parameters (η, ν). We report mean-squared error from the true (known) function.
Note that the sub-sampled GBM model (green) outperforms the full GBM model
(orange). The lasso post-processed version achieves similar error. The random
forest is outperformed by its post-processed version, but both fall short of the
other models.

Figure 16.8 shows various ISLEs on a regression example. The generating
function is

$$f(X)=10\cdot\prod_{j=1}^5e^{-2X_j^2}+\sum_{j=6}^{35}X_j\tag{16.13}$$

where X ∼ U [0, 1] 100 (the last 65 elements are noise variables). The re-
sponse Y = f (X) + ε where ε ∼ N (0, σ 2 ); we chose σ = 1.3 resulting in a
signal-to-noise ratio of approximately 2. We used a training sample of size
1000, and estimated the mean squared error E( f ˆ (X)−f (X)) 2 by averaging
over a test set of 500 samples. The sub-sampled GBM curve (light blue)
is an instance of stochastic gradient boosting (Friedman, 1999) discussed in
Section 10.12, and it outperforms gradient boosting on this example.

### 16.3.2 Rule Ensembles

Here we describe a modification of the tree-ensemble method that focuses
on individual rules (Friedman and Popescu, 2003). We encountered rules
in Section 9.3 in the discussion of the PRIM method. The idea is to enlarge
an ensemble of trees by constructing a set of rules from each of the trees
in the collection.

{{< figure
  src="https://public.guansong.wang/eslii/ch16/eslii_fig_16_09.png"
  id="f1609"
  title="**图 16.9**："
>}}
A typical tree in an ensemble, from which rules can be derived.

Figure 16.9 depicts a small tree, with numbered nodes. The following
rules can be derived from this tree:

$$\begin{align}
R_1(x) &= I(X_1 < 2.1) \\\\
R_2(x) &= I(X_1 \geq 2.1) \\\\
R_3(x) &= I(X_1 \geq 2.1) \cdot I(X_3 \in \\{S\\}) \\\\
R_4(x) &= I(X_1 \geq 2.1) \cdot I(X_3 \in \\{M,L\\}) \\\\
R_5(x) &= I(X_1 \geq 2.1) \cdot I(X_3 \in \\{S\\}) \cdot I(X_7 < 4.5) \\\\
R_6(x) &= I(X_1 \geq 2.1) \cdot I(X_3 \in \\{S\\}) \cdot I(X_7 \geq 4.5)
\end{align}\tag{16.14}$$

A linear expansion in rules 1, 4, 5 and 6 is equivalent to the tree itself
(Exercise 16.3); hence (16.14) is an over-complete basis for the tree.
For each tree T m in an ensemble T , we can construct its mini-ensemble
m, and then combine them all to form a larger ensemble

$$\mathcal{T}\_\text{RULE} = \bigcup_{m=1}^M \mathcal{T}\_\text{RULE}^m \tag{16.15}$$

This is then treated like any other ensemble, and post-processed via the
lasso or similar regularized procedure.

There are several advantages to this approach of deriving rules from the
more complex trees:
• The space of models is enlarged, and can lead to improved perfor-
mance.
Rules are easier to interpret than trees, so there is the potential for
a simplified model.
• It is often natural to augment T RULE by including each variable X j
separately as well, thus allowing the ensemble to model linear func-
tions well.

Friedman and Popescu (2008) demonstrate the power of this procedure on a
number of illustrative examples, including the simulation example (16.13).
Figure 16.10 shows boxplots of the mean-squared error from the true model
for twenty realizations from this model. The models were all fit using the
Rulefit software, available on the ESL homepage 3 , which runs in an auto-
matic mode.

{{< figure
  src="https://public.guansong.wang/eslii/ch16/eslii_fig_16_10.png"
  id="f1610"
  title="**图 16.10**："
>}}
Mean squared error for rule ensembles, using 20 realizations
of the simulation example (16.13).

On the same training set as used in Figure 16.8, the rule based model
achieved a mean-squared error of 1.06. Although slightly worse than the
best achieved in that figure, the results are not comparable because cross-
validation was used here to select the final model.