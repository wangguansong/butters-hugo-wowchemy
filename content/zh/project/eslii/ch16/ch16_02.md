---
title: 16.2 Boosting and Regularization Paths
summary: >
  第 607-616 页。

date: 2022-04-01T11:00:00+08:00
draft: true 

weight: 1602

---

In Section 10.12.2 of the first edition of this book, we suggested an analogy
between the sequence of models produced by a gradient boosting algorithm
and regularized model fitting in high-dimensional feature spaces. This was
primarily motivated by observing the close connection between a boosted
version of linear regression and the lasso (Section 3.4.2). These connec-
tions have been pursued by us and others, and here we present our current
thinking in this area. We start with the original motivation, which fits more
naturally in this chapter on ensemble learning.

### 16.2.1 Penalized Regression

Intuition for the success of the shrinkage strategy (10.41) of gradient boost-
ing (page 364 in Chapter 10) can be obtained by drawing analogies with
penalized linear regression with a large basis expansion. Consider the dic-
tionary of all possible J-terminal node regression trees T = {T k } that could
be realized on the training data as basis functions in IR p . The linear model
is

$$f(x)=\sum_{k=1}^K \alpha_k T_k(x)\tag{16.1}$$

where K = card(T ). Suppose the coefficients are to be estimated by least
squares. Since the number of such trees is likely to be much larger than
even the largest training data sets, some form of regularization is required.
Let α̂(λ) solve

$$\min_\alpha \left\\{
\sum_{i=1}^N \left(y_i - \sum_{k=1}^K \alpha_k T_k(x_i)\right)^2 + \lambda \cdot J(\alpha)
\right\\}\tag{16.2}$$

J(α) is a function of the coefficients that generally penalizes larger values.
Examples are

$$\begin{align}
&J(\alpha) = \sum_{k=1}^K |\alpha_k|^2 & \text{ridge regression} \tag{16.3} \\\\
&J(\alpha) = \sum_{k=1}^K |\alpha_k| & \text{lasso} \tag{16.4} \\\\
\end{align}$$
$$\tag{16.5}$$

both covered in Section 3.4. As discussed there, the solution to the lasso
problem with moderate to large λ tends to be sparse; many of the α̂ k (λ) =
0. That is, only a small fraction of all possible trees enter the model (16.1).

This seems reasonable since it is likely that only a small fraction of all pos-
sible trees will be relevant in approximating any particular target function.
However, the relevant subset will be different for different targets. Those
coefficients that are not set to zero are shrunk by the lasso in that their
absolute values are smaller than their corresponding least squares values 2 :
| α̂ k (λ) | < | α̂ k (0) |. As λ increases, the coefficients all shrink, each one
ultimately becoming zero.

Owing to the very large number of basis functions T k , directly solving
(16.2) with the lasso penalty (16.4) is not possible. However, a feasible
forward stagewise strategy exists that closely approximates the effect of
the lasso, and is very similar to boosting and the forward stagewise Algo-
rithm 10.2. Algorithm 16.1 gives the details. Although phrased in terms
of tree basis functions T k , the algorithm can be used with any set of ba-
sis functions. Initially all coefficients are zero in line 1; this corresponds
to λ = ∞ in (16.2). At each successive step, the tree T k ∗ is selected that
best fits the current residuals in line 2(a). Its corresponding coefficient α̌ k ∗
is then incremented or decremented by an infinitesimal amount in 2(b),
while all other coefficients α̌ k , k 6 = k ∗ are left unchanged. In principle, this
process could be iterated until either all the residuals are zero, or β ∗ = 0.
The latter case can occur if K < N , and at that point the coefficient values
represent a least squares solution. This corresponds to λ = 0 in (16.2).

----------
#### 算法 16.1：Forward Stagewise Linear Regression.
1. Initialize α̌ k = 0, k = 1, . . . , K. Set ε > 0 to some small constant, and M large.
2. For m = 1 to M :
   1. (β ∗ , k ∗ ) = arg min β,kP N i=1
   2. α̌ k ∗ ← α̌ k ∗ + ε · sign(β ∗ ).
3. Output f M (x) = k=1 α̌ k T k (x).
----------

After applying Algorithm 16.1 with M < ∞ iterations, many of the coef-
ficients will be zero, namely, those that have yet to be incremented. The oth-
ers will tend to have absolute values smaller than their corresponding least
squares solution values, | α̌ k (M ) | < | α̂ k (0) |. Therefore this M -iteration
solution qualitatively resembles the lasso, with M inversely related to λ.

Figure 16.1 shows an example, using the prostate data studied in Chap-
ter 3. Here, instead of using trees T k (X) as basis functions, we use the origi-
nal variables X k themselves; that is, a multiple linear regression model. The
left panel displays the profiles of estimated P coefficients from the lasso, for
different values of the bound parameter t = k |α k |. The right panel shows
the results of the stagewise Algorithm 16.1, with M = 250 and ε = 0.01.
[The left and right panels of Figure 16.1 are the same as Figure 3.10 and
the left panel of Figure 3.19, respectively.] The similarity between the two
graphs is striking.

{{< figure
  src="https://public.guansong.wang/eslii/ch16/eslii_fig_16_01.png"
  id="f1601"
  title="**图 16.1**："
>}}
Profiles of estimated coefficients from linear regression, for the
prostate data studied in Chapter 3. The left panel shows the results from the lasso,
for different values of the bound parameter t = k |α k |. The right panel shows
the results of the stagewise linear regression Algorithm 16.1, using M = 220
consecutive steps of size ε = .01.

In some situations the resemblance is more than qualitative. For example,
if all of the basis functions T k are mutually uncorrelated, then as ε ↓ 0, M ↑
such that M ǫ → t, Algorithm 16.1 P yields exactly the same solution as the
lasso for bound parameter t = k |α k | (and likewise for all solutions along
the path). Of course, tree-based regressors are not uncorrelated. However,
the solution sets are also identical if the coefficients α̂ k (λ) are all monotone
functions of λ. This is often the case when the correlation between the
variables is low. When the α̂ k (λ) are not monotone in λ, then the solution
sets are not identical. The solution sets for Algorithm 16.1 tend to change
less rapidly with changing values of the regularization parameter than those
of the lasso.

Efron et al. (2004) make the connections more precise, by characterizing
the exact solution paths in the ε-limiting case. They show that the coeffi-
cient paths are piece-wise linear functions, both for the lasso and forward
stagewise. This facilitates efficient algorithms which allow the entire paths
to be computed with the same cost as a single least-squares fit. This least
angle regression algorithm is described in more detail in Section 3.8.1.

Hastie et al. (2007) show that this infinitesimal forward stagewise algo-
rithm (FS 0 ) fits a monotone version of the lasso, which optimally reduces
at each step the loss function for a given increase in the arc length of the
coefficient path (see Sections 16.2.3 and 3.8.1). The arc-length for the ǫ > 0
case is M ǫ, and hence proportional to the number of steps.

Tree boosting (Algorithm 10.3) with shrinkage (10.41) closely resembles
Algorithm 16.1, with the learning rate parameter ν corresponding to ε. For
squared error loss, the only difference is that the optimal tree to be selected
at each iteration T k∗ is approximated by the standard top-down greedy
tree-induction algorithm. For other loss functions, such as the exponential
loss of AdaBoost and the binomial deviance, Rosset et al. (2004a) show
similar results to what we see here. Thus, one can view tree boosting with
shrinkage as a form of monotone ill-posed regression on all possible (J-
terminal node) trees, with the lasso penalty (16.4) as a regularizer. We
return to this topic in Section 16.2.3.

The choice of no shrinkage [ν = 1 in equation (10.41)] is analogous to
forward-stepwise regression, and its more aggressive cousin best-subset
selection, which penalizes the number of non zero coefficients J(α) = k |α k | 0 .
With a small fraction of dominant variables, best subset approaches often
work well. But with a moderate fraction of strong variables, it is well known
that subset selection can be excessively greedy (Copas, 1983), often yielding
poor results when compared to less aggressive strategies such as the lasso
or ridge regression. The dramatic improvements often seen when shrinkage
is used with boosting are yet another confirmation of this approach.

### 16.2.2 The “Bet on Sparsity” Principle

As shown in the previous section, boosting’s forward stagewise strategy
with shrinkage approximately minimizes the same loss function with a
lasso-style L 1 penalty. The model is built up slowly, searching through
“model space” and adding shrunken basis functions derived from impor-
tant predictors. In contrast, the L 2 penalty is computationally much easier
to deal with, as shown in Section 12.3.7. With the basis functions and L 2
penalty chosen to match a particular positive-definite kernel, one can solve
the corresponding optimization problem without explicitly searching over
individual basis functions.

However, the sometimes superior performance of boosting over proce-
dures such as the support vector machine may be largely due to the im-
plicit use of the L 1 versus L 2 penalty. The shrinkage resulting from the
L 1 penalty is better suited to sparse situations, where there are few basis
functions with nonzero coefficients (among all possible choices).

We can strengthen this argument through a simple example, taken from
Friedman et al. (2004). Suppose we have 10, 000 data points and our model
is a linear combination of a million trees. If the true population coefficients
of these trees arose from a Gaussian distribution, then we know that in a
Bayesian sense the best predictor is ridge regression (Exercise 3.6). That is,
we should use an L 2 rather than an L 1 penalty when fitting the coefficients.
On the other hand, if there are only a small number (e.g., 1000) coefficients
that are nonzero, the lasso (L 1 penalty) will work better. We think of this
as a sparse scenario, while the first case (Gaussian coefficients) is dense.
Note however that in the dense scenario, although the L 2 penalty is best,
neither method does very well since there is too little data from which to
estimate such a large number of nonzero coefficients. This is the curse of
dimensionality taking its toll. In a sparse setting, we can potentially do
well with the L 1 penalty, since the number of nonzero coefficients is small.
The L 2 penalty fails again.

In other words, use of the L 1 penalty follows what we call the “bet on
sparsity” principle for high-dimensional problems:

> Use a procedure that does well in sparse problems, since no pro-
> cedure does well in dense problems.

These comments need some qualification:
• For any given application, the degree of sparseness/denseness depends
on the unknown true target function, and the chosen dictionary T .
• The notion of sparse versus dense is relative to the size of the train-
ing data set and/or the noise-to-signal ratio (NSR). Larger training
sets allow us to estimate coefficients with smaller standard errors.
Likewise in situations with small NSR, we can identify more nonzero
coefficients with a given sample size than in situations where the NSR
is larger.
• The size of the dictionary plays a role as well. Increasing the size of the
dictionary may lead to a sparser representation for our function, but
the search problem becomes more difficult leading to higher variance.

Figure 16.2 illustrates these points in the context of linear models us-
ing simulation. We compare ridge regression and lasso, both for classifi-
cation and regression problems. Each run has 50 observations with 300
independent Gaussian predictors. In the top row all 300 coefficients are
nonzero, generated from a Gaussian distribution. In the middle row, only
10 are nonzero and generated from a Gaussian, and the last row has 30
non zero Gaussian coefficients. For regression, standard Gaussian noise is
added to the linear predictor η(X) = X T β to produce a continuous re-
sponse. For classification the linear predictor is transformed via the inverse-
logit to a probability, and a binary response is generated. Five differ-
ent noise-to-signal ratios are presented, obtained by scaling η(X) prior
to generating the response. In both cases this is defined to be NSR =
Var(Y |η(X))/Var(η(X)). Both the ridge regression and lasso coefficient
paths were fit using a series of 50 values of λ corresponding to a range of
df from 1 to 50 (see Chapter 3 for details). The models were evaluated on
a large test set (infinite for Gaussian, 5000 for binary), and in each case the
value for λ was chosen to minimize the test-set error. We report percentage
variance explained for the regression problems, and percentage misclassifi-
cation error explained for the classification problems (relative to a baseline
error of 0.5). There are 20 simulation runs for each scenario.

{{< figure
  src="https://public.guansong.wang/eslii/ch16/eslii_fig_16_02.png"
  id="f1602"
  title="**图 16.2**："
>}}
Simulations that show the superiority of the L 1 (lasso) penalty
over L 2 (ridge) in regression and classification. Each run has 50 observations
with 300 independent Gaussian predictors. In the top row all 300 coefficients are
nonzero, generated from a Gaussian distribution. In the middle row, only 10 are
nonzero, and the last row has 30 nonzero. Gaussian errors are added to the linear
predictor η(X) for the regression problems, and binary responses generated via the
inverse-logit transform for the classification problems. Scaling of η(X) resulted in
the noise-to-signal ratios shown. Lasso is used in the left sub-columns, ridge in the
right. We report the optimal percentage of error explained on test data (relative
to the error of a constant model), displayed as boxplots over 20 realizations for
each combination. In the only situation where ridge beats lasso (top row), neither
do well.

Note that for the classification problems, we are using squared-error loss
to fit the binary response. Note also that we do not using the training
data to select λ, but rather are reporting the best possible behavior for
each method in the different scenarios. The L 2 penalty performs poorly
everywhere. The Lasso performs reasonably well in the only two situations
where it can (sparse coefficients). As expected the performance gets worse
as the NSR increases (less so for classification), and as the model becomes
denser. The differences are less marked for classification than for regression.

These empirical results are supported by a large body of theoretical
results (Donoho and Johnstone, 1994; Donoho and Elad, 2003; Donoho,
2006b; Candes and Tao, 2007) that support the superiority of L 1 estimation
in sparse settings.

### 16.2.3 Regularization Paths, Over-fitting and Margins 😱

It has often been observed that boosting “does not overfit,” or more as-
tutely is “slow to overfit.” Part of the explanation for this phenomenon was
made earlier for random forests — misclassification error is less sensitive to
variance than is mean-squared error, and classification is the major focus
in the boosting community. In this section we show that the regulariza-
tion paths of boosted models are “well behaved,” and that for certain loss
functions they have an appealing limiting form.

Figure 16.3 shows the coefficient paths for lasso and infinitesimal forward
stagewise (FS 0 ) in a simulated regression setting. The data consists of a
dictionary of 1000 Gaussian variables, strongly correlated (ρ = 0.95) within
blocks of 20, but uncorrelated between blocks. The generating model has
nonzero coefficients for 50 variables, one drawn from each block, and the
coefficient values are drawn from a standard Gaussian. Finally, Gaussian
noise is added, with a noise-to-signal ratio of 0.72 (Exercise 16.1.) The
FS 0 algorithm is a limiting form of algorithm 16.1, where the step size ε
is shrunk to zero (Section 3.8.1). The grouping of the variables is intended
to mimic the correlations of nearby trees, and with the forward-stagewise
algorithm, this setup is intended as an idealized version of gradient boosting
with shrinkage. For both these algorithms, the coefficient paths can be
computed exactly, since they are piecewise linear (see the LARS algorithm
in Section 3.8.1).

{{< figure
  src="https://public.guansong.wang/eslii/ch16/eslii_fig_16_03.png"
  id="f1603"
  title="**图 16.3**："
>}}
Comparison of lasso and infinitesimal forward stagewise paths
on simulated regression data. The number of samples is 60 and the number of
variables is 1000. The forward-stagewise paths fluctuate less than those of lasso
in the final stages of the algorithms.

Here the coefficient profiles are similar only in the early stages of the
paths. For the later stages, the forward stagewise paths tend to be mono-
tone and smoother, while those for the lasso fluctuate widely. This is due
to the strong correlations among subsets of the variables —lasso suffers
somewhat from the multi-collinearity problem (Exercise 3.28).

The performance of the two models is rather similar (Figure 16.4), and
they achieve about the same minimum. In the later stages forward stagewise
takes longer to overfit, a likely consequence of the smoother paths.

{{< figure
  src="https://public.guansong.wang/eslii/ch16/eslii_fig_16_04.png"
  id="f1604"
  title="**图 16.4**："
>}}
Mean squared error for lasso and infinitesimal forward stagewise
on the simulated data. Despite the difference in the coefficient paths, the two
models perform similarly over the critical part of the regularization path. In the
right tail, lasso appears to overfit more rapidly.

Hastie et al. (2007) show that FS 0 solves a monotone version of the lasso
problem for squared error loss. Let T a = T ∪ {−T } be the augmented
dictionary obtained by including a P negative copy of every basis element
in T . We consider models f (x) = T k ∈T a α k T k (x) with non-negative co-
efficients α k ≥ 0. In this expanded space, the lasso coefficient paths are
positive, while those of FS 0 are monotone nondecreasing.

The monotone lasso path is characterized by a differential equation

$$\frac{\partial\alpha}{\partial\ell} =
\rho^{ml}(\alpha(\ell))\tag{16.6}$$

with initial condition α(0) = 0, where ℓ is the L 1 arc-length of the path
α(ℓ) (Exercise 16.2). The monotone lasso move direction (velocity vector)
ρ ml (α(ℓ)) decreases the loss at the optimal quadratic rate per unit increase
in the L 1 arc-length of the path. Since ρ ml k (α(ℓ)) ≥ 0 ∀k, ℓ, the solution
paths are monotone.

The lasso can similarly be characterized as the solution to a differential
equation as in (16.6), except that the move directions decrease the loss
optimally per unit increase in the L 1 norm of the path. As a consequence,
they are not necessarily positive, and hence the lasso paths need not be
monotone.

In this augmented dictionary, restricting the coefficients to be positive is
natural, since it avoids an obvious ambiguity. It also ties in more naturally
with tree boosting—we always find trees positively correlated with the
current residual.

There have been suggestions that boosting performs well (for two-class
classification) because it exhibits maximal-margin properties, much like the
support-vector machines of Chapters 4.5.2 and 12. Schapire et al. (1998)
define the normalized L 1 margin of a fitted model f (x) = k α k T k (x) as

$$m(f) = \min_i \frac{y_i f(x_i)}{\sum_{k=1}^K|\alpha_k|}\tag{16.7}$$

Here the minimum is taken over the training sample, and y i ∈ {−1, +1}.
Unlike the L 2 margin (4.40) of support vector machines, the L 1 margin
m(f ) measures the distance to the closest training point in L ∞ units (max-
imum coordinate distance).

Schapire et al. (1998) prove that with separable data, Adaboost in-
creases m(f ) with each iteration, converging to a margin-symmetric so-
lution. Rätsch and Warmuth (2002) prove the asymptotic convergence of
Adaboost with shrinkage to a L 1 -margin-maximizing solution. Rosset et
al. (2004a) consider regularized models of the form (16.2) for general loss
functions. They show that as λ ↓ 0, for particular loss functions the solution
converges to a margin-maximizing configuration. In particular they show
this to be the case for the exponential loss of Adaboost, as well as binomial
deviance.

Collecting together the results of this section, we reach the following
summary for boosted classifiers:

> The sequence of boosted classifiers form an L 1 -regularized mono-
> tone path to a margin-maximizing solution.

Of course the margin-maximizing end of the path can be a very poor, overfit
solution, as it is in the example in Figure 16.5. Early stopping amounts
to picking a point along the path, and should be done with the aid of a
validation dataset.

{{< figure
  src="https://public.guansong.wang/eslii/ch16/eslii_fig_16_05.png"
  id="f1605"
  title="**图 16.5**："
>}}
The left panel shows the L 1 margin m(f ) for the Adaboost clas-
sifier on the mixture data, as a function of the number of 4-node trees. The model
was fit using the R package gbm, with a shrinkage factor of 0.02. After 10, 000
trees, m(f ) has settled down. Note that when the margin crosses zero, the training
error becomes zero. The right panel shows the test error, which is minimized at
240 trees. In this case, Adaboost overfits dramatically if run to convergence.

[^1]: 原文脚注 2：If K > N , there is in general no unique “least squares value,” since infinitely many
solutions will exist that fit the data perfectly. We can pick the minimum L 1 -norm solution
amongst these, which is the unique lasso solution.