---
title: 3.5 衍生输入变量的方法
summary: >
  第 79-82 页。利用原输入变量（以及输出变量），衍生出一组维度低但信息含量大的“方向”，使用衍生出的变量进行回归。本节的两个方法主成分回归和部分最小二乘，其原理和表现均与岭回归相似。

date: 2018-09-23T23:02:07+08:00
lastmod: 2018-09-23T23:02:07+08:00
draft: false
math: true

type: book
weight: 305
linktitle: 3.5 衍生输入变量

authors: ["Butters"]
tags: ["译文"]
categories: ["统计学习基础（译注）"]

---

很多应用场景会包含大量通常相关性很大的输入变量。本节介绍的方法利用原始的输入变量 $X_j$ 的线性组合，生成少量的变量 $Z_m,m=1,\dots,M$，再用这些 $Z\_m$ 变量进行回归。线性组合的计算方式不同，相应的方法也不同。

### 3.5.1 主成分回归

一种方法是可以用[第 3.4.1 节]({{< relref "../ch03/ch03_04.md" >}})中的主成分来构成线性组合 $Z_m$。

主成分回归先通过线性转换 $\mathbf{z}\_m=\mathbf{X}v_m$ 生成衍生输入变量向量，然后用 $\mathbf{z}\_1$、$\mathbf{z}\_2$、……、$\mathbf{z}\_M$ 对 $\mathbf{y}$ 进行回归，
其中 $M\leq q$。主成分 $\mathbf{z}\_m$ 互相正交，所以这个回归可看成是很多单变量回归的求和：

$$\hat{\mathbf{y}}^\text{pcr}\_{(M)} =
\bar{y}\mathbf{1} + \sum_{m=1}^M \hat{\theta}\_m \mathbf{z}\_m \tag{3.61}$$

其中 $\hat{\theta}\_m=\langle\mathbf{z}\_m,\mathbf{y}\rangle/\langle\mathbf{z}\_m,\mathbf{z}\_m\rangle$。由于 $\mathbf{z}\_m $ 为原输入变量 $\mathbf{x}\_j$ 的线性组合，拟合解 3.61 也写为原输入变量 $\mathbf{x}\_j$ 的线性组合，相应的系数为（[练习 3.13](#练习-313)）：

$$\hat{\beta}^\text{pcr}(M) = \sum_{m=1}^M \hat{\theta}\_m v_m \tag{3.62}$$

类似于岭回归，主成分的结果依赖于输入变量的度量，所以通常会先将其标准化。注意当 $M=p$ 时，主成分矩阵 $\mathbf{Z}=\mathbf{U}\mathbf{D}$ 的列空间等同于输入变量 $\mathbf{X}$ 的列空间，因此主成分回归等同于使用原输入变量的最小二乘估计回归。当 $M<p$ 时，回归建立在被缩小（reduced）的输入变量空间上。主成分回归与岭回归很相似，两者都建立在输入变量矩阵的主成分上。岭回归对主成分向量上的系数进行收缩，对应的特征值越小收缩的程度越大；而主成分回归直接将其中对应特征向量最小的 $p-M$ 个成分排除掉。[图 3.17](#figure-f0317) 演示了两者的比较。

{{< figure
  id="f0317"
  src="https://public.guansong.wang/eslii/ch03/eslii_fig_03_17.png"
  title="**图 3.17**：岭回归对主成分的回归系数缩减，缩减因子为等式 3.47 中的 $d_j^2/(d_j^2+\lambda)$。主成分回归将特征变量小的成分删除。图中曲线为对应于图 3.7 中的系数的缩减和截取，横轴为主成分的从大到小的索引。"
>}}

从[图 3.7](#figure-f0307) 中可见，经过交叉验证选中的主成分个数为 7；最终的主成分回归在表 3.3 中的几个模型中，测试误差最低。

{{< figure
  src="https://public.guansong.wang/eslii/ch03/eslii_fig_03_07.png"
  title="**图 3.7**：各种子集选择和收缩方法的估计预测误差的曲线和标准误差。每个曲线函数的输入参数为该方法的相应复杂度参数。从横坐标左边到右边对应着模型的复杂度由低到高。预测误差的估计值和标准误差来自于 10 次交叉验证，在第 7.10 节会更深入介绍。每个模型中，最终选择的为在曲线最低点一个标准差范围内最低的复杂度，在每个图中为紫色的虚线交叉点。"
>}}

| Term       | LS     | Best Subset | Ridge  |Lasso | PCR    | PLS    |
|:-----------|--------|-------------|--------|------|--------|--------|
| Intercept  | 2.465  | 2.477       | 2.452  |2.468 | 2.497  | 2.452  |
| lcavol     | 0.680  | 0.740       | 0.420  |0.533 | 0.543  | 0.419  |
| lweight    | 0.263  | 0.316       | 0.238  |0.169 | 0.289  | 0.344  |
| age        | −0.141 |             | −0.046 |      | −0.152 | −0.026 |
| lbph       | 0.210  |             | 0.162  |0.002 | 0.214  | 0.220  |
| svi        | 0.305  |             | 0.227  |0.094 | 0.315  | 0.243  |
| lcp        | −0.288 |             | 0.000  |      | −0.051 | 0.079  |
| gleason    | −0.021 |             | 0.040  |      | 0.232  | 0.011  |
| pgg45      | 0.267  |             | 0.133  |      | −0.056 | 0.084  |
|____________|________|_____________|________|______|________|________|
| Test Error | 0.521  | 0.492       | 0.492  |0.479 | 0.449  | 0.528  |
| Std Error  | 0.179  | 0.143       | 0.165  |0.164 | 0.105  | 0.152  |

**表 3.3**：各种子集选择和收缩方法在前列腺癌症数据集上的估计系数和测试集误差结果。空白项意味这该变量被模型排除。

### 3.5.2 部分最小二乘

**部分最小二乘（partial least squares）** 同样是在回归中使用一组输入变量的线性组合构建的变量，与主成分回归不同的是，在构建的过程中也用到了输出变量 $\mathbf{y}$。部分最小二乘与主成分回归一样，其结果依赖于输入变量的度量，所以也同样假设已将 $\mathbf{x}\_j$ 标准化为均值 0 方差 1。首先对每个 $j$ 计算 $\hat{\varphi}\_{1j}=\langle\mathbf{x}\_j,\mathbf{y}\rangle$。再据此构建衍生输入变量 $\mathbf{z}\_1=\sum_j\varphi_{1j}\mathbf{x}\_j$即为部分最小二乘的第一个衍生方向。可见在构建 $\mathbf{z}\_m$ 时，根据独自对输出变量 $\mathbf{y}$ 的影响程度而对输入变量进行加权[^1]。从输出变量 $\mathbf{y}$ 对 $z_1$ 的回归中得到系数 $\hat{\theta}\_1$，然后将 $\mathbf{x}\_1$、……、$\mathbf{x}\_p$ 对 $\mathbf{z}\_1$ 正则化。重复这个过程，一直到计算出了 $M\leq q$ 个衍生方向。如此便衍生出了一序列正交的输入变量或方向 $\mathbf{z}\_1$、$\mathbf{z}\_2$、……、$\mathbf{z}\_M$。如同主成分回归，如果构架了所有的 $M=p$ 个方向，则得到的解即为使用原输入变量的最小二乘回归；当方向的个数 $M<p$，得到的是建立在缩小输入变量空间上的回归。[算法 3.3](#算法-33-部分最小二乘) 中详细地描述了计算过程。

----------
#### 算法 3.3 部分最小二乘
1. 将所有输入变量 $\mathbf{x}\_j$ 标准化为均值 0 方差 1。初始化 $\hat{\mathbf{y}}^{(0)}=\bar{y}\mathbf{1}$，并且 $x_j^{(0)}=x_j,j=1,\dots,p$。
2. 对所有的 $m=1,2,\dots,p$，循环过程：
   1. $\mathbf{z}\_m=\sum_{j=1}^p\hat{\varphi}\_{mj}\mathbf{x}\_j^{(m-1)}$，其中 $\hat{\varphi}\_{mj}=\langle\mathbf{x}\_j^{(m-1)},\mathbf{y}\rangle$。
   2. $\hat{\theta}\_m=\langle\mathbf{z}\_m,\mathbf{y}\rangle/\langle\mathbf{z}\_m,\mathbf{z}\_m\rangle$
   3. $\hat{\mathbf{y}}^{(m)}=\hat{\mathbf{y}}^{(m-1)}+\hat{\theta}\_m\mathbf{z}\_m$。
   4. 将 $\mathbf{x}\_j^{(m-1)}$ 对 $\mathbf{z}\_m$ 正则化：
     $\mathbf{x}\_j^{(m)}=\mathbf{x}\_j^{(m-1)}-[\langle\mathbf{z}\_m,\mathbf{x}\_j^{(m-1)} \rangle/\langle\mathbf{z}\_m,\mathbf{z}\_m\rangle]\mathbf{z}\_m,j=1,2,\dots,p$。
3. 输出拟合向量序列 $\\{\hat{\mathbf{y}}^{(m)}\\}^p\_1$。衍生方向 $\\{\mathbf{z}\_l\\}^m\_1$ 为原输入变量 $\mathbf{x}\_j$ 的线性组合，故拟合值也可写为原输入变量的线性函数 $\hat{\mathbf{y}}^{(m)}=\mathbf{X}\hat{\beta}^\text{pls}(m)$。其中的线性系数可从部分最小二乘的变换中推导出。
----------

在前列腺癌症的例子中，如[图 3.7](#figure-f0307) 所示，交叉验证确定的最终部分最小二乘方向个数为 $M=2$。对应的模型为表 3.3 的最右一列。

部分最小二乘内在的最优问题是什么呢？由于在构建衍生方向时用到了输出变量 $\mathbf{y}$，它的解是 $\mathbf{y}$ 的非线性函数。可证明（[练习 3.15](#练习-315)），部分最小二乘在寻找方差最大并且与输出变量的相关性最高的方向，相比主成分回归只考虑方差的大小（Stone and Brooks，1990；Frank and Friedman，1993）。

具体来说，第 m 个主成分方向 $v_m$ 为下式的解：

$$\begin{gather}
\max_\alpha \operatorname{Var}(\mathbf{X}\alpha) \\\\
\text{subject to } \\|\alpha\\| = 1, \alpha^T \mathbf{S} v_\ell = 0, \ell = 1,\dots, m-1
\end{gather}\tag{3.63}$$

其中 $\mathbf{S}$ 为输入变量的样本协方差矩阵。条件 $\alpha^T\mathbf{S}v\_l=0$ 保证了 $\mathbf{z}\_m=\mathbf{X}\alpha$ 与所有之前的线性组合 $\mathbf{z}\_l=\mathbf{X}v_l$ 不相关。

而第 m 个部分最小回归的方向 $\hat{\varphi}\_m$ 为下式的解：

$$\begin{gather}
\max_\alpha \operatorname{Corr}^2(y, \mathbf{X}\alpha)\operatorname{Var}(\mathbf{X}\alpha) \\\\
\text{subject to } \\|\alpha\\| = 1, \alpha^T \mathbf{S} \hat{\varphi}\_\ell = 0, \ell = 1,\dots, m-1
\end{gather}\tag{3.64}$$

更深入分析发现其中的方差项会占主导地位，因此部分最小二乘回归与岭回归和主成分回归的表现相似。下一节中会更深入对比不同的方法。

当输入变量矩阵 $\mathbf{X}$ 为正交矩阵时，在 $m=1$ 步之后部分最小二乘即得到了普通的最小二乘的结果。由于对于任意 $m>1$，$\hat\varphi\_{mj}$ 均为 0（[练习 3.14](#练习-314)），后续的计算步骤不再有影响。另也可证明（[练习 3.18](#练习-318)）部分最小二乘的系数序列 $m=1,2,\dots,p$ 也是计算最小二乘解过程中的共轭梯度（conjugate gradient）序列。

----------
### 本节练习

#### 练习 3.13

Derive the expression (3.62), and show that
$\hat{\beta}^{\text{pcr}}(p)=\hat{\beta}^{\text{ls}}$

#### 练习 3.14

Show that in the orthogonal case, PLS stops after m = 1 steps,
because subsequent ϕ̂ mj in step 2 in Algorithm 3.3 are zero.

#### 练习 3.15

Verify expression (3.64), and hence show that the partial least
squares directions are a compromise between the ordinary regression
coefficient and the principal component directions.

#### 练习 3.17

Repeat the analysis of Table 3.3 on the spam data discussed in
Chapter 1.

#### 练习 3.18

Read about conjugate gradient algorithms (Murray et al., 1981, for
example), and establish a connection between these algorithms and partial
least squares.

#### 练习 3.18

Show that $\\|\hat{\beta}^\text{ridge}\\|$ increases as its tuning parameter
$\lambda\rightarrow 0$. Does
the same property hold for the lasso and partial least squares estimates?
For the latter, consider the “tuning parameter” to be the successive steps
in the algorithm.

[^1]: 原文脚注3：输入变量 $\mathbf{x}\_j$ 已标准化，故第一个衍生方向的系数 $\hat{\varphi}\_{1j}$ 即为单变量回归的系数（或包含一个常数）；对后续的衍生方向并不如此。
