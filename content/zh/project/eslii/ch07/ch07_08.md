---
title: 7.8 最小描述长度
summary: >
  第 235-237 页。从信息编码领域中的最小描述长度也可推导出与 BIC 一致的准则。

date: 2018-12-04T15:42:00+08:00
lastmod: 2022-06-13T15:33:00+08:00
draft: false
math: true

type: book
weight: 708

authors: ["Butters"]
tags: ["译文"]
categories: ["统计学习基础（译注）"]

---

**最小描述长度（minimum description length，MDL）** 方法提供了一个与 BIC 方法形式上相同的模型选择准则，但它是从最优编码的角度推导出来。本节先简单综述数据压缩编码的理论，再将其应用于模型选择中。

将数据 $z$ 看成是是一条待编码并发送给别人（接收者）的信息。将模型看成是数据编码的方法，在传输中会选择最简洁的模型，即产生最短的编码。

首先假设需要传输的备选信息为 $z_1,z_2,\dots,z_m$。编码是长度为 $A$ 的有限字符：例如长度为 $A=2$ 的二进制编码 $\\{0,1\\}$。下面是一个对四个备选信息的二进制编码：

$$\begin{array}{l\\|l|l|l|l}
\text{Message} & z_1 & z_2 & z_3 & z_4\\\\
\hline
\text{Code} & 0 & 10 & 110 & 111
\end{array}\tag{7.42}$$

这种编码也叫做即时前缀码（instantaneous prefix code）任一编码都不是另一编码的前缀，接收者（已知所有备选信息编码）可确认接收到一个完整的信息。本节只讨论这种即时前缀码。

我们可以使用 7.42 中的编码，或者可以将编码重新排序，例如分别用 110、10、111、0 代表 $z_1$、$z_2$、$z_3$、$z_4$。那么如何决定用哪一个？这取决于发送每个信息的频率。例如若发送最多的是 $z_1$，那么用最短的编码 0 代表 $z_1$ 是合理的。使用此类策略，即越常发送的消息的编码越短，缩短了平均的消息长度。

一般地，若消息发送的概率为 $\operatorname{Pr}(z_i)$，$i=1,2,3,4$，香农的一个著名的定理认为应该使用编码长度 $l_i=-\log_2 \operatorname{Pr}(x_i)$，而且平均消息长度满足：

$$\operatorname{E}(\text{length}) \geq - \sum \operatorname{Pr}(z_i) \log_2(\operatorname{Pr}(z_i))
\tag{7.43}$$

上式右侧被称为概率分布 $\operatorname{Pr}(z_i)$ 的熵。当概率满足 $p_i = A^{-l_i}$ 时，这个不等式成为等式。在上面的例子中，若概率分别为 $\operatorname{Pr}(z_i)$ = 1/2, 1/4, 1/8, 1/8$，则 7.42 中的编码是最优的编码，并且达到了熵下界。

一般场景下无法达到下界，但如霍夫曼（Huffman）编码方案的过程可以接近于下界。注意当备选消息集合是无限集时，熵被替代为 $-\int \operatorname{Pr}(z) \log_2 \operatorname{Pr}(z)dz$。基于此可推出：

> 传输一个概率密度函数为 $\operatorname{Pr}(z)$ 的随机变量 $z$，
> 需要大约 $-\log_2\operatorname{Pr}(z)$ 比特的信息。

为了简便，本节接下来用 $\log\operatorname{Pr}(z) = \log_e\operatorname{Pr}(z)$替换 $\log_2\operatorname{Pr}(z)$，只会引入一个不重要的倍数（multiplicative）常数。

现在将这个结果应用于模型选择问题。一个参数为 $\theta$ 的模型 $M$，输入和输出变量的数据为 $\mathbf{Z} = (\mathbf{X},\mathbf{y})$。令模型中输出变量的（条件）概率为 $\operatorname{Pr}(\mathbf{y}|\theta, M, \mathbf{X})$，假设接收者已知所有的输入变量，我们要传输输出变量。那么传输输出变量所需的信息长度为：

$$\text{length} = -\log\operatorname{Pr}(\mathbf{y} | \theta, M, \mathbf{X}) -
\log\operatorname{Pr}(\theta|M) \tag{7.44}$$

即给定输入变量后目标变量的对数概率。第二项是传输模型参数 $\theta$ 的平均编码长度，而第一项是传输模型与实际输出变量值之间差异的平均编码长度。例如假设有一个目标变量 $y$，其分布为 $y\sim\mathcal{N}(\theta, \sigma^2)$，参数为 $\theta\sim\mathcal{N}(0,1)$，没有输入变量（简单起见）。则信息长度为：

$$\text{length} = \text{constant} + \log\sigma +
\frac{(y-\theta)^2}{\sigma^2} + \frac{\theta^2}{2} \tag{7.45}$$

注意 $\sigma$ 越小，$y$ 越集中在 $\theta$ 周围，平均的信息长度越短，

最小描述长度原理认为应该选择最小化表达式 7.44 的模型。可看出表达式 7.44 即为（负）对数后验概率，因此最小化i描述长度就等价于最大化后验概率。因此从对数后验概率的近似导出的 BIC 准则，也可被看成是通过（近似）最小描述长度选择模型的工具。

注意以上忽略了随机变量 $z$ 编码的准确度。一个连续变量的信息无法完全用有限的长度来编码。然而，若对 $z$ 编码可有 $\delta z$ 的容忍度，则区间 $[z, z+\delta z]$ 的信息长度为其对数概率，当 $\delta z$ 较小时可用 $\delta z \operatorname{Pr}(z)$ 较好地近似。由于 $\log \delta z\operatorname{Pr}(z) = \log \delta z + \log\operatorname{Pr}(z)$，可以忽略常数 $\log \delta z$ 而只用 $\log\operatorname{Pr}(z)$ 作为信息长度的度量，上面讨论中即如此。

上述从最小描述长度出发的模型选择认为应该选择使后验概率最高的模型。然而很多贝叶斯学派则是用基于后验概率的抽样进行推断。