---
title: 7.4 训练误差率中的乐观值
summary: >
  第 228-230 页。更详细地描述了衡量模型的几种不同的“误差”。训练误差会低估泛化误差，其差距被定义为“乐观值”。其原因不止是样本外与样本内的差别，即使在样本内，乐观值也会随着模型的拟合程度而变化。

date: 2018-11-29T22:50:00+08:00
lastmod: 2022-06-13T11:23:00+08:00

weight: 704

---

对误差率的估计中需要明确哪些变量是固定的哪些是随机的，所以在讨论中可能引起困惑[^1]。在此我们会引入几个概念，详细阐释一下[第 7.2 节](({{< relref "../ch07/ch07_02.md" >}}))中的内容。给定一个训练集 $\mathcal{T}=\\{(x_i,y_i)\\}_{i=1}^N$，模型 $\hat{f}$ 的泛化误差为：

{{< math >}}
$$\text{Err}_\mathcal{T}=
\operatorname{E}_{X^0,Y^0}[L(Y^0, \hat{f}(X^0)) | \mathcal{T}] \tag{7.15}$$
{{< /math >}}

注意在式 7.15 中训练集 $\mathcal{T}$ 是固定的。$(X^0,Y^0)$ 为从总体联合分布 $F$ 随机生成的一个新测试样本点。再对训练集 $\mathcal{T}$ 取平均（对训练集的随机性做期望），则得到预期误差：

{{< math >}}
$$\text{Err}=
\operatorname{E}_{\mathcal{T}} \operatorname{E}_{X^0,Y^0}
[L(Y^0, \hat{f}(X^0)) | \mathcal{T}] \tag{7.16}$$
{{< /math >}}

预期误差更易于进行统计分析。上文中曾提过，大多数方法实际上是对预期误差 $\text{Err}$ 的估计，而不是对 $\text{Err}\mathcal{T}$ 的估计[^2]。[第 7.12 节]({{< relref "../ch07/ch07_12.md" >}})对此有更多说明。

训练误差为：

{{< math >}}
$$\overline{\operatorname{err}} = \frac{1}{N} \sum_{i=1}^N L(y_i, \hat{f}(x_i))
\tag{7.17}$$
{{< /math >}}

由于这个误差的评估与模型拟合使用相同的数据，通常它会小于真实的误差 $\text{Err}\_\mathcal{T}$（练习 2.9）。模型的拟合通常对训练样本有一定的适应性，因此训练误差或表现（apparent）误差 $\overline{\operatorname{err}}$ 是对泛化误差 $\text{Err}\_\mathcal{T}$ 的估计过于乐观。

评估点的选取位置也是造成两者之间差异的一部分原因。由于测试的输入向量可能与训练样本中的向量都不相同，可认为 $\text{Err}\_{\mathcal{T}}$ 代表了 **样本外（extra-sample）** 的误差。为了更易于理解 $\overline{\text{err}}$ 的“乐观”性，考虑 **样本内（in-sample）** 误差：

{{< math >}}
$$\text{Err}_{\text{in}} = \frac{1}{N}
\sum_{i=1}^N \operatorname{E}_{Y^0} [L(Y^0_i, \hat{f}(x_i)) | \mathcal{T}]
\tag{7.18}$$
{{< /math >}}

式中的 $Y^0$ 表示了这些是在训练样本的每个输入向量点 $x_i$，$i=1,2,\dots,N$ 处采集的共 $N$ 个新输出变量取值。我们将 **乐观值（optimism）** 定义为样本内误差 $\text{Err}\_{\text{in}}$ 与训练误差 $\overline{err}$ 之间的差：

{{< math >}}
$$\text{op} \equiv \text{Err}_{\text{in}} - \overline{\text{err}} \tag{7.19}$$
{{< /math >}}

$\overline{\text{err}}$ 通常是对预测误差的低估，因此上式通常大于零。
最后，平均乐观值是在（固定）训练集位置上的期望:

{{< math >}}
$$w \equiv \operatorname{E}_\mathbf{y}(\text{op}) \tag{7.20}$$
{{< /math >}}

这里固定了训练集的自变量取值，对训练集的输出变量取值取期望。因此这里期望的下标记为 $\operatorname{E}\_\mathbf{y}$ 而不是 $\operatorname{E}\_\mathcal{T}$。我们可以估计预期误差 $\text{Err}$，却不好估计条件误差 $\text{Err}\_\mathcal{T}$，与之类斯我们通常只能估计预期误差 $w$ 而无法估计 $\text{op}$。

对平方误差、0-1和其他损失函数，可证明一个一般性的结论：

{{< math >}}
$$w = \frac{2}{N} \sum_{i=1}^N \operatorname{Cov}(\hat{y}_i, y_i) \tag{7.21}$$
{{< /math >}}

其中 $\operatorname{Cov}$ 为协方差。因此 $\overline{\text{err}}$ 对真实误差低估的程度，取决于 $y_i$ 对它本身的预测结果的影响有多大。对样本的拟合越彻底，$\operatorname{Cov}(\hat{y}\_i,y_i)$ 就越大，因此乐观值就会增大。练习 7.4 在回归拟合值 $\hat{y}\_i$ 和使用平方误差损失的问题中证明了这个结论。对于 0-1 损失，$\hat{y}\_i\in\\{0,1\\}$ 是对 $x_i$ 的分类；对于熵损失函数，$\hat{y}\_i\in[0,1]$ 是 $x_i$ 的分类为类型 1 的概率拟合值。

综上，我们得到了一个重要的关系式：

{{< math >}}
$$\operatorname{E}_\mathbf{y}(\text{Err}_\text{in})=
\operatorname{E}_\mathbf{y}(\overline{\text{err}}) +
\frac{2}{N} \sum_{i=1}^N \operatorname{Cov}(\hat{y}_i, y_i) \tag{7.22}$$
{{< /math >}}

若 $\hat{y}\_i$ 来自于 $d$ 个输入变量或基函数的线性拟合，则可将上式简化。例如对于加性误差的模型 $Y=f(X)+\varepsilon$：

{{< math >}}
$$\sum_{i=1}^N \operatorname{Cov}(\hat{y}_i, y_i) = d\sigma^2_\varepsilon
\tag{7.23}$$
{{< /math >}}

代入后：

{{< math >}}
$$\operatorname{E}_\mathbf{y}(\text{Err}_\text{in})=
\operatorname{E}_\mathbf{y}(\overline{\text{err}}) +
2 \cdot \frac{d}{N} \sigma^2_\varepsilon \tag{7.24}$$
{{< /math >}}

式 7.23 是[第 7.6 节]({{< relref "../ch07/ch07_06.md" >}})要讨论的 **有效参数个数** 定义的基础。乐观值随着模型中使用的输入变量或基函数个数线性增长，但随着训练样本量的增加而降低。式 7.24 对其他误差模型也大致成立，比如二分类数据以及熵损失。

一个显而易见的预测误差估计方法是先得出乐观值的估计，然后再将其与训练误差 $\overline{\text{err}}$ 相加。下一节会介绍在对参数为线性的模型族中，一些诸如 $C\_p$、AIC、BIC的方法就是采取这种策略。

而相反地，本章稍后的交叉验证和自助抽样法是直接地估计样本外误差 $\text{Err}$。这种通用的工具可以适用于任何损失函数，以及非线性和自适应的拟合方法上。

要进行预测的特征不太可能在训练样本中存在一样的取值，我们通常不会直接关注样本内误差。但样本内误差在模型对比中比较方便而且通常会得到有效的模型选择。这是因为在对比中重要的是误差的相对的大小关系（而不是绝对的取值大小）。

----------

### 本节练习

#### 练习 7.1

推导式 7.24 中的样本内误差的估计。

#### 练习 7.4

在平方误差损失函数的情况下，式 7.18 的样本内预测误差和训练误差 $\overline{\text{err}}$ 为：

{{< math >}}
$$\begin{align}
\text{Err}_\text{in} &= \frac{1}{N} \sum_{i=1}^N
                        \operatorname{E}_{Y^0}(Y_i^0-\hat{f}(x_i))^2 \\
\overline{\text{err}} &= \frac{1}{N} \sum_{i=1}^N (y_i-\hat{f}(x_i))^2
\end{align}$$
{{< /math >}}

在每个表达式中各自加建 $f(x_i)$ 和 $\operatorname{E}\hat{f}(x_i)$ 然后展开。可证明训练误差中的平均乐观值为式 7.21 所示：

{{< math >}}
$$\frac{2}{N} \sum_{i=1}^N \operatorname{Cov}(\hat{y}_i, y_i)$$
{{< /math >}}


[^1]: 原文脚注1：实际上在本书的第一版中，本节的讨论就不够清楚。
[^2]: 原文为 $\operatorname{E}_\mathcal{T}$，译者认为是笔误。