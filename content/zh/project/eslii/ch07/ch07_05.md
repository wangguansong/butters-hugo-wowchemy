---
title: 7.5 样本内预测误差的估计
summary: >
  第 230-232 页。介绍了用于模型选择的 AIC 统计量。

date: 2018-12-02T13:41:00+08:00
lastmod: 2022-06-13T11:49:00+08:00

weight: 705

---

样本内估计的一般形式是：

{{< math >}}
$$\widehat{\text{Err}}_\text{in}= \overline{\text{err}} + \hat{w} \tag{7.25}$$
{{< /math >}}

其中 $\hat{w}$ 是平均乐观值（optimism）的估计。

当用平方误差损失函数拟合 $d$ 个参数时，根据式 7.24，可得到被称为 $C_p$ 统计量的一个版本：

{{< math >}}
$$C_p = \overline{\text{err}} +
2 \cdot \frac{d}{N} \hat{\sigma}^2_\varepsilon \tag{7.26}$$
{{< /math >}}

其中 $\hat{\sigma}^2\_\varepsilon$ 为噪声方差的估计，在低偏差的模型中通常可使用均方误差。
这个准则对训练误差补充了一项，其与使用的参数或基函数个数成正比。

**赤池信息量准则（Akaike information criterion，AIC）** 是一个类似的但适用更广泛的对 $\text{Err}\_\text{in}$ 的估计，它可用于使用了对数似然损失函数的问题中。它基于一个在渐进条件 $N\rightarrow\infty$ 下的与式 7.24 类似的关系：

{{< math >}}
$$ -2 \cdot \operatorname{E}[\log\operatorname{Pr}_{\hat{\theta}}(Y)] \approx
- \frac{2}{N} \cdot \operatorname{E}[\text{loglik}] + 2 \cdot \frac{d}{N}
\tag{7.27}$$
{{< /math >}}

其中 $\operatorname{Pr}\_{\theta}(Y)$ 是 $Y$ 的一族密度函数（其中包含了真实的密度函数），$\hat{\theta}$ 是 $\theta$ 的最大似然估计，“loglik” 是最大化的对数似然度：

{{< math >}}
$$\text{loglik} = \sum_{i=1}^N \log\operatorname{Pr}_{\hat{\theta}}(y_i)
\tag{7.28}$$
{{< /math >}}

例如，在对数几率回归模型中，使用二项分布的对数似然函数时可得出：

$$\text{AIC}=-\frac{2}{N}\cdot\text{loglik}+2\cdot\frac{d}{N}
\tag{7.29}$$

在高斯模型中（假设已知方差 $\sigma^2_\varepsilon=\hat{\sigma}^2\_\varepsilon$，则 AIC 统计量与 $C_p$ 等价，因此将它们统称为 AIC。

AIC 可用于模型选择，即从备选模型集合中选择 AIC 最小的模型。对非线性和其他复杂模型，需要将 $d$ 替换为模型复杂度的某种度量，这在[第 7.6 节]({{< relref "../ch07/ch07_06.md" >}})会介绍。

给定一个由调节参数 $\alpha$ 索引的模型 $f_\alpha(x)$ 的集合，记 $\overline{err}(\alpha)$ 和 $d(\alpha)$ 分别为每个模型的训练误差和参数个数。那么在这个模型的集合上，定义：

{{< math >}}
$$\text{AIC}(\alpha) = \overline{\text{err}}(\alpha) +
2 \cdot \frac{d(\alpha)}{N} \hat{\sigma}^2_\varepsilon
\tag{7.30}$$
{{< /math >}}

函数 $\text{AIC}(\alpha)$ 是测试误差曲线的估计，通过对其最小化可找到对应的调节参数 $\hat{\alpha}$。最终选择的模型就是 $f_\hat{\alpha}(x)$。需要注意如果以自适应的方式选择基函数，那么式 7.23 不再成立。例如，如果共有 $p$ 个输入变量，选择的模型是 $d$ 个输入变量的最优子集线性模型，那么乐观值会高于 $(2d/N)\sigma^2_\varepsilon$。换句话说，如果从输入变量中选取 $d$ 个变量的最优拟合模型，拟合的有效参数个数大于 $d$。

{{< figure
  id="f0704"
  src="https://public.guansong.wang/eslii/ch07/eslii_fig_07_04.png"
  title="**图 7.4**：用于第 5.2.3 节中元音识别例子中使用 AIC 选择模型。对数几率回归系数函数的模型为 $M$ 个样条基函数的展开 $\beta(f) = \sum_{m=1}^M h_m(f)\theta_m$。左图为使用对数似然函数的估计 $\text{Err}\_\text{in}$ 的 AIC 统计量。图中包括了基于独立测试样本的 $\text{Err}$ 的估计。除了在极端过参数化的情况外（$M=256$ 个参数，$N=1000$ 个样本），AIC 的估计比较准确。右图使用 0-1 损失的结果。虽然严格来说 AIC 表达式并不成立，但它的估计仍然比较合理。"
>}}

[图 7.4](#figure-f0704) 以第 148 页的[第 5.2.3 节]({{< relref "../ch05/ch05_02.md" >}})中的元音识别为例展示了 AIC 的实际应用。输入向量为在均匀分布的 256 个频率上量化的元音发音的对数周期律。用线性对数几率回归模型（逻辑回归）来预测元音分类，系数函数是 $M$ 个样条基函数的展开 $\beta(f) = \sum_{m=1}^M h_m(f)\theta_m$。对任意给定的 $M$，用自然三次样条基函数作为 $h_m$，选择的节点均匀分布在频率的范围上（故$d(\alpha) = d(M) = M$）。利用 AIC 选择基函数的个数，也会近似地对熵损失和 0-1 损失下的 $\text{Err}(M)$ 最小化。

{{< math >}}
$$\frac{2}{N} \sum_{i=1}^N \operatorname{Cov}(\hat{y}_i, y_i) =
\frac{2d}{N} \sigma^2_\varepsilon$$
{{< /math >}}

上式对加性误差项以及平方误差损失的线性模型严格成立，对线性模型和对数似然函数近似成立。尽管它在 0-1 损失中并不一定成立（Efron, 1986），但很多作者仍会在那些场景中使用这个关系式（[图 7.4](#figure-f0704) 的右图）。