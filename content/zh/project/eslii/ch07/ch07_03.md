---
title: 7.3 偏差和方差分解
summary: >
  第 223-228 页。用理论分析、图示和实例展示一个模型与真实函数之间差异的分解。其中的偏差和方差通常是此消彼涨的关系。由于 0-1 损失的离散性，其预测误差的表现与平方误差损失不同。

date: 2018-11-28T17:00:00+08:00
lastmod: 2022-06-13T10:40:00+08:00

weight: 703

---

与第二章中一样地假设 $Y=f(X)+\varepsilon$，其中的 $E(\varepsilon)=0$ 以及 $\operatorname{Var}(\varepsilon)=\sigma^2_\varepsilon$，则可推导回归拟合 $\hat{f}(X)$ 在输入向量 $X = x_0$ 点处使用平方误差损失函数的期望预测误差：

$$\begin{align}\text{Err}(x_0)
&=\operatorname{E}[(Y-\hat{f}(x_0))^2|X=x_0]\\\\
&=\sigma^2_\varepsilon+[\operatorname{E}\hat{f}(x_0)-f(x_0)]^2 +
  \operatorname{E}[\hat{f}(x_0)-\operatorname{E}\hat{f}(x_0)]^2\\\\
&=\sigma^2_\varepsilon+\operatorname{Bias}^2(\hat{f}(x_0))+
  \operatorname{Var}^2(\hat{f}(x_0))\\\\
&=\operatorname{不可约误差}+\operatorname{偏差}^2 +\operatorname{方差}
\tag{7.9}\end{align}$$

其中第一项为输出变量以真实均值 $f(x_0)$ 为中心的方差，除非 $\sigma^2_\varepsilon=0$，否则不管对 $f(x_0)$ 的估计有多好都无法避免这一项的存在。第二项是平方偏差，衡量了估计结果的平均与真实均值之间的差距。最后一项是方差，衡量了 $\hat{f}(x_0)$ 距离它的均值的期望平方偏差。一般来说，把模型 $\hat{f}$ 做得越复杂，（平方）偏差越低而方差越高。

在 k 最近邻回归拟合中，对应的表达式可以简化为：

$$\begin{align}\text{Err}(x_0)
&=\operatorname{E}[(Y-\hat{f}\_k(x_0))^2|X=x_0]\\\\
&=\sigma^2_\varepsilon+
  \left[f(x_0)-\frac{1}{k}\sum_{\ell=1}^k f(x_{(\ell)})\right]^2 +
  \frac{\sigma^2_\varepsilon}{k}
\tag{7.10}\end{align}$$

这里简单地假设训练集的输入变量 $x_i$ 为固定的，随机性完全来自 $y_i$。模型复杂度与近邻的个数 $k$ 负相关。当 $k$ 较小，模型估计 $\hat{f}_k(x)$ 可能更好地根据隐含的 $f(x)$ 进行调整。随着 $k$ 增大，模型的偏差，即 $f(x_0)$ 与 k 最近邻的 $f(x)$ 平均值之间差的平方，通常会增加，而方差则会降低。

在线性模型拟合中，$\hat{f}\_p(x) = x^T\hat{\beta}$，其中 $p$ 个参数的向量 $\beta$ 由最小二乘拟合，则有：

$$\begin{align}\text{Err}(x_0)
&=\operatorname{E}[(Y-\hat{f}\_p(x_0))^2|X=x_0]\\\\
&=\sigma^2_\varepsilon+
  [f(x_0)-\operatorname{E}\hat{f}\_p(x_0)]^2+
  \\|\mathbf{h}(x_0)\\|^2\sigma^2_\varepsilon
\tag{7.11}\end{align}$$

其中 $\mathbf{h}(x_0)=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}x_0$，即拟合结果 $\hat{f}\_p(x_0) = x_0^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$ 中的 $N \times 1$ 线性权重向量，所以 $\operatorname{Var}[\hat{f}\_p(x_0)]=\\|\mathbf{h}(x_0)\\|^2\sigma^2_\varepsilon$。虽然这个方差会随 $x_0$ 变化，它的平均（令 $x_0$ 取值在每个样本 $x_i$ 上然后取平均[^1]）是 $(p/N)\sigma^2_\varepsilon$。所以 **样本内（in-sample）** 误差为：

$$\frac{1}{N}\sum_{i=1}^N\text{Err}(x_i)=\sigma^2_\varepsilon +
  \frac{1}{N}\sum_{i=1}^N [f(x_i)-\operatorname{E}\hat{f}(x_i)]^2 +
  \frac{p}{N}\sigma^2_\varepsilon \tag{7.12}$$

这里的模型复杂度直接由参数的个数 $p$ 决定。

岭回归 $\hat{f}\_\alpha(x\_0)$ 的测试误差 $\text{Err}(x\_0)$ 与等式 7.11 形式相同，只需要改变其中方差项的线性权重的定义：$\mathbf{h}(x\_0) = \mathbf{X}(\mathbf{X}^T\mathbf{X} + \alpha\mathbf{I})^{-1}x\_0$。同时偏差项也有区别。

在例如岭回归的线性模型中，可以对偏差进一步细分。记 $\beta_\*$ 为对 $f$ 的最优拟合线性近似的参数：

$$\beta_\* = \underset{\beta}{\arg\min}\operatorname{E}\left(f(X)−X^T\beta\right)^2\tag{7.13}$$

其中的期望是对输入变量 $X$ 的分布。则可将平均平方偏差写为：

$$\begin{align}
&\operatorname{E}\_{x_0}\left[f(x_0)-\operatorname{E}\hat{f}\_\alpha(x_0)\right]^2 \\\\
&=\operatorname{E}\_{x_0}\left[f(x_0)-x_0^T\beta_\*\right]^2+
\operatorname{E}\_{x_0}\left[x_0^T\beta_\*-\operatorname{E}x_0^T\hat{\beta}\_\alpha\right]^2\\\\
&=\operatorname{Ave}[\text{模型偏差}]^2 + \operatorname{Ave}[\text{估计偏差}]^2
\tag{7.14}\end{align}$$

等式右边第一项为平均平方 **模型偏差（model bias）**，即最优拟合线性近似与真实函数之间的偏差。第二项为平均平方 **估计偏差（estimation bias）**，即估计的平均 $\operatorname{E}(x_0^T\hat{\beta})$ 与最优拟合线性近似之间的偏差。

在用最小二乘拟合的线性模型中，估计偏差为零。在有约束的拟合中，比如岭回归，估计偏差大于零，这是为了得到更低的方差付出的代价。要降低模型偏差只能通过扩大线性模型的范畴以囊括更多类型的模型，例如在模型中添加变量的交叉项或函数变换。

{{< figure
  id="f0702"
  src="https://public.guansong.wang/eslii/ch07/eslii_fig_07_02.png"
  title="**图 7.2**：偏差和方差变化的示意图。模型空间是现有模型形式中所有可能的预测的集合，其中标记了“closest fit”的黑点为“最近拟合”。图中展示了模型与真实之间的差距，同时用标记为“closest fit in population”（总体最近拟合）的黑点周围的黄色大圆圈标记了模型的方差。图中的收缩或正则化拟合会有额外的估计偏差，但较小的方差使其预测误差更小。"
>}}

> #### 译者对上图的说明
> * Truth：$f(x)$。真实的模型，无法观测。
> * Realization：$\\{x_i, y_i\\}_{i=1}^N$。对模型的观测样本，由于存在干扰项，所以与真实的模型有偏离。
> * MODEL SPACE：$\\{f_\beta(x)|\beta\in B\subset\mathbb{R}^p\\}$。模型空间，现有模型框架所能囊括的所有模型选择。图中的红色曲线为空间的边界，曲线右侧的区域为待选模型。
> * Closest fit：$\hat{f}(x)$。最近拟合，即模型空间中距离观测样本最近的点。
> * Closest fit in population：$\operatorname{E}[\hat{f}(x)]$。总体最近拟合，最近拟合对观测样本随机性的期望。如果有很多个样本，那么它们的最近拟合的平均会不断接近总体最近拟合点。
> * RESTRICTED MODEL SPACE：$\\{f_\beta(x)|\beta\in B'\subset B\\}$。有限制的模型空间，只在模型空间满足某些条件的子集上选择模型，比如限制自变量的个数或对系数的范式有约束（正则化）。
> * Shrunken fit：$\hat{f}^{\text{ridge/lasso}}(x)$。在有限制的模型空间中，距离观测样本最近的模型。
> * Model bias：$\|f(x) - E[\hat{f}(x)]\|$。模型偏差，真实模型与模型空间（最近点）之间的差距。
> * Estimation bias：$\|\hat{f}\_\tilde{\beta}(x)-\operatorname{E}[\hat{f}(x)]\|$。估计偏差，总体最近拟合与具体使用的估计方法所对应的（有限制）模型空间（最近点）之间的差距。
> * Estimation variance：$\operatorname{Var}(\hat{f}\_\tilde{\beta})$。估计方差。

[图 7.2](#figure-f0702) 为偏差方差权衡的示意图。以线性模型为例，模型空间为所有基于 $p$ 个输入变量的线性预测的集合，标记着“closest fit”（最近拟合）的黑点为 $x^T\beta_\*$。蓝色阴影区域表示了误差项 $\sigma_\varepsilon$ 的影响，观测到的训练样本的是真实模型和误差项的叠加。

图中以标记为“closest fit in population”（总体最近拟合）为中心的黄色大圆圈，代表了最小二乘拟合的方差。如果现在要用更少的自变量拟合模型，或通过向零收缩对系数正则化，则得到的是图中的“shrunken fit”（收缩拟合）。由于它不是模型空间上的最近拟合，这个拟合中会有额外的估计偏差。但另一方面，它的方差更小。若方差的降低大于（平方）偏差的增加，那这个取舍就是值得的。

### 7.3.1 实例：偏差-方差权衡

{{< figure
  id="f0703"
  src="https://public.guansong.wang/eslii/ch07/eslii_fig_07_03.png"
  title="**图 7.3**：一个模拟例子的期望预测误差（橙色）、平方偏差（绿色）和方差（蓝色）。上面一行是使用平方误差损失函数的回归问题；下面一行是使用 0-1 损失的分类问题。左侧图使用 k-近邻，右侧图使用 $p$ 个变量的最优子集回归。在回归和分类场景中给出的方差和偏差曲线都相同，但预测误差曲线不同。"
>}}

[图 7.3](#figure-f0703) 展示了两个模拟例子中的偏差方差权衡。其中有 80 个观测点和 20 个在超立方体 $[0,1]^{20}$ 上均匀分布的自变量，真实输出变量的生成方式如下：

* 左图：若 $X_1\leq 1/2$，则 $Y=0$；若 $X_1>1/2$，则 $Y=1$。使用 k-最近邻方法。
* 右图：若 $\sum_{j=1}^{10}X_j>5$，则 $Y=1$；否则，$Y=0$。使用 $p$ 个自变量的最优子集线性回归。

顶部一行是回归的平方误差损失；底部一行是分类的 0-1 损失。图中展示了在一个足够大的测试集上计算的预测误差（红色）、平方偏差（绿色）和方差（蓝色）。

在回归问题中，预测误差曲线为偏差和方差的和，在 k 最近邻中其最低点为 $k=5$，在线性模型中最低点为 $ p \geq 10$。在分类问题中（下面二图）出现了有趣的现象。偏差和方差曲线与上面二图完全相同，而这里的预测误差为误分类比率，其曲线不再是平方偏差和方差曲线之和。在 k 最近邻分类器中，近邻数量增加到 20 之后，尽管平方偏差仍在上升，预测误差却略有下降。在线性模型分类器中，最低点和回归问题一样出现在 $p \geq 10$，但模型从 $p=1$ 开始的提升幅度非常巨大。在构成预测误差时，偏差和方差看似存在相互作用。

为什么会如此？对预测误差曲线的改变有一个简单的解释。假设给定一个输入变量点，其分类为 1 的真实概率为 0.9 而模型估计的期望值为 0.6。则此时存在合理的平方偏差 $(0.6 - 0.9)^2$，但由于两者都给会给出正确的分类，预测误差为零。或者说，在判别边界正确分类一边的统计误差不会影响预测误差。练习 7.2 从理论分析演示了这个现象，并展示了偏差和方差之间的相互影响。

总的来说，偏差-方差权衡在 0-1 损害和平方误差损失中的表现不同。这意味着在两个不同的场景中调节参数的最优选择可能截然不同。如下面章节的讨论，条件参数的选择要基于预测误差的估计。

----------
### 本节练习
----------
**练习 7.2**
假设在 0-1 损失函数下，$Y\in\\{0,1\\}$，$\operatorname{Pr}(Y=1|x_0)=f(x_0)$，证明：

$$\begin{align}
\text{Err}(x_0)
&=\operatorname{Pr}(Y\neq\hat{G}(x_0)|X=x_0)\\\\
&=\text{Err}_B(x_0)+|2f(x_0)-1|\operatorname{Pr}(\hat{G}(x_0)\neq G(x_0)|X=x_0)
\end{align}$$
$$\tag{7.62}$$

其中 $\hat{G}(x)=I(\hat{f}(x)>\frac{1}{2})$；$G(x)=I(f(x)>\frac{1}{2})$ 为贝叶斯分类器；$\text{Err}_B(x_0)=\operatorname{Pr}(Y\neq G(x_0)|X=x_0)$ 为 $x_0$ 处的不可约的 **贝叶斯误差（Bayes error）**。

利用近似关系 $\hat{f}(x_0)\sim \mathcal{N}(\operatorname{E}(\hat{f}(x_0)), \operatorname{Var}(\hat{f}(x_0)))$，证明：

$$\operatorname{Pr}(\hat{G}(x_0)\neq G(x_0)|X=x_0)\approx\Phi\left(\frac{\operatorname{sign}(\frac{1}{2}-f(x_0))(\operatorname{E}\hat{f}(x_0)-\frac{1}{2})}{\sqrt{\operatorname{Var}(\hat{f}(x_0))}}\right)$$
$$\tag{7.63}$$

在上式中的函数为高斯分布的累积分布函数，在 $t=-\infty$ 时取值为 0；在 $t=+\infty$ 时取值为 1：

$$\Phi(t)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^t\exp(-t^2/2)dt$$

我们可将 $\operatorname{sign}(\frac{1}{2}-f(x_0))(\operatorname{E}(\hat{f}(x_0))-\frac{1}{2})$ 看作是一种 **边界偏差（boundary bias）** 项，因为它只取决于真实的 $f(x_0)$ 位于边界（$\frac{1}{2}$）的哪一侧。同时也注意到偏差和方差是以乘积的形式组合的，而不是相加的形式。如果 $\operatorname{E}\hat{f}(x_0)$ 和 $f(x_0)$ 位于 $\frac{1}{2}$ 的同一侧，那么偏差为负，而降低方差就会降低误分类误差。而另一方面，如果 $\operatorname{E}\hat{f}(x_0)$ 和 $f(x_0)$ 位于 $\frac{1}{2}$ 的不同侧，则偏差为正，而且它也会使方差增大。这个对方差增大的影响会使 $\hat{f}(x_0)$ 更有可能落在 $\frac{1}{2}$ 正确分类的一侧（Friedman, 1997）。

[^1]: $\mathbf{h}$ 在样本点上的和等于 OLS 投影矩阵的迹（trace），可证明[它等于样本矩阵 $\mathbf{X}$ 的秩（rank）](https://math.stackexchange.com/questions/1582567/)。