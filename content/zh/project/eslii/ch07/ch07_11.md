---
title: 7.11 自助抽样方法
summary: >
  第 249-254 页。用自助抽样方法估计预测误差，以及对其中偏差的修正方法。

date: 2018-12-12T11:21:00+08:00
lastmod: 2022-06-14T10:43:00+08:00

weight: 711

---

**自助抽样（bootstrap）** 是评估统计准确性的一般性工具。本节先从大体上介绍自助抽样方法，然后再演示如何将其用于估计样本外预测误差。同交叉验证一样，虽然目标是要估计条件误差 $\text{Err}_\mathcal{T}$，但通常自助抽样只能较好地估计期望预测误差 $\text{Err}$。

假设使用一个模型拟合一组训练数据。记训练集为 $\mathbf{Z} = (z_1,z_2,\dots,z_N)$，其中 $z_i = (x_i, y_i)$。这个方法的基本思想是从训练数据中有放回地随机抽取数据，获得与原训练集同样大小的新集合。如此进行 $B$ 次（例如 $B=100$），则会得到 $B$ 个自助抽样数据集，如[图 7.12](#figure-f0712) 所示。然后在每个自助抽样数据集上重新拟合模型，再考察 $B$ 次重复得到拟合结果的性质。

{{< figure
  id="f0712"
  src="https://public.guansong.wang/eslii/ch07/eslii_fig_07_12.png"
  title="**图 7.12**：自助抽样过程的示意图。目标是评估从数据集中计算的数值 $S(\mathbf{Z})$ 的统计准确性。从原数据集中有放回的抽取 $B$ 个大小为 $N$ 的训练集 $\mathbf{Z}^{\*b}$，$b=1,\dots,B$。在每个自助抽样训练集上计算数值 $S(\mathbf{Z})$，并用 $S(\mathbf{Z}^{\*1})$，……，$S(\mathbf{Z}^{\*B})$ 来评估 $S(\mathbf{Z})$ 的统计准确性。"
>}}

图中的 $S(\mathbf{Z})$ 为从数据 $\mathbf{Z}$ 中计算出的任意一种统计量，例如在某个输入点处的预测值。通过自助抽样可以估计 $S(\mathbf{Z})$ 分布的任意方面特征，比如其方差为：

$$\widehat{\text{Var}}[S(\mathbf{Z})] = \frac{1}{B-1}
\sum_{b=1}^B (S(\mathbf{Z}^{\*b}) - \bar{S}^\*)^2 \tag{7.53}$$

其中 $\bar{S}^\* = \sum_b S(\mathbf{Z}^{*b})/B$。注意到 $\widehat{\text{Var}}[S(\mathbf{Z})]$ 可被视为从数据 $(z_1,z_2,\dots,z_N)$ 的经验分布函数 $\hat{F}$ 抽样的对 $S(\mathbf{Z})$ 方差的一个蒙特卡罗（Monte-Carlo）估计。

如何用自助抽样来估计预测误差？一个方法是用一组自助样本来拟合备选模型，然后记录其对原训练集的预测准确程度。若 $\hat{f}^{\*b}(x_i)$ 表示第 b 个自助集上拟合的模型在 $x_i$ 点处的预测值，那么预期损失的估计是：

$$\widehat{\text{Err}}\_\text{boot} = \frac{1}{B}\frac{1}{N}
\sum_{b=1}^B \sum_{i=1}^N L(y_i, \hat{f}^{\*b}(x_i)) \tag{7.54}$$

然而很明显 $\widehat{\text{Err}}\_\text{boot}$ 一般来说并不是一个好的估计。因为自助集作为训练样本，而原训练集作为测试样本，这两个集合中有共同的观测样本。两者间的重叠会不真实地过高评价过拟合模型的预测表现，这也是交叉验证直接地将数据分成不重叠的训练集和测试集的原因。以一个 1 最近邻分类器为例，样本为二分类，每个类别中的样本个数相等，自变量与类别标签独立。那么真实的误差率为 0.5。但如果样本 i 出现在自助抽样 b 中，它对自助抽样的估计 $\widehat{\text{Err}}\_\text{boot}$ 的贡献为零；如果样本 i 不在自助集 b 中，它的贡献的期望是 0.5，即正确的期望误差率。所以：

$$\begin{align}
\operatorname{Pr}\\{\text{样本 i}\in\text{自助样本 b}\\}
&= 1 - \left( 1 - \frac{1}{N} \right)^N \\\\
&\approx 1 - e^{-1} \\\\ &= 0.632
\tag{7.55}\end{align}$$

因此，$\widehat{\text{Err}}\_\text{boot}$ 的期望是 $0.5 \times 0.368 = 0.184$，远低于正确的误差率 0.5。

通过参考交叉验证可得到一个更好的自助抽样估计。对每个观测样本，只记录由不包含该样本的自助抽样拟合模型的预测。预测误差的留一（leave-one-out）自助抽样估计定义为：

$$\widehat{\text{Err}}^{(1)} = \frac{1}{N}
\sum_{i=1}^N \frac{1}{\|C^{-i}\|}
\sum_{b\in C^{-i}} L(y_i, \hat{f}^{*b}(x_i)) \tag{7.56}$$

其中 $C^{-i}$ 为不包含样本 i 的自助集 b 的索引下标的集合，$\|C^{-i}\|$ 为这个集合的元素个数。计算 $\widehat{\text{Err}}^{(1)}$ 时，需要 $B$ 足够大才能使所有的 $\|C^{-i}\|$ 都大于零，或者可以就舍弃式 7.56 中 $\|C^{-i}\|$ 为零所对应的样本 i。

留一自助抽样解决了 $\widehat{\text{Err}}\_\text{boot}$ 中的过拟合问题，但会存在如交叉验证中提到过的训练集样本量的偏差问题。每个自助抽样中的唯一观测样本平均数量大约为 $0.632 \cdot N$，因此其偏差大致与二折交叉验证表现类似。因此如果学习曲线在样本量 $N/2$ 处存在较大的斜率，则留一自助法对真实误差的估计会偏大。

“.632 估计量”从设计上可以减轻这种偏差。其定义为：

$$\widehat{\text{Err}}^{(.632)} =
.368 \cdot \overline{\text{err}} + .632 \cdot \widehat{\text{Err}}^{(1)}
\tag{7.57}$$

.632 估计量的推导比较复杂；从直观上，它将留一自助法估计向训练误差率方向拉低，因此减少其向上的偏差。常数 .632 来自式 7.55。

.632 估计量在“轻度拟合”问题中表现较好，但在过拟合问题中可能失效。以下为出自 Breiman et al. (1984) 的一个例子。假设样本量相等的二分类样本，自变量与类别标签独立，使用 1 最近邻分类器。
那么 $\overline{\text{err}}=0$，$\widehat{\text{Err}}^{(1)} = 0.5$，所以 $\widehat{\text{Err}}^{(.632)} = .632 \times 0.5 = .316$。然而真实的误差率为 0.5。

可以通过纳入过拟合程度的考量来改进 .632 估计量。先定义 $\gamma$ 为 **无信息误差率（no-information error rate）**，即当输入变量和类别标签独立时，预测规则的错误率。可通过在所有目标变量 $y_i$ 和自变量 $x_{i'}$ 的两两组合上评估预测结果，从而得到 $\gamma$ 的估计：

$$\hat{\gamma} = \frac{1}{N^2}\sum_{i=1}^N\sum_{i'=1}^N
L(y_i, \hat{f}(x_{i'})) \tag{7.58}$$

例如，在一个二分类问题中：令 $\hat{p}\_1$ 为样本中输出变量 $y_i$ 为 1 的比例，$\hat{q}\_1$ 为预测值 $\hat{f}(x_{i'})$ 为 1 的比例。那么：

$$\hat{\gamma} =
\hat{p}\_1 (1-\hat{q}\_1) + (1-\hat{p}\_1) \hat{q}\_1 \tag{7.59}$$

在 1 最近邻的方法中，$\hat{q}\_1 = \hat{p}\_1$，$\hat{\gamma}$ 的值为 $2\hat{p}\_1(1-\hat{p}\_1)$。式 7.59 在多类别中的推广为 $\hat{\gamma} = \sum_\ell \hat{p}\_\ell(1-\hat{q}\_\ell)$。

**相对过拟合率（relative overfitting rate）** 的定义为：

$$\hat{R} = \frac
{\widehat{\text{Err}}^{(1)} - \overline{\text{err}}}
{\hat{\gamma} - \overline{\text{err}}}\tag{7.60}$$

其取值在 0 和 1 之间。当没有过拟合（$\widehat{\text{Err}}^{(1)}=\overline{\text{err}}$）时，取值为 0；当过拟合等于无信息值（$\hat{\gamma}-\overline{\text{err}}$）时，取值为 1。最后，定义 “.632+ 估计量”为：

$$\begin{align}
\widehat{\text{Err}}^{(.632+)}
&=(1-\hat{w})\cdot \overline{\text{err}} +
  \hat{w} \cdot \widehat{\text{Err}}^{(1)} \tag{7.61} \\\\
\text{with }\hat{w}
&= \frac{.632}{1 - .368\hat{R}}
\end{align}$$

当 $\hat{R}=0$ 时，权重 $w$ 为 .632；当 $\hat{R}=1$ 时，权重为 1。所以 $\widehat{\text{Err}}^{(.632+)}$ 取值范围从 $\widehat{\text{Err}}^{(.632)}$ 到 $\widehat{\text{Err}}^{(1)}$。式 7.61 的推导也比较复杂；粗略地说，它在留一自助抽样和依赖于过拟合程度的训练误差率之间的折中。在自变量与类别标签独立的 1 最近邻问题中，$\hat{w} = \hat{R} = 1$，所以 $\widehat{\text{Err}}^{(.632+)}=\widehat{\text{Err}}^{(1)}$，它的期望为正确的 0.5。在过拟合程度轻一些的其他问题中，$\widehat{\text{Err}}^{(.632+)}$ 会介于 $\overline{\text{err}}$ 与 $\widehat{\text{Err}}^{(1)}$ 之间。

### 7.11.1 示例（续）

{{< figure
  id="f0713"
  src="https://public.guansong.wang/eslii/ch07/eslii_fig_07_13.png"
  title="**图 7.13**：图 7.3 中四个场景的相对误差分布的箱形图，$100 \cdot [\text{Err}\_\hat{\alpha} - \min_\alpha \text{Err}(\alpha)] / [\max_\alpha \text{Err}(\alpha) - \min_\alpha \text{Err}(\alpha)]$。其衡量了使用所选模型相对于最佳模型的误差。每个箱形图中包含了 100 组训练集。"
>}}

[图 7.13](#figure-f0713) 展示了与[图 7.7]({{< relref "../ch07/ch07_09.md#figure-f0707" >}}) 中相同的四个场景的十折交叉验证和 .632+ 自助抽样估计结果。与[图 7.7]({{< relref "../ch07/ch07_09.md#figure-f0707" >}}) 一样，[图 7.13](#figure-f0713) 展示了使用所选模型相对于最佳模型的误差的箱形图，即 $100 \cdot [\text{Err}\_\hat{\alpha} - \min_\alpha \text{Err}(\alpha)]/[\max_\alpha \text{Err}(\alpha)-\min_\alpha \text{Err}(\alpha)]$ 每个箱形图中使用了 100 组不同的训练集。两个方法的总体表现比较好，可能与图 7.7 中的 AIC 表现类似或略差。

我们的结论是对这些特定的问题和拟合方法，最小化 AIC、交叉验证或自助抽样所产生的模型比较接近于可得的最佳模型。注意在做模型选择时，这些度量准则都可能有偏差，但只要偏差没有改变拟合方法表现的相对关系，就对结果没有影响。例如，给任意度量加上一个常数不会改变模型选择的结果。然而，对于许多自适应的非线性方法（比如树模型），难以估计有效参数个数。这会使 AIC 类型的方法不再可用，这样就只能用交叉验证或自助抽样来做模型选择。

另一个问题是：各个估计测试误差的方法有多准确？在四个场景中，AIC 准则对所选模型的预测误差的平均高估分别 38%、37%、51% 和 30%，BIC 准则的表现类似。与之相比，交叉验证对误差的高估为 1%、4%、0% 和 4%，自助抽样的表现类似。因此如果需要准确地估计测试误差，值得通过额外的步骤来计算交叉验证或自助抽样的估计。在类似于树模型的其他拟合方法中，由于寻找最佳树模型会很大地被验证集影响，交叉验证和自助抽样可能会对真实误差有 10% 的低估。在这些场景中，只有用独立的测试集可以的到测试误差的无偏估计。

----------

### 本节练习

----------

**练习 7.9**
For the prostate data of Chapter 3, carry out a best-subset linear
regression analysis, as in Table 3.3 (third column from left). Compute the
AIC, BIC, five- and tenfold cross-validation, and bootstrap .632 estimates
of prediction error. Discuss the results.