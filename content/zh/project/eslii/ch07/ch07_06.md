---
title: 7.6 有效参数数量
summary: >
  第 232-233 页。将线性回归中参数数量的概念拓展到更一般的模型中，即有效参数数量或有效自由度。

date: 2018-12-02T14:46:00+08:00
lastmod: 2022-06-13T15:17:00+08:00
draft: false
math: true

type: book
weight: 706

authors: ["Butters"]
tags: ["译文"]
categories: ["统计学习基础（译注）"]

---

可以将“参数数量”的概念一般化，特别是在使用了正则化拟合的模型中。假设 $\mathbf{y}$ 为输出变量 $y_1,y_2,\dots,y_N$ 组成的向量，与之类似 $\hat{\mathbf{y}}$ 为预测结果的向量。那么一个线性拟合模型可表达为：

$$\hat{\mathbf{y}}=\mathbf{S}\mathbf{y}\tag{7.31}$$

其中的 $\mathbf{S}$ 是 $N \times N$ 的矩阵，它依赖于输入向量 $x_i$ 但不依赖于 y_i$。
线性拟合方法中包含了在原始特征或衍生出的基函数上的线性回归，以及使用了二次收缩的平滑方法，比如岭回归和三次平滑样条。那么，**有效参数数量（effective number of parameters）** 可定义为 $\mathbf{S}$ 的对角线元素之和：

$$\operatorname{df}(\mathbf{S})=\operatorname{trace}(\mathbf{S})\tag{7.32}$$

这也被称为 **有效自由度（effective degrees-of-freedom）**。注意如果 $\mathbf{S}$ 为一个到 $M$ 个特征生成的基函数集合的正交投影矩阵，那么 $\operatorname{trace}(\mathbf{S})=M$。这样的话，在 $\operatorname{trace}(\mathbf{S})$ 的数值恰好等于等式 7.26 的 $C_p$ 统计量中的参数数量 $d$。如果 $\mathbf{y}$ 来自于加性误差模型 $Y=f(X)+\varepsilon$，其中 $\operatorname{Var}(\varepsilon)=\sigma^2_\varepsilon$，则可以证明 $\sum_{i=1}^N \operatorname{Cov}(\hat{y}\_1,y_i)=\operatorname{trace}(\mathbf{S})\sigma^2_\varepsilon$，这也启发了一个更具一般性的（参数有效数量）定义方式：

$$\operatorname{df}(\hat{\mathbf{y}})=\frac
{\sum_{i=1}^N\operatorname{Cov}(\hat{y}\_i,y_i)}{\sigma^2_\varepsilon}
\tag{7.33}$$

（见练习 7.4 和 7.5）。
第 153 页的[第 5.4.1 节]({{< relref "../ch05/ch05_04.md" >}})在平滑样条的场景中对 $\operatorname{df} = \operatorname{trace}(\mathbf{S})$ 做了更多直觉上的说明。

在类似于神经网络的模型中，会对一个含有权重衰减惩罚（正则）项 $\alpha\sum_m w_m^2$ 的误差函数 $R(w)$ 最小化，它的有效参数数量可写为：

$$\operatorname{df}(\alpha) = \sum_{m=1}^M
\frac{\theta\_m}{\theta\_m + \alpha)} \tag{7.34}$$

其中 $\theta\_m$ 为海森（Hessian）矩阵 $\partial^2 R(w) / \partial w \partial w^T$ 的特征值。如果利用在最优解处误差函数的二次近似，则可从式 7.32 得出式 7.34（Bishop, 1995）。

----------
### 本节练习
----------

**练习 7.5**

对于一个线性平滑模型 $\hat{\mathbf{y}}=\mathbf{S}\mathbf{y}$，证明：

$$\sum_{i=1}^N\operatorname{Cov}(\hat{y}\_i,y_i)=\operatorname{trace}(\mathbf{S})\sigma_\varepsilon^2\tag{7.65}$$

这印证了它作为有效参数数量的合理性。