---
title: 12.6 惩罚判别分析
summary: >
  第 446-449 页。若 LDA 可通过线性回归实现，那么当自变量很多时，适当的正则项会有助于模型的表现。

date: 2022-05-23T18:45:00+08:00

weight: 1206

---

尽管 FDA 是从最优评分函数的推广过程启发而来的，但也可以将其直接理解为一个正则化的判别分析。假设 FDA 中的回归是在基扩展 $h(X)$ 上的线性回归，并有系数的二次惩罚项：

{{< math >}}
$$\begin{align}
  &\text{ASR}(\{\beta_\ell, \beta_\ell\}_{\ell=1}^N) = \\
  &\frac{1}{N} \sum_{\ell=1}^L \left[
    \sum_{i=1}^N (\theta_\ell(g_i) - h^T(x_i)\beta_\ell)^2 +
    \lambda \beta_\ell^T \mathbf{\Omega} \beta_\ell \right]
\tag{12.57}\end{align}$$
{{< /math >}}

$\mathbf{\Omega}$ 的选择取决于具体的问题。如果 $\eta_\ell(x)=h(x)\beta_\ell$ 是一个样条基函数的扩展，那么 $\mathbf{\Omega}$ 可以是令 $\eta_\ell$ 在 $\mathbb{R}^p$ 上平滑的约束。如果是加性样条，则每个坐标有 $N$ 个样条基函数，$h(x)$ 中总共有 $Np$ 个基函数；这里的 $\mathbf{\Omega}$ 就是一个 $Np\times Np$ 的分块对角矩阵。

那么 FDA 的计算步骤就可以看成是一个正则化的 LDA，我们称之为 **惩罚判别分析（penalized discriminant analysis，PDA）**：

- 用基扩展 $h(X)$ 扩大自变量 $X$ 的集合。
- 在扩大的特征空间上进行带惩罚的 LDA，其中的惩罚马氏（Mahalanobis）距离为
  {{< math >}}
  $$\begin{align}
  & D(x,\mu) = \\
  & (h(x) - h(\mu))^T (\mathbf{\Sigma}_W +\lambda\mathbf{\Omega})^{-1}
    (h(x) - h(\mu)) \tag{12.58}
  \end{align}$$
  {{< /math >}}
  其中 $\mathbf{\Sigma}\_W$ 衍生变量 $h(x_i)$ 的样本内协方差矩阵。
- 使用一个惩罚度量，在分类子空间上就行分解。
  {{< math >}}
  $$\max u^T \mathbf{\Omega}_\text{Bet} u \text{ subject to }
  u^T (\mathbf{\Sigma}_W + \lambda \mathbf{\Omega}) u = 1$$
  {{< /math >}}

不太严格地说，惩罚马氏距离倾向于给”粗糙“的坐标更少权重，而给平滑的坐标更多权重。这是因为惩罚项不是对角的，对粗糙和平滑的线性组合都是一样的（惩罚）。

在一些类型的问题中，并不需要进行第一步的基扩展，因为可能已经有了过多的（互相关联的）自变量。一个重要的例子是当代分类的对象是被数字化的模拟信号：

- 在 256 个不同频率上取样的一段口头语音的对数周期图（log-periodogram）；参考图 5.5。
- 手写阿拉伯数字图片经过数字化后的像素点灰度值。

从直觉上也很容易理解为什么在这些情况下需要进行正则化。以数字化图片为例。相邻的像素点往往会是相关联的，常常是几乎一样的。这意味着这些像素点在 LDA 中对应的系数可能会非常不同甚至符号相反，因此会被相互约掉。正相关的自变量会导致被干扰的（noisy）、负相关的系数估计，而这个干扰会导致多余的样本方差。所以在例如图片的空间域上对系数进行平滑的正则（regularize）约束是一个合理的策略。这即是 PDA 的做法。它的计算过程与 FDA 一样，只是使用了一个恰当的惩罚回归方法。其中 $h^T(X)\beta_\ell=X\beta_\ell$，并且 $\mathbf{\Omega}$ 的选择可使 $\beta_\ell^T\mathbf{\Omega}\beta_\ell$ 看成是对 $\beta_\ell$ 在图片中的粗糙程度的惩罚。图 1.2 展示了手写数字的一些示例。

{{< figure
  id="f1211"
  src="https://public.guansong.wang/eslii/ch12/eslii_fig_12_11.png"
  title="**图 12.11**：两两的图，代表了数字识别问题中的九个判别系数函数。每对图的左图为 LDA 系数，而右图为 PDA 系数；PDA 为使图片平滑而进行了正则化。"
>}}

[图 12.11](#figure-f1211) 展示了 LDA 和 PDA 的判别变量。LDA 得出的那些是颗粒感（salt-and-pepper）很强的图片，而 PDA 得出的是光滑的图片。第一个平滑图可被视为是一个线性对比函数的系数，这个函数分离了中心垂直条状的图形（数字 1 或 7）和中间空心的图片（数字 0 和 4）。

{{< figure
  id="f1212"
  src="https://public.guansong.wang/eslii/ch12/eslii_fig_12_12.png"
  title="**图 12.12**：测试集上的前两个惩罚典型变量。圆圈代表了类别中心点。第一个系数主要区分了类别 0 和类别 1，而第二个系数主要区别了类别 6 和类别 7/9。"
>}}

[图 12.12](#figure-f1212) 也可说明这一点，其中第二个分离没有第一个分离那么明显。Hastie et al. (1995) 介绍了这个和其他列子的更多细节，同时也在他们的案例中演示了 LDA 加上了正则项后其在独立的测试集上的分类效果提升了 25%。

----------

### 本节练习

#### 练习 12.7

Derive the solution to the penalized optimal scoring problem (12.57).

#### 练习 12.8

Show that coefficients βℓ found by optimal scoring are proportional
to the discriminant directions νℓ found by linear discriminant analysis.